[
["index.html", "Анализ временных рядов с помощью R Аннотация", " Анализ временных рядов с помощью R Мастицкий С. Э. 2020-04-11 Аннотация Мастицкий С. Э. (2020) Анализ временных рядов с помощью R. — Электронная книга, адрес доступа: https://ranalytics.github.io/tsa-with-r Эта книга представляет собой небольшое пособие по использованию языка программирования и системы статистических вычислений R для анализа временных рядов. Упор сделан на решение нескольких стандартных задач, включая прогнозирование, выявление структурных изменений и аномалий в данных, а также кластеризацию временных рядов. Описание соответствующих подходов и программного обеспечения сопровождается многочисленными примерами кода в применении к данным из реального мира. Книга рассчитана на опытных пользователей R, которым знакомы принципы построения предсказательных моделей, ряд стандартных методов статистики (регрессия, метод главных компонент, кластерный анализ), а также основы байесовской статистики. Данная работа распространяется в рамках лицензии Creative Commons “Атрибуция — Некоммерческое использование — На тех же условиях 4.0 Всемирная” (CC BY-NC-SA 4.0). Согласно этой лицензии, Вы можете свободно копировать, распространять и видоизменять данное произведение при условии точного указания его автора и источника. При изменении этого произведения или использовании его в своих работах, Вы можете распространять результат только по такой же или подобной лицензии. Запрещается использовать эту работу в коммерческих целях без согласования с автором. Более подробная информация о лицензии представлена на странице https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ru © 2020, Сергей Эдуардович Мастицкий Эл. почта: rtutorialsbook@gmail.com Веб–сайт: http://r-analytics.blogspot.com "],
["thanks.html", "Благодарности", " Благодарности С благодарностью ко всем читателям моего блога. Без вашей поддержки эта книга не появилась бы на свет. "],
["other-formats.html", "Дополнительные форматы книги", " Дополнительные форматы книги Эта книга доступна в платном виде также в форматах PDF, epub и azw3. Стоимость всех трех файлов составляет 900₽. Сделать заказ можно через Яндекс-сервис Я.Соберу. "],
["intro.html", "ГЛАВА 1 Введение 1.1 О чем эта книга и чего в ней нет 1.2 Что ожидается от читателя 1.3 Основные понятия 1.4 Формат данных tsibble 1.5 Данные, используемые в примерах", " ГЛАВА 1 Введение 1.1 О чем эта книга и чего в ней нет Несколько лет назад, когда я впервые столкнулся с задачей по прогнозированию выручки компании, в которой на тот момент работал, я не знал с чего начать, потому что прежде не имел дела с временными рядами. Как это часто бывает, я обратился к CRAN Task Views, чтобы получить общее представление о том, какие пакеты для R подошли бы для решения моей задачи. Увиденное разнообразие инструментов и большой объем знаний, которые потребовались бы для уверенного пользования этими инструментами, вызвали у меня небольшую панику. С чего начать? Какой пакет выбрать, чтобы потратить минимальное время на его изучение и успешно завершить проект в срок? Какую обзорную статью почитать, чтобы быстро вникнуть в тот или иной метод? Эта книга — моя скромная попытка помочь тем, кто оказался в похожей ситуации. Анализ временных рядов — обширная тема, по которой есть очень много литературы, и у меня ни в коей мере не было цели написать еще одную теоретическую работу. Вместо этого я делаю упор на решение нескольких стандартных задач и делюсь с читателями многочисленными примерами кода в применении к данным из реального мира. Описанное здесь — это методы и программное обеспечение, которые я добавил в свой личный “набор инструментов” в результате работы над несколькими аналитическими проектами и которые показали свою практическую ценность. Надеюсь, что эти инструменты окажутся полезными и для читателей. Теоретическая часть обсуждаемых методов представлена лишь в том объеме, который необходим для осознанного использования соответствующих пакетов для R. Дополнительные теоретические выкладки можно найти в приведенных в тексте ссылках на ключевые публикации. Большинство представленных здесь материалов ранее было опубликовано в моем блоге “R: Анализ и визуализация данных”. Не затронутые в блоге, но добавленные в книгу темы включают визуализацию различных свойств временных рядов (гл. 4), прогнозирование с помощью байесовских структурных моделей (гл. 7) и кластерный анализ временных рядов (гл. 10–13). 1.2 Что ожидается от читателя Книга в основном рассчитана на опытных пользователей языка программирования и системы статистических вычислений R. Поэтому ожидается, что читателю знакомы: основные пакеты из группы tidyverse (в частности, dplyr, ggplot2, tibble, и tidyr); принципы построения предсказательных моделей; такие стандартные методы статистики и машинного обучения, как кусочно-линейная регрессия, обобщенные аддитивные модели, метод главных компонент и кластерный анализ; основы байесовской статистики. Если вы только начинаете работать с R, то я порекомендовал бы сначала обратиться к вводным главам любой из следующих книг на русском языке: Кабаков Р.И. Анализ и визуализация данных в программе R / пер. с англ. П. Волковой. М.: ДМК Пресс, 2014. Мастицкий С.Э., Шитиков В.К. Статистический анализ и визуализация данных с помощью R. М.: ДМК Пресс, 2015. Уикем X., Гроулмунд Г. Язык R в задачах науки о данных. Импорт, подготовка, обработка, визуализация и моделирование данных / пер. с англ. А.Г. Гузикевича. М.: Вильямс, 2018. Приведенные в этой книге примеры можно без труда воспроизвести на любом современном компьютере. Для этого лишь потребуются R c версией не ниже 3.6.3 и любая удобная для вас интегрированная среда разработки (например, RStudio). Большинство пакетов для R, используемых в примерах, можно установить стандартным образом из хранилища CRAN с помощью функции install.packages(). В редких случаях, когда тот или иной пакет отсутствует в CRAN, я привожу дополнительные инструкции по его установке. Небольшой скрипт для инсталляции всех нужных пакетов есть в Github–репозитории ranalytics/tsa-r. Используйте, пожалуйста, раздел Issues этого репозитория также для сообщения об обнаруженных в тексте ошибках и неточностях. 1.3 Основные понятия Временной ряд представляет собой последовательность значений некоторой переменной (или переменных), регистрируемых через определенные промежутки времени (регулярные или нерегулярные). Когда есть только одна наблюдаемая переменная, временной ряд называют одномерным. В случае же с несколькими параллельно наблюдаемыми переменными говорят о многомерном временном ряде. Мы будем рассматривать только одномерные ряды. Временные ряды можно встретить в самых разных областях — от метеорологии, где ведется учет данных по погодным показателям, до Интернета вещей, где регистрируются сигналы от всевозможных встроенных датчиков и сенсоров. Под анализом временных рядов мы будем понимать процесс применения методов статистики и машинного обучения для выявления закономерностей в структуре временных рядов и предсказания будущего поведения описываемых этими рядами систем. В частности, будут рассмотрены следующие распространенные задачи: прогнозирование, т.е. предсказание будущих значений временного ряда; выявление структурных изменений и аномалий, вызванных в изучаемой системе влиянием внешних или внутренних факторов (например, изменения в экономических показателях, связанные с политическим событиями в стране; всплески уровней продаж, обусловленные рекламными кампаниями; кратковременные неисправности в технических системах, и т.п.); кластеризация, т.е. нахождение групп временных рядов, похожих по своим свойствам. В общем случае одномерный временной ряд из наблюдений \\(y_t\\), учтенных в моменты времени \\(t\\), можно разложить на следующие составляющие, или компоненты: тренд (\\(T_t\\)): характеризует долговременную тенденцию в данных (снижение или возрастание). Тренд может быть линейным или нелинейным. В некоторых временных рядах может также наблюдаться изменение направления тренда (например, когда рост сменяется спадом). циклическая компонента (\\(C_t\\)): долговременные циклические колебания, обычно занимающие не менее 2 лет. Как правило, частота таких изменений непостоянна (например, хорошо известен 11-летний цикл солнечной активности, хотя на самом деле длительность этого цикла варьирует). сезонная компонента (\\(S_t\\)): кратковременные периодические изменения, обладающие фиксированной частотой (например, суточные изменения количества солнечного света, падающего на единицу поверхности Земли). нерегулярная компонента (\\(\\epsilon_t\\)): эффекты случайных факторов (“шум”). Хотя функциональная связь между перечисленными компонентами может принимать практически любую форму, обычно рассматривают зависимости следующих двух видов: аддитивная модель: \\(y_t = T_t + C_t + S_t + \\epsilon_t\\); мультипликативная модель: \\(y_t = T_t \\times C_t \\times S_t \\times \\epsilon_t\\). Часто длина временного ряда оказывается недостаточной для выделения циклической составляющей, в связи чем \\(C_t\\) исключают из рассмотрения. Аддитивную модель применяют к т.н. стационарным временным рядам, в которых среднее значение и дисперсия \\(y_t\\) примерно постоянны для всех \\(t\\). Мультипликативная же модель лучше подходит для описания нестационарных рядов, в которых обычно имеют место выраженный тренд и возрастание дисперсии \\(y_t\\) во времени. На рис. 1.1 приведены примеры временных рядов, свойства которых иллюстрируют описанные выше понятия. Дополнительные понятия и термины мы будем вводить по мере необходимости в ходе рассмотрения соответствующих методов анализа. РИСУНОК 1.1: (А): Месячные продажи новых домов в США демонстрируют заметную годовую сезонность, а также более долговременную цикличность с периодом в 6–10 лет. В этих данных нет выраженного тренда. (B): Количество фьючерных контрактов Департамента казначейства США, проданных на Чикагской фондовой бирже в течение 100 дней в 1981 г. В этом временном ряду отсутствуют сезонные колебания, но имеет место тренд на снижение. (С): Квартальные объемы выработки электроэнергии в Австралии демонстрируют четко выраженные тренд и сезонность. Кроме того, дисперсия данных возрастает во времени, что указывает на мультипликативность породившего их процесса. (D): В дневных изменениях цены акций Google на момент закрытия торгов нет ни тренда, ни сезонности, ни цикличности. Источник: Hyndman and Athanasopoulos (2019) 1.4 Формат данных tsibble Анализ временных рядов в R часто сопровождается приличной “головной болью”, что связано с необходимостью представления данных в виде объектов таких традиционных классов, как ts, zoo или xts. К сожалению, эти классы плохо подходят для современных наборов данных, которые часто характеризуются нерегулярной регистрацией набюдений во времени, наличием нескольких переменных разных типов, нескольких группирующих переменных и т.п. Кроме того, традиционные форматы представления временных рядов противоречат принципам организации и хранения “опрятных данных” (“tidy data”) и, как следствие, затрудняют анализ и моделирование с помощью широко используемых сегодня инструментов из группы tidyverse. Для решения этих проблем группа исследователей под руководством проф. Роба Хиндмана (Rob Hyndman) разработала новый формат, реализованный в пакете tsibble (Wang, Cook, and Hyndman 2020). Именно этот формат мы будем использовать для представления данных в большинстве рассматриваемых ниже примеров. Он характеризуется следующими свойствами: данные хранятся в табличном виде; в таблице должны присутствовать как минимум два столбца — со значениями наблюдаемой во времени количественной переменной и с упорядоченными по возрастанию (т.е. от прошлого к будущему) временными отметками (столбец с временными отметками называется индексирующим — index); кроме того, в таблицу могут входить одна или несколько группирующих переменных (key) — значения этих переменных указывают на принадлежность каждого наблюдения к соответствующему временному ряду; любое наблюдение в таблице можно уникально идентифицировать по сочетанию значений индексирующей и группирующих переменных. Для создания объектов класса tsibble служит функция as_tsibble(), которая имеет следующие аргументы: x — объект с данными, подлежащий преобразованию в объект класса tsibble (это может быть, например, числовой вектор, матрица, таблица с данными (data.frame или tibble) и др.). index — переменная с временными отметками (указывается без кавычек). Допускается использование временных отметок на шкале от наносекунд до года. key — одна или несколько группирующих, или ключевых, переменных, которые уникально определяют каждый хранящийся в таблице временной ряд. Названия переменных указываются без кавычек и объединяются с помощью функции конкатенации c(). По умолчанию этот аргумент равен NULL, т.е. предполагается, что группирующих переменных в таблице нет. regular — логический аргумент, который указывает на регулярность учета хранящихся в таблице наблюдений. Значение TRUE (принято по умолчанию) предполагает, что учет выполнялся с одинаковым интервалом (например, каждую минуту, час, день, и т.п.). validate — логический аргумент (по умолчанию равен TRUE), позволяющий выполнить проверку уникальности каждого наблюдения по сочетанию значений переменных index и key. Если вы уверены, что каждое наблюдение уникально, то такую проверку можно отключить (FALSE) — это приведен к более быстрому выполнению команды as_tsibble() в случае с большими наборами данных. .drop — логический аргумент, позволяющий исключить из таблицы “пустые” временные ряды, т.е. такие сочетания значений группирующих переменных, для которых значения x отсутствуют. Примеры работы с функцией as_tsibble() приведены в следующем разделе. 1.5 Данные, используемые в примерах Для воспроизведения рассматриваемых в этой книге примеров необходимо скопировать содержимое Github–репозитория ranalytics/tsa-r любым удобным для вас способом и сделать корневую директорию скопированного проекта рабочей директорией R. Используемые в примерах четыре набора данных хранятся в папке data этого проекта. Ниже приведено их краткое описание. 1.5.1 Стоимость 22 криптовалют Набор данных cryptos содержит собранные с сайта CoinMarketCap значения стоимости 22 криптовалют на момент закрытия торгов. Эти данные охватывают период с 1 января 2018 г. по 6 декабря 2019 г. и имеют очень простую структуру: require(readr) require(tibble) cryptos &lt;- read_csv(&quot;data/cryptos_price.csv&quot;) glimpse(cryptos, width = 60) ## Observations: 15,510 ## Variables: 3 ## $ y &lt;dbl&gt; 7547.00, 7448.31, 7252.03, 7320.15, 7321.9... ## $ ds &lt;date&gt; 2019-12-06, 2019-12-05, 2019-12-04, 2019-... ## $ coin &lt;chr&gt; &quot;bitcoin&quot;, &quot;bitcoin&quot;, &quot;bitcoin&quot;, &quot;bitcoin&quot;... Переменная y — это стоимость криптовалюты coin (в долларах США), отмеченная в день ds. Все 22 временных ряда изображены на рис. 1.2. require(dplyr) require(ggrepel) require(ggplot2) cryptos %&gt;% group_by(coin) %&gt;% mutate(label = ifelse(ds == max(ds), coin, NA)) %&gt;% ggplot(., aes(ds, y, group = coin)) + geom_line() + geom_text_repel(aes(label = label), size = 3, nudge_x = 50, segment.size = 0.4, segment.color = &quot;gray60&quot;, point.padding = 0.2, force = 5, na.rm = TRUE) + scale_y_log10() + theme_minimal() + xlim(c(as.Date(&quot;2018-01-01&quot;), as.Date(&quot;2020-06-01&quot;))) РИСУНОК 1.2: Динамика стоимости 22 криптовалют. Источник: CoinMarketCap Заметьте, что стоимость одной из криптовалют (tether) на протяжении всего исследованного периода была настолько низковолатильной, что ее временной ряд на рис. 1.2 выглядит почти как сплошная горизонтальная линия. Воспользуемся функцией as_tibble() для преобразования таблицы cryptos в объект класса tsibble: require(tsibble) (cryptos &lt;- as_tsibble(cryptos, key = coin, index = ds)) ## # A tsibble: 15,510 x 3 [1D] ## # Key: coin [22] ## y ds coin ## &lt;dbl&gt; &lt;date&gt; &lt;chr&gt; ## 1 74.5 2018-01-01 augur ## 2 79.5 2018-01-02 augur ## 3 77.5 2018-01-03 augur ## 4 73.7 2018-01-04 augur ## 5 72.8 2018-01-05 augur ## 6 77.5 2018-01-06 augur ## 7 80.2 2018-01-07 augur ## 8 98.1 2018-01-08 augur ## 9 91.9 2018-01-09 augur ## 10 103. 2018-01-10 augur ## # ... with 15,500 more rows Если внимательно присмотреться, то можно увидеть некоторые внешние отличия полученной таблицы от исходной cryptos: R теперь “знает”, что таблица cryptos является объектом класса tsibble с 15510 наблюдениями, учтенными с дневным интервалом (см. комментарий # A tsibble: 15,510 x 3 [1D]). у таблицы cryptos есть группирующая переменная coin с 22 уровнями (см. # Key: coin [22]). Все наблюдения каждого временного ряда упорядочены по возрастанию значений индексирующей переменной ds. Однако это всего лишь внешние различия. Гораздо важнее то, что к данным из таблицы cryptos теперь можно применять методы анализа, специфичные для временных рядов, и делать это обычным для инструментов tidyverse образом. Аналогичные преобразования в формат tsibble будут выполнены нами ниже и для других наборов данных, используемых в примерах из этой книги. 1.5.2 Стоимость биткоина Набор данных bitcoin содержит собранные с сайта CoinMarketCap значения стоимости биткоина на момент закрытия торгов в период с 1 января 2016 г. по 24 августа 2019 г. bitcoin &lt;- read_csv(&quot;data/bitcoin_price.csv&quot;) %&gt;% as_tsibble(., key = NULL, index = ds) glimpse(bitcoin, width = 60) ## Observations: 1,332 ## Variables: 2 ## $ y &lt;dbl&gt; 434.33, 433.44, 430.01, 433.09, 431.96, 429.... ## $ ds &lt;date&gt; 2016-01-01, 2016-01-02, 2016-01-03, 2016-01... Переменная y — это стоимость биткоина (в долларах США), отмеченная в день ds. Заметьте, что в отличие от рассмотренного выше набора данных cryptos, таблица bitcoin содержит только один временной ряд. Поэтому при преобразовании этой таблицы в формат tsibble аргументу key было присвоено значение NULL — таким образом мы сообщили программе, что в таблице нет группирующих переменных. Динамика стоимости биткоина в рассматриваемый период времени изображена на рис. 1.3. bitcoin %&gt;% ggplot(., aes(ds, y)) + geom_line() + scale_y_log10() + theme_minimal() РИСУНОК 1.3: Динамика стоимости биткоина. Источник: CoinMarketCap 1.5.3 Цена акций трех компаний Набор данных shares содержит значения цены акций компаний Amazon, Facebook и Google на момент закрытия торгов в период с 1 января 2016 г. по 26 мая 2019 г. Эти данные были собраны с сайта Yahoo Finance и по своей структуре напоминают описанную выше таблицу cryptos: shares &lt;- read_csv(&quot;data/shares_price.csv&quot;) %&gt;% as_tsibble(., key = share, index = ds) glimpse(shares, width = 60) ## Observations: 3,726 ## Variables: 3 ## Key: share [3] ## $ ds &lt;date&gt; 2016-01-01, 2016-01-02, 2016-01-03, 2016... ## $ share &lt;chr&gt; &quot;amzn&quot;, &quot;amzn&quot;, &quot;amzn&quot;, &quot;amzn&quot;, &quot;amzn&quot;, &quot;... ## $ price &lt;dbl&gt; NA, NA, NA, 636.99, 633.79, 632.65, 607.9... Переменная price — это цена (в долларах США) акции share, отмеченная в день ds. Обратите внимание на наличие пропущенных значений в переменной price, связанное с прекращением торгов в выходные дни и во время государственных праздников США. Эти пропущенные значения соответствуют многочисленным “пробелам” во временных рядах, изображенных на рис. 1.4. shares %&gt;% ggplot(., aes(ds, price)) + geom_line() + scale_y_log10() + facet_wrap(~share, ncol = 1, scales = &quot;free_y&quot;) + theme_minimal() РИСУНОК 1.4: Динамика цены акций компаний Amazon, Facebook и Google. Источник: Yahoo Finance 1.5.4 Стоимость номеров в трех гостиницах Набор данных hotels содержит значения суточной стоимости номеров в трех гостиницах за период c 1 ноября 2012 г. по 30 июня 2013 г. Эта таблица является частью гораздо большего набора данных, предоставленного компанией Expedia в рамках одного из Kaggle–соревнований. В таблицу входят три переменные: prop_id (идентификатор гостиницы), date_time (время, когда пользователь увидел цену на соответствующий гостиничный номер в результате онлайн-поиска) и price_usd (цена в долларах США): hotels &lt;- read_csv(&quot;data/hotels_price.csv&quot;) %&gt;% as_tsibble(., key = prop_id, index = date_time) glimpse(hotels, width = 60) ## Observations: 1,647 ## Variables: 3 ## Key: prop_id [3] ## $ prop_id &lt;dbl&gt; 13252, 13252, 13252, 13252, 13252, 13... ## $ date_time &lt;dttm&gt; 2012-11-01 10:16:16, 2012-11-02 17:5... ## $ price_usd &lt;dbl&gt; 184.00, 203.00, 203.00, 186.00, 169.0... Динамика стоимости номеров в рассматриваемых трех гостиницах изображена на рис. 1.5. hotels %&gt;% ggplot(., aes(date_time, price_usd)) + geom_line() + facet_wrap(~prop_id) + theme_minimal() РИСУНОК 1.5: Динамика суточной стоимости номеров в трех гостиницах. Источник: Expedia "],
["ch-missing-values.html", "ГЛАВА 2 Обработка пропущенных наблюдений", " ГЛАВА 2 Обработка пропущенных наблюдений Очень часто на практике приходится сталкиваться с пропущенными наблюдениями во временных рядах. Это может стать проблемой при использовании многих методов анализа и моделирования, которые предполагают наличие полных наборов упорядоченных во времени наблюдений. В пакете tsibble (разд. 1.4) есть несколько функций, позволяющих выполнить диагностику данных на наличие пропущенных наблюдений, а также восстановить такие наблюдения с помощью любой подходящяей ситуации логики. Для иллюстрации работы с этими функциями мы воспользуемся таблицей shares, которая содержит данные по цене акций компаний Amazon, Facebook и Google (подразд. 1.5.3). Для начала применим функцию has_gaps(), чтобы узнать о наличии хотя бы одного пропущенного наблюдения в каждом из анализируемых временных рядов: require(dplyr) require(tsibble) has_gaps(shares) ## # A tibble: 3 x 2 ## share .gaps ## &lt;chr&gt; &lt;lgl&gt; ## 1 amzn FALSE ## 2 fb FALSE ## 3 goog FALSE Согласно полученному результату, пропущенных наблюдений нет ни в одном из рядов (все значения в столбце .gaps равны FALSE). Как же так получилось? Ведь мы знаем, что в таблице shares отсутствует большое количество наблюдений! Дело в том, что с “точки зрения” функции has_gaps() “пробелов” в данных действительно нет, поскольку все отсутствующие наблюдения в явном виде представлены с помощью значений NA (кстати, это хорошая практика). Посмотрим, что произойдет, если мы удалим все NA (после этой операции таблицу нужно заново преобразовать в формат tsibble): shares_na_dropped &lt;- shares %&gt;% na.omit() %&gt;% as_tsibble(key = share, index = ds) has_gaps(shares_na_dropped) ## # A tibble: 3 x 2 ## share .gaps ## &lt;chr&gt; &lt;lgl&gt; ## 1 amzn TRUE ## 2 fb TRUE ## 3 goog TRUE Теперь, как видим, функция has_gaps() успешно обнаружила наличие пропущенных наблюдений для некоторых дат. С помощью другой функции — scan_gaps() — можно выяснить, какие именно наблюдения отсутствуют в каждом из рядов: scan_gaps(shares_na_dropped) ## # A tsibble: 1,149 x 2 [1D] ## # Key: share [3] ## share ds ## &lt;chr&gt; &lt;date&gt; ## 1 amzn 2016-01-09 ## 2 amzn 2016-01-10 ## 3 amzn 2016-01-16 ## 4 amzn 2016-01-17 ## 5 amzn 2016-01-18 ## 6 amzn 2016-01-23 ## 7 amzn 2016-01-24 ## 8 amzn 2016-01-30 ## 9 amzn 2016-01-31 ## 10 amzn 2016-02-06 ## # ... with 1,139 more rows Строго говоря, полученный с помощью scan_gaps() результат не совсем верен. Например, для акций компании Amazon эта функция “решила”, что первое пропущенное наблюдение приходится на 9 января 2016 г., тогда как в действительности временной ряд amzn начинается с 1 января 2016 г. и в нем пропущены несколько первых наблюдений (см. подразд. 1.5.3). Впрочем, такой результат вполне логичен, поскольку в ходе создания таблицы shares_na_dropped пропущенные наблюдения в начале ряда были удалены и функция scan_gaps() просто ничего об этом “не знала”. Функция count_gaps() дает более развернутый отчет по отсутствующим наблюдениям. В частности, она определяет длину каждого “пробела” в каждом временном ряду: (gaps &lt;- count_gaps(shares_na_dropped)) ## # A tibble: 552 x 4 ## share .from .to .n ## &lt;chr&gt; &lt;date&gt; &lt;date&gt; &lt;int&gt; ## 1 amzn 2016-01-09 2016-01-10 2 ## 2 amzn 2016-01-16 2016-01-18 3 ## 3 amzn 2016-01-23 2016-01-24 2 ## 4 amzn 2016-01-30 2016-01-31 2 ## 5 amzn 2016-02-06 2016-02-07 2 ## 6 amzn 2016-02-13 2016-02-15 3 ## 7 amzn 2016-02-20 2016-02-21 2 ## 8 amzn 2016-02-27 2016-02-28 2 ## 9 amzn 2016-03-05 2016-03-06 2 ## 10 amzn 2016-03-12 2016-03-13 2 ## # ... with 542 more rows Обнаруженные пробелы далее можно легко визуализировать с помощью пакета ggplot2 (рис. 2.1; во избежание плотного перекрытия точек показаны только первые шесть месяцев 2016 г.): gaps %&gt;% filter(.to &lt;= as.Date(&quot;2016-06-30&quot;)) %&gt;% ggplot(., aes(x = share)) + geom_linerange(aes(ymin = .from, ymax = .to)) + geom_point(aes(y = .from)) + geom_point(aes(y = .to)) + coord_flip() + theme_minimal() РИСУНОК 2.1: Визуализация пропущенных наблюдений во временных рядах Как отмечено выше, хорошей практикой является представление пропущенных наблюдений в явном виде с помощью NA. Для этого достаточно воспользоваться функцией fill_gaps() из пакета tsibble: shares_na_dropped %&gt;% fill_gaps() ## # A tsibble: 3,711 x 3 [1D] ## # Key: share [3] ## ds share price ## &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2016-01-04 amzn 637. ## 2 2016-01-05 amzn 634. ## 3 2016-01-06 amzn 633. ## 4 2016-01-07 amzn 608. ## 5 2016-01-08 amzn 607. ## 6 2016-01-09 amzn NA ## 7 2016-01-10 amzn NA ## 8 2016-01-11 amzn 618. ## 9 2016-01-12 amzn 618. ## 10 2016-01-13 amzn 582. ## # ... with 3,701 more rows К сожалению, многие реализованные в R статистические методы, включая методы моделирования временных рядов, не могут работать с данными, в которых есть NA–значения. Для решения этой проблемы можно попробовать восстановить пропущенные наблюдения тем или иным подходящим ситуации способом. Для восстановления NA–значений во временных рядах amzn, fb и goog в таблице shares мы воспользуемся простой и хорошо подходящей для этого случая стратегией LOCF (“last observation carried forward”), которая заключается в том, что каждое пропущенное значение заменяется на последнее предшествующее ему непропущенное значение (или на ближайшее следующее за ним непропущенное значение, если ряд начинается с NA, как в наших данных). Одну из реализаций метода LOCF можно найти в функции na_locf() из пакета imputeTS. В приведенном ниже коде мы сначала примененяем функцию group_by_key() из пакета tsibble для группирования данных по ключевой переменной (share), а затем вызываем функции mutate() (из пакета dplyr) и na_locf() для восстановления пропущенных наблюдений в пределах каждого временного ряда (рис. 2.2): require(imputeTS) shares_na_filled &lt;- shares %&gt;% group_by_key() %&gt;% mutate(price = na_locf(price)) shares_na_filled %&gt;% ggplot(., aes(ds, price, color = share)) + geom_line() + scale_y_log10() + theme_minimal() РИСУНОК 2.2: Данные по цене акций трех компаний после восстановления пропущенных наблюдений с помощью метода LOCF (сравните с рис. 1.4) "],
["ch-aggregation.html", "ГЛАВА 3 Агрегирование наблюдений", " ГЛАВА 3 Агрегирование наблюдений Обычной задачей при работе с временными рядам является расчет агрегированных показателей за определенный календарный период (например, средние или суммарные значения за день, неделю, месяц, квартал и т.п.). В пакете tsibble (разд. 1.4) для таких вычислений используется связка из двух функций: index_by() и summarise(). Первая из этих функций входит в состав пакета tsibble и аналогична group_by() из пакета dplyr. Как следует из ее названия, index_by() группирует данные по заданному пользователем периоду времени. Для указания необходимого периода применяются такие функции из пакета tsibble, как yearweek() (неделя), yearmonth() (месяц) и yearquarter() (квартал). Кроме того, можно также использовать базовую as.Date() и многие функции из пакета lubridate (Grolemund and Wickham 2011). Вторая функция из указанной выше связки — summarise() — входит в состав пакета dplyr и служит для вычисления необходимых агрегированных величин. Воспользуемся данными cryptos (подразд. 1.5.1) и в качестве примера рассчитаем среднемесячную стоимость каждой криптовалюты, а также общее число наблюдений, учтенных в каждом месяце исследованного периода: require(dplyr) monthly &lt;- cryptos %&gt;% group_by_key() %&gt;% index_by(year_month = ~ yearmonth(.)) %&gt;% summarise( avg_y = mean(y), n = n() ) monthly ## # A tsibble: 528 x 4 [1M] ## # Key: coin [22] ## coin year_month avg_y n ## &lt;chr&gt; &lt;mth&gt; &lt;dbl&gt; &lt;int&gt; ## 1 augur 2018 Jan 84.1 31 ## 2 augur 2018 Feb 51.1 28 ## 3 augur 2018 Mar 35.9 31 ## 4 augur 2018 Apr 32.5 30 ## 5 augur 2018 May 45.8 31 ## 6 augur 2018 Jun 34.5 30 ## 7 augur 2018 Jul 31.9 31 ## 8 augur 2018 Aug 21.7 31 ## 9 augur 2018 Sep 14.7 30 ## 10 augur 2018 Oct 13.1 31 ## # ... with 518 more rows Заметьте, что перед вычислением агрегированных показателей мы сначала сгруппировали данные по каждой криптовалюте с помощью функции group_by_key(). Поскольку в этом наборе данных есть только одна группирующая переменная (coin) и она была задана при создании таблицы cryptos, то при вызове group_by_key() не было необходимости указывать эту группирующую переменную в явном виде. Результат вычисления среднемесячной стоимости анализируемых криптовалют показан на рис. 3.1: require(ggrepel) monthly %&gt;% mutate(label = ifelse(year_month == max(year_month), coin, NA)) %&gt;% ggplot(., aes(year_month, avg_y, group = coin)) + geom_line() + scale_y_log10() + geom_text_repel(aes(label = label), size = 3, nudge_x = 50, segment.size = 0.4, segment.color = &quot;gray60&quot;, point.padding = 0.2, force = 4, na.rm = TRUE) + xlim(c(yearmonth(&quot;2018-01&quot;), yearmonth(&quot;2020-06&quot;))) + theme_minimal() РИСУНОК 3.1: Динамика среднемесячной стоимости 22 криптовалют из таблицы cryptos Еще одной распространенной задачей при работе с временными рядами является расчет агрегированных показателей в пределах скользящего блока (“окна”) данных. В пакете tsibble для таких вычислений есть несколько функций со следующими приставками в названии: slide_ — выполняют вычисления в пределах перекрывающихся окон размером .size (размер “нахлеста” контролируется с помощью аргумента .step); tile_ — вычисления ведутся в пределах следующих друг за другом, но неперекрывающихся окон размером .size; stretch_ — исходная ширина окна .init постепенно увеличивается на .step временных отметок. В зависимости от типа возвращаемых значений названия перечисленных выше функций заканчиваются на lgl (логические), int (целые числа), dbl (действительные числа), или chr (строковые значения). Например, при работе с действительными числами агрегирование по скользящему окну с перекрывающимися наблюдениями выполняется с помощью функции slide_dbl(). В качестве примера вычислим скользящую среднюю стоимость каждой из 22 криптовалют в пределах окна шириной 14 дней, которое каждый раз передвигается на одну временную отметку вперед: cryptos_ma &lt;- cryptos %&gt;% group_by_key() %&gt;% mutate(moving_avg_y = slide_dbl(y, ~mean(.), .size = 14, .step = 1)) На рис. 3.2 показаны результы вычисления скользящей средней для двух случайно выбранных криптовалют: set.seed(1984) cryptos_ma %&gt;% ungroup() %&gt;% filter(coin %in% sample(unique(coin), 2)) %&gt;% ggplot(aes(ds, y)) + geom_point(alpha = 0.1) + geom_line(aes(ds, moving_avg_y), col = &quot;blue&quot;) + facet_wrap(~coin, ncol = 1) + scale_y_log10() + theme_minimal() РИСУНОК 3.2: Скользящие средние стоимости двух криптовалют (синие линии), рассчитанные по окну шириной 14 дней "],
["ch-visualisation.html", "ГЛАВА 4 Визуализация временных рядов и их свойств", " ГЛАВА 4 Визуализация временных рядов и их свойств Анализ временных рядов, равно как и данных других типов, практически невозможно представить себе без их визуализации в том или ином виде и многочисленные примеры графического представления временных рядов (преимущественно средствами пакета ggplot2) можно встретить на протяжении всей книги. В этой небольшой главе показаны некоторые дополнительные приемы, которые могут оказаться полезными в ходе разведочного анализа и подготовки данных к моделированию. Поскольку эти приемы являются стандартными, то их готовые реализации можно найти в нескольких пакетах для R. Мы воспользуемся одним из таких пакетов — feasts (от “feature extraction and statistics for time series”), который представляет собой коллекцию функций для визуализации временных рядов и расчета многочисленных показателей, обобщающих их свойства (гл. 5). Пакет feasts разработан в соответствии с принципами организации и анализа “опрятных данных”, что помимо простого и удобного синтаксиса команд делает его масштабируемым для одновременной обработки большого количества временных рядов, хранящихся в таблицах формата tsibble (разд. 1.4), В примерах будем использовать данные по стоимости гостиничных номеров из таблицы hotels, которые мы сначала агрегируем (гл. 3) до среднедневных значений (на логарифмической шкале), а затем восстановим пропущенные наблюдения (гл. 2) путем простой линейной интерполяции (c помощью функции na.interp() из пакета forecast): require(tsibble) require(dplyr) hotels_daily &lt;- hotels %&gt;% group_by_key() %&gt;% index_by(dt = as.Date(date_time)) %&gt;% summarise(y = mean(log(price_usd))) %&gt;% fill_gaps() %&gt;% mutate(y = forecast::na.interp(y)) Полученные временные ряды можно легко представить на графике с помощью функции autoplot() из пакета feasts, которая автоматически распознает наличие группирующей переменной в данных (prop_id) и изобразит три отдельные линии для каждой гостиницы: require(feasts) hotels_daily %&gt;% autoplot(y) РИСУНОК 4.1: Результат применения функции feasts::autoplot() к таблице hotels_daily, которая является объектом класса tsibble с одной группирующей переменной (prop_id) Функция autoplot() очень удобна для быстрого разведочного анализа данных: для построения графика на рис. 4.1 потребовалась всего одна строка кода. По внешнему виду полученного графика можно легко догадаться что он представляет собой результат работы пакета ggplot2. Функция autoplot(), а также другие функции из пакета feasts, выполняющие визуализацию данных — это всего лишь “обертки” (wrappers), которые автоматически вызывают необходимые функции ggplot2 в соответствии с классом и структурой подаваемых на них объектов. Из этого, в частности, следует, что графические функции из пакета feasts можно использовать в сочетании со многими функциями ggplot2 для тонкой настройки внешнего вида графиков. Ниже мы воспользуемся этим обстоятельством и будем применять графические функции feasts в сочетании с theme_minimal() для изменения шаблона графиков со стандартного “серого” на предпочитаемый в этой книге “минимальный”. Как было отмечено в разд. 1.3, многие (но далеко не все!) временные ряды в общем виде можно представить как сумму трех компонент: тренда, сезонных изменений и шума (колебаний, связанных со случайными факторами). Хорошее понимание сруктуры временного ряда является важным условием для построения надежных прогнозных моделей. К счастью, наше зрение устроено так, что часто мы без труда можем выделить тренд и сезонные изменения, просто глядя на график временного ряда. Однако при наличии слабого тренда и большого “шума” в данных, сделать это бывает труднее. В таких случаях может помочь декомпозиция временного ряда (time series decomposition) — прием, позволяющий автоматически разложить временной ряд на отдельные компоненты, которые далее можно изобразить графически. Существует несколько методов декомпозиции временных рядов. Одним из наиболее широко используемых является разложение на тренд и сезонную составляющую с помощью локальной полиномиальной регрессии (Seasonal and Trend decomposition using Loess, STL; Cleveland et al. (1990)). Проще всего с этим методом можно разобраться, применив его к конкретным данным. Для примера воспользуемся данными по гостинице 73738 из созданной нами ранее таблицы hotels_daily (рис. 4.2): hotels_daily %&gt;% filter(prop_id == 73738) %&gt;% model(STL(y ~ trend(window = 30) + season(period = &quot;week&quot;))) %&gt;% components() %&gt;% autoplot() + theme_minimal() РИСУНОК 4.2: Результат применения метода STL для декомпозиции одного из временных рядов из таблицы hotels_daily В приведенном коде использованы не встречавшиеся нам ранее функции из нескольких пакетов: model() и components() из пакета fabletools, который устанавливается и автоматически загружается одновременно с feasts, и STL() из пакета feasts. Для спецификации формулы STL–модели использованы также особые функции trend() и season() (подробнее см. справочный файл, доступный по команде ?STL). Аргумент window функции trend() задает количество наблюдений в скользящем окне, используемом для оценивания тренда (в данном случае 30; чем выше это число, тем более гладкой окажется в итоге кривая тренда). Аргумент period функции season() используется для указания периода сезонных колебаний (в данном случае одна неделя). На рис. 4.2 выделенные с помощью метода STL компоненты временного ряда показаны на трех нижних графиках (trend — тренд, season_week — компонента недельной сезонности, remainder — остатки). Если их просуммировать, то получим исходный ряд y (приведен на верхнем графике). Серые вертикальные столбцы, изображенные слева от графиков, отражают удельный вклад каждой компоненты в общую дисперсию в данных. Чтобы лучше понять, что это значит, примем длину столбца на графике с исходными данными за единицу дисперсии. Длина столбца на графике тренда примерно в четыре раза больше, что указывает на незначительный вклад тренда в общую дисперсию. Другими словами, если бы мы сжали шкалу тренда настолько, чтобы длина его столбца стала равной длине столбца исходных данных, то дисперсия значений тренда оказалась бы довольно низкой. Аналогичный вывод можно сделать и в отношении вклада сезонной компоненты (который, что интересно, заметно варьирует во времени). В то же время длина столбца на графике остатков сравнима с длиной столбца на графике с исходными данными, что указывает на значительный вклад случайных факторов в общую дисперсию. На рис. 4.2 приведен результат декомпозиции одного временного ряда, однако мы могли бы выполнить такую декомпозицию сразу для всех рядов, хранящихся в таблице hotels_daily (рис. 4.3): hotels_daily %&gt;% group_by_key() %&gt;% model(STL(y ~ trend(window = 30) + season(period = &quot;week&quot;))) %&gt;% components() %&gt;% autoplot() + theme_minimal() + theme(legend.position = &quot;bottom&quot;, legend.direction = &quot;vertical&quot;) РИСУНОК 4.3: Результат применения метода STL для декомпозиции всех трех временных рядов из таблицы hotels_daily Более детальное изучение сезонных колебаний в анализируемых временных рядах можно выполнить также с помощью функции gg_season() из пакета feasts(). В приведенном ниже примере эта функция применена для изображения динамики среднедневных значений стоимости номеров в каждой гостинице в течение каждой недели исследованного периода времени. На рис. 4.4 хорошо видно отсутствие каких–либо выраженных колебаний стоимости гостиничных номеров в течение недели, что согласуется с полученными ранее результами декомпозиции этих временных рядов: hotels_daily %&gt;% index_by(w = yearweek(dt)) %&gt;% gg_season(y, period = &quot;week&quot;) + theme_minimal() РИСУНОК 4.4: Динамика среднедневной стоимости номеров в гостиницах из таблицы hotels_daily в течение каждой недели исследованного периода времени Важным свойством многих временных рядов является наличие автокорреляции в их значениях, т.е. связи между значениями одного ряда, взятыми с определенным сдвигом (“лагом”, или “задержкой”). Наличие автокорреляции в данных является важной информацией для построения надежных прогнозных моделей. Выявление автокорреляции в остатках таких моделей также важно для диагностики их качества (в надежных моделях такой корреляции в остатках быть не должно). Визуально корреляцию между значениями одного временного ряда на разных сдвигах (по умолчанию от 1 до 9) можно представить с помощью функции gg_lag() из пакета feasts(): hotels_daily %&gt;% filter(prop_id == 73738) %&gt;% gg_lag(geom = &quot;point&quot;) + theme_minimal() РИСУНОК 4.5: Связь между значениями временного ряда 73738 из таблицы hotels_daily его копиями, сдвинутыми на несколько временных отметок (от 1 до 9) На рис. 4.5 видна умеренная связь между значениями временного ряда гостиницы 73738 на всех девяти сдвигах. Существует несколько способов, позволяющих выразить степень связи между временным рядом и его сдвинутыми копиями (т.е. автокорреляционную функцию) количественно. В простейшем случае для этого рассчитывают обычный линейный коэффициент корреляции: \\[r_k = \\frac{\\sum_{t=k+1}^T (y_t - \\bar{y})(y_{t-k} - \\bar{y})}{\\sum_{t=1}^T (y_t = \\bar{y})^2},\\] где \\(T\\) — это длина временного ряда. В пакете feasts для расчета автокорреляционной функции служит команда ACF(). По умолчанию ACF() выполняет вычисления для сдвигов от 1 до 23, однако максимальный сдвиг можно изменить с помощью аргумента lag_max: hotels_daily %&gt;% filter(prop_id == 73738) %&gt;% ACF(lag_max = 6) ## # A tsibble: 6 x 3 [1D] ## # Key: prop_id [1] ## prop_id lag acf ## &lt;dbl&gt; &lt;lag&gt; &lt;dbl&gt; ## 1 73738 1D 0.350 ## 2 73738 2D 0.315 ## 3 73738 3D 0.234 ## 4 73738 4D 0.242 ## 5 73738 5D 0.265 ## 6 73738 6D 0.315 Полученные коэффициенты корреляции принято изображать в виде графика, который называют коррелограммой; рис. 4.6): hotels_daily %&gt;% filter(prop_id == 73738) %&gt;% ACF() %&gt;% autoplot() + theme_minimal() РИСУНОК 4.6: Коррелограмма временного ряда 73738 из таблицы hotels_daily для сдвигов от 1 до 23 На полученном графике видно, что все коэффициенты автокорреляции являются умеренно положительными, указывая на наличие во временном ряду тренда на возрастание. Кроме того, можно видеть небольшие “всплески” коэффициентов, видимо обусловленные (слабо выраженной) недельной сезонностью. Синие пунктирные линии соответствуют критическим значениям корреляции (на уровне значимости 0.05). Если полученные коэффициенты по модулю превышают эти критические значения (как в нашем случае), то соответствующие корреляции можно считать значимо отличающимися от нуля. На этом мы завершим рассмотрение способов графического представления временных рядов и их свойств. С некоторыми дополнительными приемами визуализации можно познакомиться в великолепной книге Hyndman and Athanasopoulos (2019). "],
["ch-feature-extraction.html", "ГЛАВА 5 Извлечение признаков с помощью пакета feasts 5.1 Функция features() 5.2 Встроенные функции для расчета признаков 5.3 Примеры использования извлеченных признаков", " ГЛАВА 5 Извлечение признаков с помощью пакета feasts Подобно свойствам другим совокупностей, мы можем компактно охарактеризовать свойства временных рядов с помощью стандартных показателей описательной статистики (среднее значение, медиана, дисперсия, стандартное отклонение, квантили и т.п.). Поскольку наблюдения во временных рядах упорядочены, то для них можно рассчитать также и многие специфичные показатели, такие как автокорреляция (гл. 4), выраженность тренда и сезонной компоненты, стационарность ряда и т.п. Помимо того, что описательные статистики временных рядов представляют интерес сами по себе, их используют также в качестве признаков, или предикторов, при создании предсказательных моделей и в других задачах, решаемых с помощью методов машинного обучения (кластеризация, выявление аномалий и т.п.). Извлечение таких признаков из временных рядов с помощью R не составляет труда — для этого вполне подойдут как базовые функции, так и функции из многочисленных дополнительных пакетов. Однако учитывая то, как часто возникает необходимость в подобных вычислениях, в идеале было бы удобно иметь один–два пакета, в которых реализован соответствующий функционал. К счастью, такие пакеты для R есть. С одним из них — feasts — мы уже познакомились в гл. 4. Ниже будут рассмотрены возможности feasts по извлечению признаков из временных рядов на примере данных по стоимости 22 криптовалют (подразд. 1.5.1). 5.1 Функция features() В пакете feasts извлечение признаков из временных рядов выполняется с помощью функции–менеджера features(), которая имеет следующие аргументы: .tbl — таблица с данными (например, объект класса data.frame, tibble, или tsibble); .var — имя переменной в таблице .tbl, по значениям которой необходимо вычислить те или иные признаки; features — список функций, которые непосредственно выполняют вычисления соответствующих признаков; ... — дополнительные аргументы, которые необходимо подать на функции, перечисленные в списке features. Важно отметить, что такие аргументы будут переданы на ту или иную функцию только, если они входят в состав формальных аргументов этой функции, а не являются ее дополнительными аргументами (...). Например, в случае с базовой функцией var(), указание аргумента na.rm = TRUE сработает, поскольку это формальный аргумент var(). В то же время в случае с базовой функцией mean() это не сработает, поскольку na.rm является ее дополнительным аргументом. Чтобы убедиться в том, что необходимые аргументы будут поданы на нужную функцию, ее следует добавить в список features в виде анонимной функции, как показано в следующем примере: require(feasts) # Неправильный способ подать дополнительный аргумент на функцию, # которая вычисляет некоторый признак (здесь - среднее значение) # при наличии пропущенных наблюдений (NA): ## features(dat_ts, y, features = list(mean), na.rm = TRUE) # Правильный способ (с использованием анонимной функции): features(cryptos, y, features = list(~ mean(., na.rm = TRUE))) ## # A tibble: 22 x 2 ## coin V1 ## &lt;chr&gt; &lt;dbl&gt; ## 1 augur 23.2 ## 2 bitcoin 7492. ## 3 cardano 0.136 ## 4 chainlink 0.938 ## 5 dash 223. ## 6 decred 41.8 ## 7 dogecoin 0.00350 ## 8 eos 6.08 ## 9 ethereum 340. ## 10 iota 0.776 ## # ... with 12 more rows Заметьте, что в последней команде нам не нужно было сначала сгруппировать данные по переменной coin — расчет средних значений для отдельных уровней этой переменной был выполнен автоматически. Такой результат стал возможным благодаря тому, что таблица dat_ts является объектом класса tsibble, в котором переменная coin служит “ключом”. Функции пакета feasts “знают” как работать с подобными объектами. Другой момент, на который стоит обратить внимание: результатом выполнения последней команды стала таблица, в которой рассчитанная новая переменная имеет мало о чем говорящее название V1. Чтобы изменить такое поведение feasts рекомендуется сначала сохранить определение анонимной функции в виде полноценной пользовательской функции. Эта функция должна возвращать вектор с одним или несколькими поименованными элементами — их имена в итоге станут именами соответствующих переменных в итоговой таблице. В приведенном ниже примере пользовательская функция my_func(x) возвращает вектор со значениями арифметического среднего и дисперсии x: my_func &lt;- function(x) { m &lt;- mean(x, na.rm = TRUE) v &lt;- var(x, na.rm = TRUE) return(c(avg = m, var = v)) } features(cryptos, y, features = list(~ my_func(.))) ## # A tibble: 22 x 3 ## coin avg var ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 augur 23.2 3.28e+2 ## 2 bitcoin 7492. 6.72e+6 ## 3 cardano 0.136 2.61e-2 ## 4 chainlink 0.938 7.71e-1 ## 5 dash 223. 4.66e+4 ## 6 decred 41.8 7.46e+2 ## 7 dogecoin 0.00350 3.46e-6 ## 8 eos 6.08 1.23e+1 ## 9 ethereum 340. 7.00e+4 ## 10 iota 0.776 5.82e-1 ## # ... with 12 more rows 5.2 Встроенные функции для расчета признаков Помимо подаваемых на features() пользовательских функций можно также воспользоваться целым рядом готовых функций, входящих в состав пакета feasts. Например, для расчета коэффициента Xёрста и спектральной энтропии ряда служат функции coef_hurst() и feat_spectral(): require(dplyr) require(tidyr) cryptos %&gt;% features(., y, list(coef_hurst, feat_spectral)) ## # A tibble: 22 x 3 ## coin coef_hurst spectral_entropy ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 augur 1.00 0.423 ## 2 bitcoin 1.00 0.444 ## 3 cardano 1.00 0.422 ## 4 chainlink 1.00 0.437 ## 5 dash 1.00 0.398 ## 6 decred 1.00 0.422 ## 7 dogecoin 1.00 0.535 ## 8 eos 1.00 0.494 ## 9 ethereum 1.00 0.407 ## 10 iota 1.00 0.417 ## # ... with 12 more rows Некоторые встроенные в feasts функции возвращают сразу несколько признаков. Например, feat_stl() выполняет декомпозицию временного ряда на отдельные компоненты (гл. 4) и далее на их основе вычисляет различные величины, включая выраженность тренда (trend strength) и сезонной составляющей (seasonal strength), линейность (linearity) и кривизну (curvature) ряда и т.д. (Hyndman and Athanasopoulos 2019; Kang, Hyndman, and Smith-Miles 2017): cryptos %&gt;% features(., y, list(feat_stl)) %&gt;% glimpse(width = 60) ## Observations: 22 ## Variables: 10 ## $ coin &lt;chr&gt; &quot;augur&quot;, &quot;bitcoin&quot;, &quot;car... ## $ trend_strength &lt;dbl&gt; 0.9907166, 0.9908439, 0.... ## $ seasonal_strength_week &lt;dbl&gt; 0.14638033, 0.21990784, ... ## $ seasonal_peak_week &lt;dbl&gt; 6, 6, 6, 6, 6, 6, 6, 6, ... ## $ seasonal_trough_week &lt;dbl&gt; 2, 2, 2, 2, 1, 4, 2, 2, ... ## $ spikiness &lt;dbl&gt; 4.733049e-04, 6.274527e+... ## $ linearity &lt;dbl&gt; -3.523179e+02, -4.067290... ## $ curvature &lt;dbl&gt; 2.204012e+02, 5.028015e+... ## $ stl_e_acf1 &lt;dbl&gt; 0.40641299, 0.29884561, ... ## $ stl_e_acf10 &lt;dbl&gt; 0.4719425, 0.2286323, 0.... На момент написания этой книги пакет feasts позволял рассчитать 48 признаков, характеризующих различные свойства временных рядов. Для одновременного расчета всех этих признаков можно применить вспомогательную функцию feature_set() из пакета fabletools (гл. 4) следующим образом: (all_crypto_features &lt;- cryptos %&gt;% features(., y, feature_set(pkgs = &quot;feasts&quot;))) ## # A tibble: 22 x 49 ## coin trend_strength seasonal_streng~ seasonal_peak_w~ seasonal_trough~ ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 augur 0.991 0.146 6 2 ## 2 bitc~ 0.991 0.220 6 2 ## 3 card~ 0.990 0.249 6 2 ## 4 chai~ 0.996 0.226 6 2 ## 5 dash 0.996 0.312 6 1 ## 6 decr~ 0.993 0.232 6 4 ## 7 doge~ 0.983 0.303 6 2 ## 8 eos 0.988 0.274 6 2 ## 9 ethe~ 0.995 0.159 6 4 ## 10 iota 0.995 0.228 6 2 ## # ... with 12 more rows, and 44 more variables: spikiness &lt;dbl&gt;, ## # linearity &lt;dbl&gt;, curvature &lt;dbl&gt;, stl_e_acf1 &lt;dbl&gt;, stl_e_acf10 &lt;dbl&gt;, ## # acf1 &lt;dbl&gt;, acf10 &lt;dbl&gt;, diff1_acf1 &lt;dbl&gt;, diff1_acf10 &lt;dbl&gt;, ## # diff2_acf1 &lt;dbl&gt;, diff2_acf10 &lt;dbl&gt;, season_acf1 &lt;dbl&gt;, pacf5 &lt;dbl&gt;, ## # diff1_pacf5 &lt;dbl&gt;, diff2_pacf5 &lt;dbl&gt;, season_pacf &lt;dbl&gt;, ## # zero_run_mean &lt;dbl&gt;, nonzero_squared_cv &lt;dbl&gt;, zero_start_prop &lt;dbl&gt;, ## # zero_end_prop &lt;dbl&gt;, lambda_guerrero &lt;dbl&gt;, kpss_stat &lt;dbl&gt;, ## # kpss_pvalue &lt;dbl&gt;, pp_stat &lt;dbl&gt;, pp_pvalue &lt;dbl&gt;, ndiffs &lt;int&gt;, ## # nsdiffs &lt;int&gt;, bp_stat &lt;dbl&gt;, bp_pvalue &lt;dbl&gt;, lb_stat &lt;dbl&gt;, ## # lb_pvalue &lt;dbl&gt;, var_tiled_var &lt;dbl&gt;, var_tiled_mean &lt;dbl&gt;, ## # shift_level_max &lt;dbl&gt;, shift_level_index &lt;dbl&gt;, shift_var_max &lt;dbl&gt;, ## # shift_var_index &lt;dbl&gt;, shift_kl_max &lt;dbl&gt;, shift_kl_index &lt;dbl&gt;, ## # spectral_entropy &lt;dbl&gt;, n_crossing_points &lt;int&gt;, n_flat_spots &lt;int&gt;, ## # coef_hurst &lt;dbl&gt;, stat_arch_lm &lt;dbl&gt; Функция feature_set() имеет два аргумента: pkgs — вектор с названиями пакетов, функции из которых вычисляют те или иные признаки, и tags — вектор с “метками”, обозначающими группы тематически сходных признаков. В приведенной выше команде мы запросили вычисление всех признаков, реализованных в пакете feasts (feature_set(pkgs = \"feasts\")). Однако часто более удобным и гибким подходом для формирования списка признаков будет использование аргумента tags. Например, команда feature_set(tags = c(\"stl\", \"spectral\")) приведет к вычислению всех признаков, возвращаемых функциями feat_stl() и feat_spectral() (см. выше). Список меток всех признаков, реализованных в пакете feasts, можно просмотреть с помощью команды ?features_by_tag. Этот список и краткие пояснения к нему приведены ниже (названия меток выделены жирным шрифтом; после названия каждой метки идут краткое пояснение и список функций, непосредственно выполняющих соответствующие вычисления): acf Набор признаков, рассчитываемых на основе коэффициентов автокорреляции. feat_acf() autocorrelation Набор признаков, рассчитываемых на основе коэффициентов автокорреляции и частных коэффициентов автокорреляции. feat_acf() feat_pacf() boxcox Оптимальное значение параметра \\(\\lambda\\) для преобразования Бокса–Кокса. guerrero() coefficients Коэффициент Хёрста. coef_hurst() count Количество раз, когда временной ряд пересекает свою собственную медиану, и максимальная длина временного отрезка, в пределах которого наблюдения имеют одинаковые значения: n_crossing_points() n_flat_spots() decomposition Признаки, рассчитываемые на основе компонент временного ряда. feat_stl() intermittent Признаки, отражающие наличие и структуру временных отрезков, в пределах которых все значения ряда равны нулю. feat_intermittent() lumpiness Дисперсия дисперсий, рассчитанных по наблюдениям из непересекающихся временных отрезков (окон). var_tiled_var() optimisation То же, что и метка boxcox выше, т.е. оптимальное значение параметра \\(\\lambda\\) для преобразования Бокса–Кокса. guerrero() pacf Набор признаков, рассчитываемых на основе частных коэффициентов автокорреляции. feat_pacf() portmanteau Статистики, рассчитываемые в результате применения тестов Льюнг–Бокса и Бокса–Пирса на наличие автокорреляции в данных. box_pierce() ljung_box() rle Максимальная длина временного отрезка, в пределах которого значения ряда принадлежат к одному из децилей. n_flat_spots() roll Признаки, рассчитываемые по данным из пересекающихся временных отрезков (окон), в частности максимальный сдвиг среднего уровня, дисперсии и расстояния Кульбака–Лейблера в следующих друг за другом отрезках. shift_level_max() shift_var_max() shift_kl_max() seasonal Признаки, рассчитываемые на основе компонент временного ряда, а также минимальное число дифференцирований ряда, необходимое для достижения его стационарности. feat_stl() unitroot_nsdiffs() slide То же, что и метка roll (см. выше). spectral Спектральная энтропия ряда. feat_spectral() stability Максимальный сдвиг среднего уровня, рассчитанного по данным из двух пересекающихся временных отрезков (окон). var_tiled_mean() stl Признаки, рассчитываемые на основе компонент временного ряда. feat_stl() test Статистики, рассчитываемые в ходе различных тестов (на наличие автокорреляции, единичного корня, авторегрессионной условной гетероскедастичности и др.). unitroot_kpss() unitroot_pp() unitroot_ndiffs() unitroot_nsdiffs() box_pierce() ljung_box() stat_arch_lm() tile Признаки, рассчитываемые по данным из непересекающихся временных отрезков (окон), в частности дисперсия дисперсий и дисперсия средних значений. var_tile_mean() var_tile_var() trend То же, что и метка stl. unitroot Статистики, рассчитываемые в ходе тестов на наличие в данных единичного корня, а также минимальное число дифференцирований ряда, необходимое для достижения его стационарности. unitroot_kpss() unitroot_pp() unitroot_ndiffs() unitroot_nsdiffs() Пользовательские функции тоже можно сделать “видимыми” для feature_set(). Для этого их необходимо зарегистрировать с помощью вспомогательной функции register_feature(): register_feature(my_func, tags = c(&quot;avg_and_var&quot;)) cryptos %&gt;% features(., y, feature_set(tags = &quot;avg_and_var&quot;)) %&gt;% tibble::glimpse(width = 60) ## Observations: 22 ## Variables: 3 ## $ coin &lt;chr&gt; &quot;augur&quot;, &quot;bitcoin&quot;, &quot;cardano&quot;, &quot;chainlink&quot;... ## $ avg &lt;dbl&gt; 2.315007e+01, 7.491721e+03, 1.359970e-01, ... ## $ var &lt;dbl&gt; 3.283892e+02, 6.722352e+06, 2.614519e-02, ... 5.3 Примеры использования извлеченных признаков Описанные в предыдущем разделе признаки позволяют проанализировать и понять свойства сразу нескольких временных рядов. Представим, например, что перед нами стоит задача выявить временные ряды с наименьшей и наибольшей выраженностью тренда. Это можно было бы сделать следующим образом (рис. 5.1): require(ggplot2) require(gridExtra) cryptos_trend &lt;- cryptos %&gt;% features(., y, feature_set(tags = &quot;stl&quot;)) min_trend &lt;- cryptos_trend %&gt;% filter(trend_strength == min(trend_strength)) %&gt;% dplyr::select(coin) %&gt;% left_join(., cryptos, by = &quot;coin&quot;) %&gt;% ggplot(., aes(ds, y)) + geom_line() + facet_grid(~coin) + theme_minimal() max_trend &lt;- cryptos_trend %&gt;% filter(trend_strength == max(trend_strength)) %&gt;% dplyr::select(coin) %&gt;% left_join(., cryptos, by = &quot;coin&quot;) %&gt;% ggplot(., aes(ds, y)) + geom_line() + facet_grid(~coin) + theme_minimal() grid.arrange(min_trend, max_trend, ncol = 2) РИСУНОК 5.1: Временные ряды из набора данных cryptos с минимальной (слева) и максимальной (справа) выраженностью тренда Другой вопрос, на который было бы интересно ответить: какие ряды необычны по сочетанию нескольких признаков одновременно? Посмотрим, например, как выглядит распределение анализируемых рядов по выраженности тренда и недельной сезонности (рис. 5.2): require(ggrepel) cryptos_trend %&gt;% ggplot(., aes(trend_strength, seasonal_strength_week, label = coin)) + geom_point() + geom_text_repel(force = 10, segment.color = &quot;gray60&quot;) + theme_minimal() РИСУНОК 5.2: Распределение временных рядов из набора данных cryptos в соответствии с выраженностью тренда и недельной сезонности На рис. 5.2 четко выделяется временной ряд tether, для которого характерны низкие значения обоих показателей. Мы уже видели, как выглядит этот временной ряд (см. рис. 5.1 слева). Изобразим теперь и данные по криптовалюте qtum, которая характеризуется необычно высокой выраженностью как тренда, так и недельной сезонности (рис. 5.3): cryptos %&gt;% filter(coin == &quot;qtum&quot;) %&gt;% ggplot(., aes(ds, y)) + geom_line() + theme_minimal() РИСУНОК 5.3: Динамика стоимости криптовалюты qtum Мы можем пойти еще дальше и проанализировать данные из таблицы cryptos с помощью метода главных компонент (PCA) одновременно по всем признакам с ненулевой дисперсией (обратите внимание на логарифмирование исходных значений y, выполнененное для снижения дисперсии в отдельных рядах и различий между рядами): # Функция для нахождения признаки c нулевой дисперсией: zero_var_cols &lt;- function(dat) { checks &lt;- lapply(dat, function(x) length(unique(x))) keep &lt;- which(!checks &gt; 1) unlist(keep) %&gt;% names(.) } set.seed(1984) pca &lt;- all_crypto_features %&gt;% dplyr::select(-c(coin, zero_var_cols(.))) %&gt;% prcomp(scale = TRUE) # summary(pca) Согласно полученному результату (не приведен для экономии места), первые две главные компоненты объясняют примерно 56% всей дисперсии в данных. Изобразим все временные ряды в системе координат, образованной этими двумя главными компонентами (рис. 5.4): pc &lt;- all_crypto_features %&gt;% dplyr::select(coin) %&gt;% bind_cols(., as_tibble(pca$x)) pc %&gt;% ggplot(., aes(PC1, PC2, label = coin)) + geom_point() + geom_text_repel(force = 10, segment.color = &quot;gray60&quot;) + theme_minimal() РИСУНОК 5.4: Распределение временных рядов из таблицы cryptos в соответствии со значениями первых двух главных компонент PCA–анализ по совокупности нескольких десятков признаков еще раз подтвердил аномальность временного ряда tether, а также подчеркнул необычность ряда bitcoin. Следует отметить, что этот анализ был выполнен в демонстрационных целях: аномальность tether легко заметить и без применения PCA — достаточно изобразить все имеющиеся временные ряды на одном графике (см. рис. 1.2, где этот ряд выглядит практически как горизонтальная линия). Тем не менее, во многих реальных ситуациях приходится иметь дело с сотнями и даже тысячами временных рядов, простое визуальное исследование которых невозможно. Описанный выше подход в сочетании с определенным методом обнаружения аномалий помог бы выявить необычные временные ряды в автоматическом режиме. "],
["ch-intro-to-prophet.html", "ГЛАВА 6 Пакет prophet 6.1 Методология 6.2 Первый простой пример 6.3 Функция prophet() 6.4 Точки излома тренда 6.5 Эффекты праздников и других важных событий 6.6 Сезонные компоненты 6.7 Модели с предикторами 6.8 Выбор оптимальной модели 6.9 Моделирование емкости системы", " ГЛАВА 6 Пакет prophet Прогнозирование — это, пожалуй, самая распространенная задача, возникающая при работе с временными рядами. Однако получить надежные прогнозы непросто — для этого требуется серьезная подготовка специалиста, который решает подобную задачу, а также наличие подходящего (и желательно удобного в использовании) программного обеспечения. В R существует большое количество пакетов для анализа временных рядов (см. также сайт проф. Роба Хиндмана). Например, одним из наиболее популярных является пакет forecast, в котором реализованы как классические (экспоненциальное сглаживание, модель Хольта–Винтерса, ARIMA и др.), так и недавно разработанные методы прогнозирования (модели для сгруппированных временных рядов, рядов с несколькими сезонными компонентами и др.). Такое разнообразие методов является и преимуществом, и недостатком пакета forecast. Другой важный недостаток состоит в том, что все реализованные в forecast методы имеют свои собственные параметры настройки, и даже опытные аналитики не застрахованы от выбора неправильного метода и/или набора параметров для решения стоящей задачи. В 2017 г. специалисты компании Facebook объявили о разработанном ими новом пакете для прогнозирования временных рядов — prophet (“пророк”). prophet во многом лишен указанных выше недостатков forecast и других подобных пакетов и позволяет создавать точные прогнозные модели в (полу–)автоматическом режиме. В этой главе мы рассмотрим основные возможности prophet и особенности работы с ним. Пакет prophet распространяется бесплатно по лицензии MIT. Его легко установить стандартным образом из хранилища CRAN (на Windows–машинах предварительно нужно будет установить Rtools): install.packages(&quot;prophet&quot;) Если вы работаете на компьютере Mac под управлением OS X, не забудьте добавить аргумент type = \"source\": install.packages(&quot;prophet&quot;, type = &quot;source&quot;) 6.1 Методология Подробное описание реализованной в prophet методологии можно найти в статье Taylor and Letham (2017). Вкратце, в основе этой методологии лежит процедура подгонки аддитивных регрессионных моделей (Generalized Additive Models, GAM) следующего вида: \\[y(t) = g(t) + s(t) + h(t) + \\epsilon_t,\\] где \\(g(t)\\) и \\(s(t)\\) — функции, аппроксимирующие тренд ряда и сезонные колебания (например, годовые, недельные и т.п.) соответственно, \\(h(t)\\) — функция, отражающая эффекты праздников и других влиятельных событий, а \\(\\epsilon_t\\) — нормально распределенные случайные возмущения. Для аппроксимации перечисленных функций используются следующие методы: тренд: кусочная линейная регрессия или кусочная логистическая кривая роста; годовая сезонность: частичные суммы ряда Фурье, число членов которого (порядок) определяет гладкость функции; недельная сезонность: представлена в виде индикаторной переменной; “праздники” (например, официальные праздничные и выходные дни — Новый год, Рождество и т.п., а также другие дни, во время которых свойства временного ряда могут существенно измениться — спортивные или культурные события, природные явления и т.п.): представлены в виде индикаторных переменных. Оценивание параметров подгоняемой модели выполняется с использованием принципов байесовской статистики (либо методом нахождения апостериорного максимума (MAP), либо путем полного байесовского вывода). Для этого применяется платформа вероятностного программирования Stan. Пакет prophet представляет собой ни что иное, как удобный интерфейс для работы с этой платформой из среды R (имеется также аналогичная библиотека для Python — fbprophet). 6.2 Первый простой пример Для иллюстрации особенностей работы с пакетом prophet воспользуемся набором данных bitcoin, в котором хранятся исторические данные по стоимости биткоина на момент закрытия торгов (подразд. 1.5.2). Стоимость биткоина — не самая простая переменная для моделирования (что справедливо для подавляющего большинства финансовых временных рядов). Этот ряд обладает сложным трендом, дисперсия его значений возрастает со временем, имеют место резкие изменения уровней, вероятно сопряженные с какими–то (в большинстве случаев неизвестными нам) особыми событиями (рис. 1.3). Тем не менее, это хороший пример реальных данных, с которыми аналитик может столкнуться на практике. Тем интереснее будет посмотреть, как с задачей прогнозирования этого ряда справится prophet! Предположим, что нам необходимо сделать прогноз стоимости биткоина на следующие 90 дней. Приведенный ниже код выполняет подготовку данных для построения подобной модели: сначала присходит логарифмирование значений стоимости биткоина y (для снижения дисперсии), а затем разбиение исходной выборки на обучающую (все наблюдения за исключением последних 90 дней) и проверочную (последние 90 дней). Обратите внимание также на замену класса итоговых таблиц bitcoin_train и bitcoin_test с tsibble на стандартный data.frame — это обусловлено тем, что в настоящее время функции пакета prophet, к сожалению, не могут корректно работать с данными в формате tsibble. bitcoin_train &lt;- bitcoin %&gt;% mutate(y = log(y)) %&gt;% slice(1:(n() - 90)) %&gt;% as.data.frame() bitcoin_test &lt;- bitcoin %&gt;% mutate(y = log(y), ds = as.Date(ds)) %&gt;% tail(90) %&gt;% as.data.frame() Подгонку моделей с разными параметрами мы будем выполнять на обучающих данных (bitcoin_train). Проверочная выборка (bitcoin_test) пригодится в самом конце процесса моделирования, чтобы выяснить насколько наши ожидания в отношении качества выбранной оптимальной модели соответствуют действительности. Заметьте, что в обеих этих таблицах столбец с датами обозначен как ds, а столбец со значениями отклика как y. Это условные обозначения, принятые в prophet. Использование каких–либо других имен приведет к ошибке при вызове соответствующих функций. Обучающие данные, подготовленные описанным выше способом, представлены на рис. 6.1. bitcoin_train %&gt;% ggplot(., aes(ds, y)) + geom_line() + theme_minimal() РИСУНОК 6.1: Обучающие данные по стоимости биткоина Нашу первую модель (обозначим ее M0) мы построим с использованием параметров, принятых в prophet по умолчанию. Для этого потребуется всего одна строка кода: require(prophet) M0 &lt;- prophet(bitcoin_train) Объект M0 представляет собой большой список (выполните команду str(M0), чтобы просмотреть его структуру). Для получения прогноза на основе этой модели необходимо сначала воспользоваться функцией make_future_dataframe() и создать таблицу с датами, охватывающими необходимый временной промежуток в будущем (“горизонт”), а затем подать эту таблицу вместе с модельным объектом на функцию predict(): future_df &lt;- make_future_dataframe(M0, periods = 90) forecast_M0 &lt;- predict(M0, future_df) Объект forecast_M0 — это обычная таблица, в которой хранятся значения нескольких рассчитанных на основе модели M0 величин, включая компоненты модели (см. разд. 6.1), предсказанные значения отклика, а также верхние и нижние границы доверительных интервалов соответствующих величин. Вот так, например, выглядят первые несколько предсказанных значений стоимости биткоина и их (принятые по умолчанию) 80%–ные доверительные границы: forecast_M0 %&gt;% dplyr::select(yhat, yhat_lower, yhat_upper) %&gt;% head() ## yhat yhat_lower yhat_upper ## 1 6.134964 6.024296 6.230752 ## 2 6.134025 6.034873 6.237148 ## 3 6.127710 6.026591 6.232882 ## 4 6.124329 6.018906 6.227581 ## 5 6.119314 6.008620 6.227911 ## 6 6.111687 6.006040 6.218552 Таблицу forecast_M0 и объект M0 далее можно подать на функцию plot(), чтобы изобразить подогнанную модель и прогнозные значения на графике (рис. 6.2): plot(M0, forecast_M0) РИСУНОК 6.2: Прогноз стоимости биткоина, полученный на основе модели M0 Точки на рис. 6.2 соответствуют (логарифмированным) значениям стоимости биткоина из обучающей выборки. Сплошная голубая линия — это предсказанные моделью значения стоимости, а огибающая эту линию светло–голубая “лента” обозначает 80%–ные доверительные границы предсказанных значений. Прогнозные значения у на следующие 90 дней видны в правой части графика. С помощью функции prophet_plot_components() мы можем также изобразить отдельные компоненты модели (рис. 6.3): prophet_plot_components(M0, forecast_M0) РИСУНОК 6.3: Компоненты модели M0 На рис. 6.3 видно, что модель M0 хорошо передает имеющийся в данных сложный тренд. Видно также, что в этом временном ряду есть очень слабо выраженные внутригодовые колебания и практически несуществующие колебания в пределах недели (обратите внимание на шкалы ординат этих трех графиков, которые помогают оценить вклад каждой из компонент). Для создания модели M0 потребовалась всего одна строка кода и процесс подгонки занял пару секунд. Учитывая, что анализируемый нами временной ряд не самый простой для моделирования, полученная модель довольно хорошо передает свойства этого ряда. Возможность быстро и удобно создавать такие качественные модели в автоматическом режиме является основным преимуществом prophet. Тем не менее, качество прогноза M0 оставляет желать лучшего. На данном этапе моделирования главным признаком неудовлетворительного качества предсказаний M0 является чрезмерно расширяющиеся доверительные границы прогнозных значений (рис. 6.3). В следующих разделах мы постараемся улучшить эту базовую модель путем добавления предикторов и настройки параметров функции prophet(). 6.3 Функция prophet() Ниже приведено описание основных аргументов функции prophet(), с которой мы познакомились в предыдущем разделе: df — необязательный аргумент, с помощью которого указывают таблицу с историческими данными. Такая таблица должна содержать как минимум два столбца: ds (даты в формате YYYY-MM-DD) и y (значения моделируемого отклика). В случаях, когда тренд в y моделируется как логистический рост, таблица с историческими данными должна также содержать столбец cap (“емкость”), который соответствует максимально достижимым значениям y для соответствующих дат. Если аргумент df не указан (NULL), то произойдет только инициализация модельного объекта, а для непосредственного запуска подгонки модели необходимо будет воспользоваться функцией fit.profit(m, df). growth — тип тренда. Принимает два возможных значения: \"linear\" (“линейный”, присвоено по умолчанию) и \"logistic\" (“логистический”). changepoints — текстовый вектор с датами (в формате YYYY-MM-DD), соответствующими “переломным моментам”, или “точкам излома” в y (т.е. датам, когда произошли существенные изменения в тренде временного ряда). Если вектор changepoints не указан, то такие переломные моменты будут оценены автоматически. n.changepoints — предполагаемое количество “переломных моментов” (25 по умолчанию). Если аргумент changepoints задан, то аргумент n.changepoints будет проигнорирован. Если же changepoints не задан, то n.changepoints потенциальных точек излома будут распределены равномерно в пределах исторического отрезка, задаваемого аргументом changepoint.range. changepoint.range — доля исторических данных (начиная с самого первого наблюдения), по которым будут оценены точки излома. По умолчанию составляет 0.8 (т.е. 80% наблюдений). yearly.seasonality — параметр настройки годовой сезонности (т.е. закономерных колебаний в пределах года). Принимает следующие возможные значения: \"auto\" (автоматический режим, принят по умолчанию), TRUE, FALSE или количество членов ряда Фурье, с помощью которого аппроксимируется компонента годовой сезонности. weekly.seasonality — параметр настройки недельной сезонности (т.е. закономерных колебаний в пределах недели). Возможные значения те же, что и у yearly.seasonality. daily.seasonality — параметр настройки дневной сезонности (т.е. закономерных колебаний в пределах дня). Возможные значения те же, что и у yearly.seasonality. holidays — таблица, содержащая два обязательных столбца: holiday (текстовая переменная с названиями “праздников” и других важных событий, потенциально влияющих на свойства временного ряда) и ds (даты). По желанию в такую таблицу можно добавить еще два столбца — lower_window и upper_window, которые задают отрезок времени вокруг соответствующего события. Так, например, при \"lower_window = -2\" в модель будут добавлены 2 дня, предшествующие соответствующему событию. По желанию можно также добавить столбец prior_scale — априорное значение стандартного отклонения (нормального) распределения, с помощью которого моделируется эффект того или иного события. seasonality.mode — режим моделирования сезонных компонент. Принимает два возможных значения: \"additive\" (аддитивный, задан по умолчанию) и \"multiplicative\" (мультипликативный). seasonality.prior.scale — параметр, определяющий выраженность сезонных компонент модели (10 по умолчанию). Более высокие значения приведут к более “гибкой” модели, а низкие — к модели со слабее выраженными сезонными эффектами. Этот параметр можно задать отдельно для каждого типа сезонности с помощью функции add_seasonality(). holidays.prior.scale — параметр, определяющий выраженность эффектов “праздников” и других важных событий (10 по умолчанию). Если таблица, подаваемая на аргумент holidays, имеет столбец prior_scale (см. выше), то аргумент holidays.prior.scale будет проигнорирован. changepoint.prior.scale — параметр, задающий чувствительность автоматического механизма обнаружения точек излома в тренде временного ряда y (0.05 по умолчанию). Более высокие значение позволят иметь больше таких точек излома (что одновременно увеличит риск переобучения модели). mcmc.samples — целое число (0 по умолчанию). Если &gt;0, то параметры модели будут оценены путем полного байесовского анализа с использованием mcmc.samples итераций алгоритма MCMC (процесс подгонки модели при этом может существенно замедлиться). interval.width — число, определяющее ширину доверительного интервала для предсказанных моделью значений (0.8 по умолчанию, что соответствует 80%–ному интервалу). При \"mcmc.samples = 0\" этот интервал будет оценен с использованием MAP–метода и только на основе неопределенности в отношении тренда в у. Если же mcmc.samples &gt;0, то доверительные интервалы будут оцениваться с учетом неопределенности в отношении оценок всех параметров модели (включая сезонные компоненты). uncertainty.samples — число итераций для оценивания доверительных интервалов (1000 по умолчанию). fit — логическое значение (TRUE по умолчанию). При fit = FALSE произойдет только инициализация модельного объекта, но подгонка модели не запустится. ... — дополнительные параметры, которые передаются на функцию fit.prophet(). 6.4 Точки излома тренда Как было отмечено в предыдущем разделе, аналитик может задать точки излома либо самостоятельно (с помощью аргумента changepoints функции prophet()), либо довериться их автоматическому обнаружению. Рассмотрим, как работает каждый из этих режимов, и что происходит в результате изменения соответствующих аргументов функции prophet(). В автоматическом режиме при инициализации модели 25 потенциальных точек излома равномерно распределяются в пределах интервала, который охватывает первые 80% наблюдений из обучающей выборки. Именно это произошло, когда мы построили базовую модель М0 в разд. 6.2. Однако эти 25 точек — лишь предполагаемые места существенных изменений в тренде: в большинстве случаев на практике тренд временного ряда не изменяется так часто. Поэтому в ходе подгонки модели срабатывает механизм регуляризации (подобный \\(l1\\)–регуляризации), в результате чего выбирается минимальное необходимое количество точек излома. Изобразить эти автоматически обнаруженные точки излома можно с помощью функции add_changepoints_to_plot(). Так, для модели M0 получаем (рис. 6.4): plot(M0, forecast_M0) + add_changepoints_to_plot(M0) РИСУНОК 6.4: Точки излома тренда, оцененные в результате подгонки модели M0. Сплошная красная линия — тренд. Штриховые красные линии — точки излома тренда Судя по полученному графику, модель M0 все еще переоценивает количество “переломных моментов” в тренде. Построим новую модель, которая будет инициализирована с меньшим начальным количеством потенциальных точек излома (15 вместо 25; рис. 6.5): M1 &lt;- prophet(bitcoin_train, n.changepoints = 15) forecast_M1 &lt;- predict(M1, future_df) plot(M1, forecast_M1) + add_changepoints_to_plot(M1) РИСУНОК 6.5: Точки излома тренда, оцененные в результате подгонки модели M1. Сплошная красная линия — тренд. Штриховые красные линии — точки излома тренда Как и ожидалось, оцененный тренд получился более гладким, чем в модели M0. Хорошо это или плохо, мы узнаем позже, когда рассмотрим диагностику качества моделей с помощью перекрестной проверки (разд. 6.8). Помимо изменения начального количества потенциальных точек излома тренда мы можем также изменить временной интервал, в пределах которого происходит их оценивание. По умолчанию этот интервал охватывает первые 80% наблюдений. Однако из приведенных выше графиков видно, что примерно в начале ноября 2018 г. произошло резкое падение стоимости биткоина. Ни одна из построенных нами моделей пока не учла это изменение, поскольку оно не вошло в интервал, в пределах которого происходило оценивание точек излома. Увеличим этот интервал до 90%, воспользовавшись аргументом changepoint.range (одновременно увеличим количество потенциальных точек излома с 15 до 20, поскольку на большем промежутке времени можно ожидать больше перепадов в тренде): M2 &lt;- prophet(bitcoin_train, n.changepoints = 20, changepoint.range = 0.9) forecast_M2 &lt;- predict(M2, future_df) plot(M2, forecast_M2) + add_changepoints_to_plot(M2) РИСУНОК 6.6: Точки излома тренда, оцененные в результате подгонки модели M2. Сплошная красная линия — тренд. Штриховые красные линии — точки излома тренда Как видно на рис. 6.6, полученная модель M2 намного лучше передает свойства анализируемого временного ряда. Это касается и получаемого с ее помощью прогноза (как с точки зрения направления тренда, так и с точки зрения ширины доверительных границ предсказанных значений). Еще один параметр для настройки гладкости тренда в моделируемом временном ряду — это changepoint.prior.scale. Чем больше значение этого параметра (по сравнению с принятым по умолчанию значением 0.05), тем больше точек излома останется в полученной модели. Рассмотрим эффект действия changepoint.prior.scale на следующем примере (рис. 6.7): # В этой модели мы увеличиваем интервал, в пределах которого # оцениваются точки излома тренда (до 90%), одновременно увеличивая # уровень регуляризации с помощью параметра changepoint.prior.scale. # Начальное количество потенциальных точек излома оставим равным # значению, принятому по умолчанию (25): M3 &lt;- prophet(bitcoin_train, changepoint.range = 0.9, changepoint.prior.scale = 0.02) forecast_M3 &lt;- predict(M3, future_df) plot(M3, forecast_M3) + add_changepoints_to_plot(M3) РИСУНОК 6.7: Точки излома тренда, оцененные в результате подгонки модели M3. Сплошная красная линия — тренд. Штриховые красные линии — точки излома тренда Как видим, модели M2 и M3 дают похожие результаты, что скорее определяется значением параметра changepoint.range, нежели способом регуляризации количества точек излома. Наконец, посмотрим, что получится, если задать точки излома тренда “вручную”, а не оценивать их в автоматическом режиме. Для этого служит аргумент changepoints (рис. 6.8): # Здесь мы задаем точки излома тренда самостоятельно # (выбор дат, подаваемых на аргумент changepoints, основан на # визуальном анализе обучающих данных): M4 &lt;- prophet(bitcoin_train, changepoints = c(&quot;2016-04-01&quot;, &quot;2016-06-15&quot;, &quot;2016-10-01&quot;, &quot;2017-04-01&quot;, &quot;2017-07-01&quot;, &quot;2017-09-01&quot;, &quot;2017-12-26&quot;, &quot;2018-04-01&quot;, &quot;2018-11-13&quot;, &quot;2018-12-15&quot;, &quot;2019-04-01&quot;)) forecast_M4 &lt;- predict(M4, future_df) plot(M4, forecast_M4) + add_changepoints_to_plot(M4) РИСУНОК 6.8: Точки излома тренда, оцененные в результате подгонки модели M4. Сплошная красная линия — тренд. Штриховые красные линии — точки излома тренда Модель M4 хорошо описывает тренд в анализируемом временном ряду, хотя не исключено что она несколько переобучена. 6.5 Эффекты праздников и других важных событий Продолжим начатое ранее знакомство с параметрами функции prophet() и рассмотрим способы моделирования эффектов “праздников”. Употребляемый здесь термин “праздник” — результат прямого перевода термина “holiday”, принятого в пакете prophet. Под этим термином мы будем понимать как настоящие официальные праздничные и выходные дни (например, Новый год, Рождество и т.п.), так и другие события, сопровождающиеся заметными изменениями свойств временного ряда (спортивные или культурные мероприятия, природные явления, политические события и т.п.). Поэтому слова “праздник” и “событие” будут применяться ниже как синонимы. 6.5.1 Формат представления Как было отмечено в разд. 6.3, для добавления эффектов праздников в prophet–модель необходимо сначала создать отдельную таблицу, содержащую как минимум два обязательных столбца: holiday (названия праздников и других событий) и ds (временные метки; обычно это даты в формате YYYY-MM-DD). Важно, чтобы в такую таблицу входил как исторический период, на основе которого происходит обучение модели, так и период в будущем, для которого необходимо сделать прогноз. Например, если какое–то важное событие встречается в обучающих данных, то его следует указать и для прогнозного периода (при условии, конечно, что мы ожидаем повторение этого события в будущем, и что дата этого события входит в прогнозный период). История биткоина полна событий, которые косвенно или непосредственно оказали влияние на стоимость этой криптовалюты (см. также здесь). В качестве примера, возьмем некоторые из этих событий: # event_1 - cоздание Bitcoin Cash # event_2 - запрет ICO в Китае # event_3 - новые правила торгов в Ю. Корее # event_4 - удаление южнокорейского рынка из трекера CoinMarketCap # event_5 - разветвление Bitcoin Cash key_dates &lt;- dplyr::tibble( holiday = paste0(&quot;event_&quot;, 1:5), ds = as.Date(c(&quot;2017-08-01&quot;, &quot;2017-09-04&quot;, &quot;2017-12-28&quot;, &quot;2018-01-08&quot;, &quot;2018-11-15&quot;)) ) Теперь добавим эти события в модель (аргумент holidays): M5 &lt;- prophet(bitcoin_train, holidays = key_dates, changepoint.range = 0.9) forecast_M5 &lt;- predict(M5, future_df) prophet_plot_components(M5, forecast_M5) РИСУНОК 6.9: Оцененные компоненты модели M5 На рис. 6.9 приведены оцененные компоненты модели M5, включая эффекты событий, которые были добавлены с помощью аргумента holidays (см. второй график сверху). Функция plot_forecast_component() позволяет изобразить эффекты отдельных событий (см. аргумент name; рис. 6.10): plot_forecast_component(M5, forecast_M5, name = &quot;event_5&quot;) РИСУНОК 6.10: Эффект разветвления Bitcoin Cash на Bitcoin Cash SV и Bitcoing Cash ABC (event_5), оцененный с помощью модели M5 На аргумент name функции plot_forecast_component() можно также подать значение \"holidays\", что приведет к изображению на отдельном графике эффектов всех включенных в модель событий (рис. 6.11): plot_forecast_component(M5, forecast_M5, name = &quot;holidays&quot;) РИСУНОК 6.11: Эффекты всех событий, включенных в модель M5 О наступлении некоторых событий в истории биткоина было известно заранее, как например о разветвлении Bitcoin Cash на Bitcoin Cash SV и Bitcoin Cash ABC (event_5 в модели M5). Поэтому многие спекулянты начали скупать Bitcoin Cash за несколько дней до “вилки”, или “форка”, чтобы впоследствии удвоить свой капитал (при наступлении “форка” владелец старой монеты автоматически становится владельцем и новой монеты). В связи с этим имело бы смысл моделировать event_5 как событие c “предысторией”, т.е. как событие, чей эффект начал проявлять себя за несколько дней до главной даты (в данном случае 15 ноября 2018 г.). В пакете prophet это можно сделать, добавив в таблицу с моделируемыми событиями столбцы lower_window (определят длительность “предыстории”) и upper_window (определяет длительность эффекта после главной даты). Для примера предположим, что спекулянты начали скупать Bitcoin Cash за две недели до “вилки” (lower_window = -14) и прекратили делать это сразу после “вилки” (upper_window = 0): key_dates2 &lt;- dplyr::bind_cols(key_dates, lower_window = c(0, 0, 0, 0, -14), upper_window = c(0, 0, 0, 0, 0)) M6 &lt;- prophet(bitcoin_train, holidays = key_dates2, changepoint.range = 0.9) forecast_M6 &lt;- predict(M6, future_df) # Эффект события с &quot;предысторией&quot;: plot_forecast_component(M6, forecast_M6, name = &quot;event_5&quot;) РИСУНОК 6.12: Эффект события с предысторией Как видно на рис. 6.12, теперь эффект event_5 включает несколько дней до главной даты этого события (сравните с рис. 6.10). 6.5.2 Встроенные даты праздников До этого момента мы моделировали эффекты событий, которые случались только один раз. Однако многие события, такие как официальные государственные праздники и выходные дни, повторяются регулярно и их эффекты также могут оказаться важными для прогноза. Конечно, мы могли бы воспользоваться описанным выше способом для включения таких событий в модель, т.е. путем создания таблицы, подобной key_dates или key_dates2. К счастью, в большинстве случаев в этом не будет необходимости — в prophet уже включены даты официальных праздников и выходных дней для более чем 60 стран (полный список можно найти на сайте с официальной документацией по пакету). Эти встроенные даты праздников охватывают период с 1995 по 2044 гг. Для их добавления в модель служит функция add_country_holidays(), которая принимает два аргумента: m (модельный объект) и country_name (международное обозначение страны, например \"Russia\" или \"RU\"). Для примера построим модель, которая включает как рассмотренные выше важные в истории биткоина разовые события, так и регулярные официальные праздники США: # Обратите внимание: здесь мы инициализируем объект M7, # но пока не подаем на него таблицу с обучающими данными M7 &lt;- prophet(holidays = key_dates2, changepoint.range = 0.9) # Добавляем официальные праздничные дни США: M7 &lt;- add_country_holidays(m = M7, country_name = &#39;US&#39;) # Обратите внимание на использование функции fit.prophet(): M7 &lt;- fit.prophet(M7, bitcoin_train) forecast_M7 &lt;- predict(M7, future_df) plot_forecast_component(M7, forecast_M7, name = &quot;holidays&quot;) РИСУНОК 6.13: Эффекты разовых событий и официальных праздников США, оцененные с помощью модели M7 Оцененные эффекты всех событий из модели M7 показаны на рис. 6.13. Просмотреть названия этих событий можно следующим образом: M7$train.holiday.names ## [1] &quot;event_1&quot; &quot;event_2&quot; ## [3] &quot;event_3&quot; &quot;event_4&quot; ## [5] &quot;event_5&quot; &quot;New Year&#39;s Day&quot; ## [7] &quot;Martin Luther King, Jr. Day&quot; &quot;Washington&#39;s Birthday&quot; ## [9] &quot;Memorial Day&quot; &quot;Independence Day&quot; ## [11] &quot;Labor Day&quot; &quot;Columbus Day&quot; ## [13] &quot;Veterans Day&quot; &quot;Thanksgiving&quot; ## [15] &quot;Christmas Day&quot; &quot;Christmas Day (Observed)&quot; ## [17] &quot;New Year&#39;s Day (Observed)&quot; &quot;Veterans Day (Observed)&quot; 6.5.3 Регуляризация В пакете prophet имеется возможность подавлять величину эффектов “праздников”, что может оказаться полезным при возникновении риска переобучения модели. Такой контроль (регуляризация) осуществляется одним из двух способов: глобальный контроль — распространяется на все моделируемые события; контроль на уровне отдельных событий. Аргумент holidays.prior.scale функции prophet() позволяет реализовать первый из этих способов. Данный аргумент задает априорное значение стандартного отклонения (нормального) распределения, с помощью которого моделируется эффект того или иного события. По умолчанию holidays.prior.scale = 10, что соответствует незначительной регуляризации. Уменьшение этого заданного по умолчанию значения приведет к подавлению эффектов всех событий, включенных в модель (рис. 6.14): # Для глобальной регуляризации эффектов праздников служит аргумент # holidays.prior.scale: M8 &lt;- prophet(holidays = key_dates2, changepoint.range = 0.9, holidays.prior.scale = 0.01) M8 &lt;- add_country_holidays(M8, country_name = &quot;US&quot;) M8 &lt;- fit.prophet(M8, bitcoin_train) forecast_M8 &lt;- predict(M8, future_df) # Эффекты праздников до (модель M7) и после (M8) # глобальной регуляризации: m7_holidays &lt;- plot_forecast_component(M7, forecast_M7, name = &quot;holidays&quot;) + labs(title = &quot;M7&quot;) + ylim(c(-0.15, 0.25)) m8_holidays &lt;- plot_forecast_component(M8, forecast_M8, name = &quot;holidays&quot;) + labs(title = &quot;M8&quot;) + ylim(c(-0.15, 0.25)) gridExtra::grid.arrange(m7_holidays, m8_holidays, nrow = 1) РИСУНОК 6.14: Эффекты событий до (модель M7) и после (модель M8) глобальной регуляризации Для управления эффектами отдельных событий в таблицу с перечнем событий необходимо добавить столбец prior_scale. В качестве примера уменьшим величину эффекта события event_5, оставив остальные уровни без изменений (т.е. используя принятое по умолчанию значение параметра prior_scale = 10): key_dates3 &lt;- dplyr::bind_cols(key_dates2, prior_scale = c(10, 10, 10, 10, 0.01)) M9 &lt;- prophet(holidays = key_dates3, changepoint.range = 0.9) M9 &lt;- add_country_holidays(M9, country_name = &#39;US&#39;) M9 &lt;- fit.prophet(M9, bitcoin_train) forecast_M9 &lt;- predict(M9, future_df) m9_holidays &lt;- plot_forecast_component(M9, forecast_M9, name = &quot;holidays&quot;) + labs(title = &quot;M9&quot;) + ylim(c(-0.15, 0.25)) gridExtra::grid.arrange(m7_holidays, m9_holidays, nrow = 1) РИСУНОК 6.15: Пример регуляризации эффекта отдельного события На рис. 6.14 хорошо виден результат подавления эффекта события event_5 (сравните уровень самого большого пика на графике слева с уровнем пика в той же позиции на графике справа). При этом эффекты большинства других событий и официальных праздников остались почти неизменными. 6.6 Сезонные компоненты Как было отмечено в разд. 6.1, сезонные компоненты аппроксимируются в prophet с помощью частичных сумм ряда Фурье, число членов которого (порядок) определяет гладкость соответствующей функции. Рассмотрим различные способы спецификации сезонных колебаний. 6.6.1 Годовая, недельная и дневная компоненты Функция prophet() имеет три аргумента, с помощью которых можно контролировать гладкость функций годовой, недельной и дневной сезонности: yearly.seasonality, weekly.seasonality и daily.seasonality (см. разд. 6.3). Увеличение значений этих аргументов приведет к подгонке менее гладких функций соответствующих компонент (что одновременно увеличит риск переобучения модели). На рис. 6.16 для примера показана функция годовой сезонности, оцененная с помощью модели M3 из разд. 6.4 (график этой функции построен с помощью служебной функции plot_yearly(), которую можно вызвать только обычным в таких случаях образом, т.е. указав имя пакета в сочетании с тройным двоеточем перед именем скрытой функции). prophet:::plot_yearly(M3) РИСУНОК 6.16: Функция годовой сезонности, оцененная с помощью модели M3 Увеличив значение аргумента yearly.seasonality с заданного по умолчанию 10 до 20, мы получим менее гладкую кривую (рис. 6.17): M3B &lt;- prophet(bitcoin_train, yearly.seasonality = 20, changepoint.range = 0.9, changepoint.prior.scale = 0.02) prophet:::plot_yearly(M3B) РИСУНОК 6.17: Функция годовой сезонности, оцененная с помощью модели M3B 6.6.2 Пользовательские сезонные компоненты Для данных, охватывающих как минимум два года, функция prophet() автоматически добавит в модель компоненты годовой и недельной сезонности. Если гранулярность данных превышает дневную (например, когда имеются почасовые наблюдения зависимой переменной), то в модель автоматически будет добавлена также и компонента дневной сезонности. Помимо этого, пользователи имеют возможность добавить и любые другие сезонные компоненты с помощью функции add_seasonality() (например, часовую, месячную, квартальную и т.п.). Эта функция имеет следующие аргументы: m — модельный объект; name — название сезонной компоненты; period — число (необязательно целое), соответствующее количеству временных интервалов в одном сезонном цикле; fourier.order — порядок (количество членов) ряда Фурье (по умолчанию равен 3 для недельной сезонности и 10 для годовой); mode — тип модели; принимает два возможных значения — \"additive\" (аддитивная; выбирается по умолчанию) и \"multiplicative\" (мультипликативная); condition.name — название сторонней переменной, которая задает разные режимы моделируемой сезонности (см. ниже). Рассмотрим примеры использования функции add_seasonality() и ее аргументов. В приведенном ниже коде мы сначала отключаем автоматически добавляемую в модель недельную сезонность и вместо нее добавляем месячную (допустив, что один месячный период составляет 30.5 дней). На рис. 6.18 представлены все сезонные компоненты полученной модели (тренд, годовая сезонность и месячная сезонность). M10 &lt;- prophet(weekly.seasonality = FALSE) M10 &lt;- add_seasonality(m = M10, name = &quot;monthly&quot;, period = 30.5, fourier.order = 5) M10 &lt;- fit.prophet(M10, bitcoin_train) forecast_M10 &lt;- predict(M10, future_df) prophet_plot_components(M10, forecast_M10) РИСУНОК 6.18: Компоненты модели M10 Аналогичным образом вместо компоненты месячных колебаний мы могли бы добавить, например, компоненту квартальной сезонности (задав период длиной 365.25/4 дней): M11 &lt;- prophet(weekly.seasonality = FALSE) M11 &lt;- add_seasonality(m = M11, name = &quot;quarter&quot;, period = 365.25/4, fourier.order = 2) M11 &lt;- fit.prophet(M11, bitcoin_train) forecast_M11 &lt;- predict(M11, future_df) prophet_plot_components(M11, forecast_M11) РИСУНОК 6.19: Компоненты модели M11 6.6.3 Условные режимы сезонности В ряде случаев функция, аппроксимирующая ту или иную сезонную составляющую, может изменять свои свойства в зависимости от каких–то сторонних факторов. Например, колебания в течение рабочих дней могут иметь характер, сильно отличающийся от такового в выходные дни. Пакет prophet позволяет моделировать такие условные режимы сезонности (т.е. режимы, которые зависят от сторонних факторов) с помощью аргумента condition.name функции add_seasonality(). Как следует из названия, на этот аргумент подается имя (булевой) переменной, которая определяет соответствующий режим. Такие переменные должны хранится в той же таблице, что и основные данные по временному ряду. Исключительно в качестве примера предположим, что недельные колебания стоимости биткоина в летние месяцы отличаются от таковых в другие месяцы. Чтобы смоделировать такое различие добавим в таблицу с данными bitcoin_train две новые индикаторные переменные: summer (принимает значение TRUE в летние месяцы и FALSE в другие месяцы) и not_summer (TRUE в нелетние месяцы и FALSE летом). Важно помнить, что такие же переменные нужно добавить и в таблицу с будущими датами future_df — иначе прогнозные значения рассчитать не получится: # Функция для удобного добавления переключателей режимов: is_summer &lt;- function(ds) { month &lt;- as.numeric(format(ds, &#39;%m&#39;)) return(month &gt; 5 &amp; month &lt; 9) } # Добавляем переключатели режима в данные: bitcoin_train$summer &lt;- is_summer(bitcoin_train$ds) bitcoin_train$not_summer &lt;- !bitcoin_train$summer future_df$summer &lt;- is_summer(future_df$ds) future_df$not_summer &lt;- !future_df$summer # Подгоняем модель: M12 &lt;- prophet(weekly.seasonality = FALSE) M12 &lt;- add_seasonality(M12, name = &#39;weekly_summer&#39;, period = 7, fourier.order = 3, condition.name = &#39;summer&#39;) M12 &lt;- add_seasonality(M12, name = &quot;weekly_not_summer&quot;, period = 7, fourier.order = 3, condition.name = &quot;not_summer&quot;) M12 &lt;- fit.prophet(M12, bitcoin_train) forecast_M12 &lt;- predict(M12, future_df) prophet_plot_components(M12, forecast_M12) РИСУНОК 6.20: Оцененные компоненты модели M12 # Удалим переменные `summer` и `not_summer` из таблицы # `bitcoin_train` - в будущем они нам не понадобятся: bitcoin_train$summer &lt;- NULL bitcoin_train$not_summer &lt;- NULL Согласно полученной модели, в нелетние месяцы стоимость биткоина в течение недели обычно достигает максимума по средам, тогда как в летние месяцы по средам обычно наблюдается противоположная картина (рис. 6.20). 6.6.4 Регуляризация сезонных компонент Подобно тому, как это было с эффектами праздников и других важных событий (разд. 6.5), мы можем контролировать уровень вклада сезонных компонент. Глобальный контроль выполняется с помощью аргумента seasonality.prior.scale функции prophet() (разд. 6.3). Контроль на уровне отдельных сезонных компонент возможен с помощью аргумента prior.scale функции add_seasonality(). По умолчанию prior.scale = 10. Уменьшение этого значения приведет к подавлению вклада соответствующей компоненты. 6.6.5 Аддитивная и мультипликативная сезонности Как было отмечено в разд. 1.3, по характеру функциональной связи между своими компонентами модели временных рядов делятся на два основных типа — аддитивные и мультипликативные. Первый из них применяется в случаях, когда амплитуда сезонных колебаний приблизительно постоянна. Если же эта амплитуда заметно изменяется во времени (обычно возрастает), то строят мультипликативную модель. В пакете prophet по умолчанию подгоняются аддитивные модели временных рядов (разд. 6.1). В мультипликативных моделях, как следует из их названия, сезонная компонента умножается на тренд (в связи с этим вклад сезонных колебаний моделируется в виде доли (%) от уровня тренда): \\[y(t) = g(t) \\times s(t) + h(t) + \\epsilon_t.\\] В приведенном уравнении предполагается, что амплитуда всех сезонных компонент существенно изменяется во времени. Для подгонки соответствующих моделей необходимо воспользоваться аргументом seasonality.mode функции prophet() (рис. 6.20): M14 &lt;- prophet(bitcoin_train, seasonality.mode = &quot;multiplicative&quot;) forecast_M14 &lt;- predict(M14, future_df) plot(M14, forecast_M14) РИСУНОК 6.21: Прогноз стоимости биткоина, полученный на основе модели M14 Однако в пакете prophet имеется возможность и более тонкого контроля над аддитивностью сезонных компонент. Так, например, можно построить модели, в которых недельная колебания представлены в аддитивном виде, а годовые — в мультипликативном. Вероятно, вы уже догадались, что для этого применяется функция add_seasonality(): M15 &lt;- prophet(yearly.seasonality = FALSE) M15 &lt;- add_seasonality(M15, name = &#39;yearly&#39;, period = 365.25, fourier.order = 10, mode = &quot;multiplicative&quot;) M15 &lt;- fit.prophet(M15, bitcoin_train) forecast_M15 &lt;- predict(M15, future_df) prophet_plot_components(M15, forecast_M15) РИСУНОК 6.22: Оцененные компоненты модели M15 6.7 Модели с предикторами Поскольку модели временных рядов, построенные с помощью пакета prophet, представляют собой одну из разновидностей регрессионных моделей (разд. 6.1), то помимо обычных компонент и эффектов “праздников” в такие модели можно добавить и любые другие предикторы. Для этого служит функция add_regressor(), которая имеет следующие аргументы: m — модельный объект; name — название добавляемого предиктора; prior.scale — параметр, используемый для регуляризации эффекта добавляемого предиктора (здесь не рассматривается, однако см., например, разд. 6.5); standardize — позволяет стандартизовать значения добавляемого предиктора перед подгонкой модели (по умолчанию принимает значение \"auto\" — в этом случае предиктор будет стандартизован, если только он не является индикаторной переменной со значениями 1 и 0; другие возможные значения этого аргумента: TRUE и FALSE); mode — необязательный параметр, определяющий характер сезонности добавляемого предиктора (по умолчанию равен m$seasonality.mode). Все предикторы, добавляемые в модель с помощью функции add_regressor(), должны присутствовать в таблице с обучающими данными. Для расчета предсказаний будущие значения каждого предиктора должны присутствовать также в таблице с датами, задающими прогнозный отрезок времени. Последнее обстоятельство делает расчет прогноза с использованием количественных предикторов и многих качественных переменных проблематичным: будущие значения таких предикторов (в отличие, например, от дат официальных праздников и других регулярных событий) обычно исследователю не известны. Как правило, для решения этой проблемы сначала строят отдельные модели временных рядов для каждого предиктора, а затем используют предсказанные с помощью таких моделей будущие значения предикторов для получения искомого прогноза. Конечно, такой подход существенно усложняет весь процесс моделирования (поскольку возникает необходимость построения надежных моделей для отдельных предикторов) и потенциально увеличивает неточность прогноза, однако в большинстве случаев это — лучшее, что можно сделать. Именно такой стратегией мы воспользуемся в описанном ниже примере. Добавим в модель стоимости биткоина три предиктора: цены на акции компаний Amazon, Google и Facebook, хранящиеся в таблице shares_na_filled (гл. 2). Нужно подчеркнуть, что выбор цен на акции в качестве предикторов, равно как и выбор именно этих компаний, ни с чем не связаны — просто такие данные легко получить из публично доступных источников. Добавим этим данные в таблицу bitcoin_train: require(tidyr) # Размах дат в таблицах: bitcoin_train %&gt;% pull(ds) %&gt;% range() ## [1] &quot;2016-01-01&quot; &quot;2019-05-26&quot; shares_na_filled %&gt;% pull(ds) %&gt;% range() ## [1] &quot;2016-01-01&quot; &quot;2019-05-26&quot; # Объединение таблиц (по ключу `ds`): bitcoin_train &lt;- bitcoin_train %&gt;% left_join(., shares_na_filled, by = &quot;ds&quot;) %&gt;% mutate(price = log(price)) %&gt;% pivot_wider(., names_from = share, values_from = price) Обратите внимание: даты в обеих объединенных нами таблицах лежат в пределах от 2016-01-01 до 2019-05-26 (включительно), что соответствует периоду обучающих данных. Важно также понимать, что на момент написания этого раздела (октябрь 2019 г.) интересующий нас прогнозный период (c 2019-05-27 по 2019-08-24) уже стал историей и, соответственно, цены на акции для этого периода были известны. Но, конечно же, мы не можем использовать эти ставшие историей данные для прогнозирования стоимости биткоина на период с 2019-05-27 по 2019-08-24: в реальной ситуации при расчете прогноза подобное “заглядывание в будущее” было бы невозможно. Следует также отметить, что хотя prophet–модели мало чувствительны к наличию пропущенных значений в зависимой переменной, пропущенные значения в предикторах недопустимы. Поэтому в приведенном выше коде мы добавили в исходные данные таблицу shares_na_filled, в которой пропущенные наблюдения уже были восстановлены (методом LOCF, см. гл. 2). Как отмечено выше, цены на акции Amazon, Google и Facebook выбраны в качестве предикторов исключительно из удобства. Тем не менее, все три переменные демонстрируют определенную (нелинейную) связь со стоимостью биткоина и, соответственно, вполне подходят для наших целей (рис. 6.23): bitcoin_train %&gt;% pivot_longer(-c(y, ds), names_to = &quot;share&quot;, values_to = &quot;price&quot;) %&gt;% ggplot(., aes(price, y)) + geom_point(alpha = 0.1) + geom_smooth(se = FALSE) + facet_wrap(~share, scales = &quot;free_x&quot;, ncol = 3) + theme_minimal() РИСУНОК 6.23: Связь между ценой акций трех компаний и стоимостью биткоина Однако прежде, чем перейти к построению модели стоимости биткоина, нам необходимо решить проблему с будущими значениями предикторов. Для этого мы построим отдельные модели для каждого предиктора, а затем рассчитаем их предсказанные значения для интересующего нас прогнозного периода: # Функция, которая поможет нам получить прогнозные значения # цены акций одновременно для всех трех компаний get_price_futures &lt;- function(df) { df &lt;- dplyr::select(df, -share) %&gt;% rename(y = price) m &lt;- prophet(df, n.changepoints = 15, changepoint.range = 0.9, weekly.seasonality = FALSE) future_df_shares &lt;- future_df %&gt;% dplyr::select(ds) price_forecast &lt;- predict(m, future_df_shares) future_df_shares %&gt;% mutate(price = price_forecast$yhat) %&gt;% return() } future_share_price &lt;- bitcoin_train %&gt;% pivot_longer(-c(y, ds), names_to = &quot;share&quot;, values_to = &quot;price&quot;) %&gt;% dplyr::select(-y) %&gt;% group_by(share) %&gt;% do(get_price_futures(.)) %&gt;% pivot_wider(., names_from = share, values_from = price) head(future_share_price) ## # A tibble: 6 x 4 ## ds amzn fb goog ## &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2016-01-01 00:00:00 6.34 4.55 6.55 ## 2 2016-01-02 00:00:00 6.35 4.55 6.55 ## 3 2016-01-03 00:00:00 6.35 4.55 6.55 ## 4 2016-01-04 00:00:00 6.35 4.55 6.56 ## 5 2016-01-05 00:00:00 6.36 4.55 6.56 ## 6 2016-01-06 00:00:00 6.36 4.56 6.56 Исходные данные и прогнозные значения цен на акции показаны на рис. 6.24). future_share_price %&gt;% pivot_longer(-ds, names_to = &quot;share&quot;, values_to = &quot;price&quot;) %&gt;% mutate(period = ifelse(ds &gt; as.Date(&quot;2019-05-26&quot;), &quot;future&quot;, &quot;history&quot;)) %&gt;% ggplot(aes(ds, price, col = period)) + geom_line(size = 1) + scale_colour_manual(values = c(&quot;red&quot;, &quot;gray70&quot;)) + facet_wrap(~share, scales = &quot;free_y&quot;, ncol = 3) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) РИСУНОК 6.24: Исторические данные по цене акций (серые линии) и предсказанные значения (красные линии) Теперь у нас есть все, чтобы построить модель стоимости биткоина с тремя предикторами и рассчитать прогнозные значения (рис. 6.25): M16 &lt;- prophet(n.changepoints = 15, changepoint.range = 0.9) M16 &lt;- add_regressor(M16, &#39;amzn&#39;) M16 &lt;- add_regressor(M16, &#39;goog&#39;) M16 &lt;- add_regressor(M16, &#39;fb&#39;) M16 &lt;- fit.prophet(M16, bitcoin_train) forecast_M16 &lt;- future_share_price %&gt;% predict(M16, .) plot(M16, forecast_M16) РИСУНОК 6.25: Прогноз стоимости биткоина, полученный на основе модели M16 На рис. 6.26 представлены оцененные компоненты модели M16. Заметьте, что эффекты всех трех предикторов объединены в одную компоненту (см. extra_regressors_additive на графике внизу): prophet_plot_components(M16, forecast_M16) РИСУНОК 6.26: Оцененные компоненты модели M16 С помощью рассмотренной ранее функции plot_forecast_component() (разд. 6.5) мы можем также изобразить эффекты отдельных предикторов (рис. 6.27): amzn_comp &lt;- plot_forecast_component(M16, forecast_M16, name = &quot;amzn&quot;) + ggtitle(&quot;Amazon&quot;) fb_comp &lt;- plot_forecast_component(M16, forecast_M16, name = &quot;fb&quot;) + ggtitle(&quot;Facebook&quot;) goog_comp &lt;- plot_forecast_component(M16, forecast_M16, name = &quot;goog&quot;) + ggtitle(&quot;Google&quot;) gridExtra::grid.arrange(amzn_comp, fb_comp, goog_comp, ncol = 3) РИСУНОК 6.27: Оцененные эффекты предикторов, включенных в модель M16 Согласно полученной модели, эффект переменной amzn оказался сильнее (примерный диапазон значений от –0.2 до 0.2), чем эффекты goog (от –0.005 до 0.005) и fb (от –0.06 до 0.06). Мы можем сделать такое заключение благодаря тому, что эффекты всех трех предикторов представлены на одной шкале и поэтому сравнимы (значения всех этих переменных перед подгонкой модели были стандартизованы — см. выше описание аргумента standardize функции add_regressor()). Интересно также, что характер изменения эффекта amzn во времени почти зеркально отличается от такового у переменных goog и fb. Оставим читателям возможность подумать над интерпретацией этого наблюдения самостоятельно. В рассмотренном нами примере все добавленные в модель независимые переменные были количественными. В случае с качественными предикторами у исследователя есть два варианта на выбор: либо представить значения таких переменных в виде “праздников” (разд. 6.5), либо воспользоваться описанной выше более общей функцией add_regressor(). В обоих случаях качественные переменные необходимо преобразовать в индикаторные (т.е. создать отдельные столбцы для каждого уровня переменной со значениями 1 и 0). 6.8 Выбор оптимальной модели В предыдущих разделах этой главы мы построили целый ряд моделей для прогнозирования стоимости биткоина. Но какая из этих моделей лучше? Стандартным методом оценки качества нескольких альтернативных моделей является перекрестная проверка. Суть этого метода сводится к тому, что исходные обучающие данные случайным образом разбиваются на \\(K\\) частей (блоков), после чего модель \\(k\\) раз подгоняется по \\(K - 1\\) блокам, а оставшийся блок каждый раз используется для проверки качества предсказаний на основе той или иной подходящей случаю метрики. Полученная таким образом средняя метрика будет хорошей оценкой качества предсказаний модели на новых данных. К сожалению, в случае с моделями временных рядов такой способ выполнения перекрестной проверки не имеет смысла и не отвечает стоящей задаче. Поскольку во временных рядах, как правило, имеет место значительная автокорреляция (гл. 4), то мы не можем просто разбить такой ряд случайным образом на \\(K\\) частей — это приведет к потере указанной корреляции. Более того, в результате случайного разбиения данных на несколько блоков может получиться так, что в какой–то из итераций мы построим модель преимущественно по недавним наблюдениям, а затем оценим ее качество на блоке из давних наблюдений. Другими словами, мы построим модель, которая будет предсказывать прошлое, что не имеет никакого смысла — ведь мы пытаемся решить задачу по предсказанию будущего! Для решения описанной проблемы при работе с временными рядами применяют несколько модификаций перекрестной проверки (Hyndman and Athanasopoulos 2019; Tashman 2000). В пакете prophet, в частности, реализован т.н. метод “имитированных исторических прогнозов” (Simulated Historical Forecasts, SHF). Рассмотрим, что этот метод собой представляет, и как им пользоваться для выбора оптимальной модели. 6.8.1 Метод имитированных исторических прогнозов Любая модель временного ряда строится на основе данных, собранные в течение некоторого периода в прошлом. Далее по полученной модели рассчитываются прогнозные значения для интересующего нас промежутка времени (горизонта) в будущем. Такая процедура повторяется каждый раз, когда необходимо сделать новый прогноз (рис. 6.28). РИСУНОК 6.28: Стандартная процедура построения моделей для прогнозирования временных рядов Метод SHF (рис. 6.29) пытается сымитировать описанную выше процедуру. В пределах отрезка с исходными обучающими данными выбирают \\(K\\) “точек отсчета” (“cut–off points” по терминологии prophet), на основе которых формируются блоки данных для выполнения перекрестной проверки: все исторические наблюдения, предшествующие \\(k\\)–й точке отсчета (а также сама эта точка), образуют обучающие данные для подгонки соответствующей модели, а \\(H\\) исторических наблюдений, следующих за точкой отсчета, образуют прогнозный горизонт. Расстояние между точками отсчета называется “периодом” (“period”) и по умолчанию составляет \\(H / 2\\). Обучающие наблюдения в первом из \\(K\\) блоков образуют т.н. “initial period” (“начальный отрезок”). В prophet длина этого отрезка по умолчанию составляет \\(3 \\times H\\), однако исследователь при желании может ее изменить. Каждый раз после подгонки модели на обучающих данных из \\(k\\)–го блока рассчитываются предсказания для прогнозного горизонта того же блока, что позволяет оценить качество прогноза с помощью подходящей случаю метрики (например, RMSE; см. ниже). Значения этой метрики, усредненные по каждой дате прогнозных горизонтов каждого блока, в итоге дают оценку качества предсказаний, которую можно ожидать от модели, построенной по всем исходным обучающим данным. Это в свою очередь позволяет сравнить несколько альтернативных моделей и выбрать оптимальную. РИСУНОК 6.29: Процедура, лежащая в основе метода имитированных исторических прогнозов Таким образом, SHF близок к классическому способу выполнения перекрестной проверки моделей временных рядов по методу “скользящей точки отсчета” (“rolling origin evaluation”; Tashman 2000), однако отличается от последнего тем, что для оценивания качества предсказаний используется меньше блоков и прогнозные горизонты этих блоков удалены друг от друга на некоторое расстояние. С одной стороны, это является преимуществом метода SHF, поскольку он требует меньше вычислений и получаемые оценки качества предсказаний не так сильно коррелируют друг с другом, как в случае с перекрестной проверкой по методу “скользящей точки отсчета”. С другой стороны, при небольшом количестве блоков \\(K\\) оценка качества модели может оказаться ненадежной. С решением последней проблемы отчасти помогает увеличение отрезка исходных исторических данных (поскольку чем он длиннее, тем больше блоков можно в него вместить). Однако следует помнить, что модели, построенные на длинных временных рядах, часто демонстрируют низкое качество прогнозов, поскольку параметры таких моделей задать труднее и возрастает риск переобучения. 6.8.2 Выполнение перекрестной проверки В пакете prophet перекрестная проверка по методу имитированных исторических прогнозов выполняется с помощью функции cross_validation(), которая имеет следующие аргументы: model — модельный объект; horizon — длина прогнозного горизонта в каждом блоке данных, используемом для выполнения перекрестной проверки; units — название единицы измерения времени (например, \"days\", \"hours\", \"secs\" — см. справочный файл по базовой функции difftime()); initial — длина начального отрезка с обучающими данными в первом блоке. Функция cross_validation() возвращает таблицу c наблюденными (y) и оцененными (yhat) значениями моделируемой переменной, а также доверительными границами предсказанных значений (yhat_lower и yhat_upper) для каждой точки отсчета cutoff и каждой даты ds соответствующего прогнозного периода (в примере используется модель M3 из разд. 6.4): M3_cv &lt;- cross_validation(M3, initial = 730, period = 90, horizon = 90, units = &quot;days&quot;) head(M3_cv) ## y ds yhat yhat_lower yhat_upper cutoff ## 1 9.349206 2018-03-03 9.226595 9.113810 9.338773 2018-03-02 ## 2 9.351197 2018-03-04 9.212202 9.108078 9.321019 2018-03-02 ## 3 9.356456 2018-03-05 9.206319 9.096260 9.311673 2018-03-02 ## 4 9.285439 2018-03-06 9.193538 9.085362 9.307952 2018-03-02 ## 5 9.206891 2018-03-07 9.181843 9.069791 9.287407 2018-03-02 ## 6 9.147934 2018-03-08 9.171255 9.056305 9.283333 2018-03-02 6.8.3 Метрики качества модели Как следует из ее названия, функция performance_metrics() позволяет рассчитать метрики, характеризующие качество предсказаний моделей. В частности, имеется возможность рассчитать следующие показатели: Среднеквадратичная ошибка (mean squared error, MSE): \\[MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\] Квадратный корень из среднеквадратичной ошибки (root mean squared error, RMSE): \\[RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\\] Средняя абсолютная ошибка (mean absolute error, MAE): \\[MAE = \\sum_{i=1}^n |y_i - \\hat{y}_i| \\] Средняя абсолютная удельная ошибка (mean absolute percentage error, MAPE): \\[MAPE = \\sum_{i=1}^n |\\frac{y_i - \\hat{y}_i}{y_i}|\\] “Покрытие” (coverage): доля истинных значений моделируемой переменной, которые находятся в пределах доверительных границ прогноза. В приведенных формулах \\(y_i\\) и \\(\\hat{y}_i\\) — это истинное и предсказанное значения моделируемой переменной соответственно, а \\(n\\) — количество наблюдений. Функция performance_metrics() имеет следующие аргументы: df — таблица, полученная с помощью функции cross_validation(); metrics — вектор с названиями метрик качества модели (по умолчанию этот аргумент принимает значение NULL, что приводит к расчету всех перечисленных выше метрик, т.е. c(\"mse\", \"rmse\", \"mae\", \"mape\", \"coverage\")); rolling_window — размер “скользящего окна”, в пределах которого происходит усреднение каждой метрики (по умолчанию принимает значение 0.1, т.е. 10% от длины прогнозного горизонта; см. пояснения ниже). На рис. 6.30 представлено схематичное изображение того, как функция performance_metrics() рассчитывает метрики качества модели в случае с перекрестной проверкой по 5 блокам данных с длиной прогнозного горизонта \\(H = 100\\) и аргументом rolling_window = 0.1. РИСУНОК 6.30: Процедура расчета метрик качества моделей при выполнении перекрестной проверки по методу SHF В приведенном ниже примере функция performance_metrics() применена для расчета среднеквадратичной ошибки прогноза построенной нами ранее модели M3: performance_metrics(M3_cv, metrics = &quot;mse&quot;, rolling_window = 0.1) %&gt;% head() ## horizon mse ## 1 9 days 0.06424673 ## 2 10 days 0.07572021 ## 3 11 days 0.08695892 ## 4 12 days 0.09707747 ## 5 13 days 0.10990773 ## 6 14 days 0.12260315 Как видно из полученного результата, первое усредненное значение MSE приходится на 9–й день прогнозного горизонта, поскольку длина этого горизонта для модели M3 составляет 90 дней (см. выше код для создания таблицы M3_cv), а 9 - это 10% от этой длины (размер скользящего окна, задаваемый аргументом rolling_window). Если аргументу rolling_window присвоить значение 0, то запрашиваемые метрики качества будут рассчитаны для каждой даты прогнозного горизонта (т.е. размер скользящего окна в данном случае фактически равен 1): performance_metrics(M3_cv, metrics = &quot;mse&quot;, rolling_window = 0) %&gt;% head() ## horizon mse ## 1 1 days 0.03595728 ## 2 2 days 0.03941046 ## 3 3 days 0.05343681 ## 4 4 days 0.05035319 ## 5 5 days 0.05454100 ## 6 6 days 0.07256616 Если же аргументу rolling_window присвоить значение 1, то запрашиваемые метрики качества будут усреднены по всему прогнозному горизонту: performance_metrics(M3_cv, metrics = &quot;mse&quot;, rolling_window = 1) %&gt;% head() ## horizon mse ## 1 90 days 0.192834 Метрики качества моделей, полученные в ходе перекрестной проверки, можно визуализировать с помощью функции plot_cross_validation_metric(), которая имеет следующие аргументы: df_cv — таблица, полученная с помощью функции cross_validation(); metric — название метрики; rolling_window — размер “скользящего окна”, в пределах которого происходит усреднение метрики (см. рис. 6.30 и описание функции performance_metrics()). Функция plot_cross_validation_metric() возвращает объект класса ggplot: plot_cross_validation_metric(M3_cv, metric = &quot;mse&quot;, rolling_window = 0.1) РИСУНОК 6.31: Визуализация метрики качества предсказаний (MSE), полученной по результатам перекрестной проверки модели M3 На рис. 6.31 приведены оценки MSE для каждой из дат прогнозного горизонта (\\(H = 90\\)) каждого из \\(K = 5\\) блоков данных, участвовавших в перекрестной проверке. Голубая линия соответствует усредненным значениям в пределах каждого скользящего окна размером в 9 наблюдений. Судя по большому разбросу полученных оценок MSE, качество модели M3 желает оставлять лучшего. 6.8.4 Пример выбора оптимальной модели Рассмотрим теперь, как описанную выше методологию выполнения перекрестной проверки можно применить для выбора оптимальной модели из нескольких альтернативных. Предположим, что перед нами стоит задача выбрать оптимальную модель стоимости биткоина из построенных ранее моделей M4, M5 и M12. Для описания качества этих моделей воспользуемся двумя метриками: MAPE и покрытие (подразд. 6.8.3). Для упрощения примера предположим также, что нас интересует качество предсказаний в целом для 90–дневного прогнозного горизонта (т.е. нам неинтересны отдельные даты этого горизонта). Рассчитаем обе метрики качества для каждой из моделей–кандидатов: M4_cv &lt;- cross_validation(M4, initial = 730, period = 180, horizon = 90, units = &quot;days&quot;) M5_cv &lt;- cross_validation(M5, initial = 730, period = 180, horizon = 90, units = &quot;days&quot;) M12_cv &lt;- cross_validation(M12, initial = 730, period = 180, horizon = 90, units = &quot;days&quot;) M4_perf &lt;- performance_metrics(M4_cv, metrics = c(&quot;mape&quot;, &quot;coverage&quot;), rolling_window = 1) M5_perf &lt;- performance_metrics(M5_cv, metrics = c(&quot;mape&quot;, &quot;coverage&quot;), rolling_window = 1) M12_perf &lt;- performance_metrics(M12_cv, metrics = c(&quot;mape&quot;, &quot;coverage&quot;), rolling_window = 1) M4_perf ## horizon mape coverage ## 1 90 days 0.03214045 0.2851852 M5_perf ## horizon mape coverage ## 1 90 days 0.03717749 0.462963 M12_perf ## horizon mape coverage ## 1 90 days 0.02347762 0.637037 Как видим, M12 лучше других моделей по обеим выбранным метрикам качества. Это можно видеть также из графиков, построенных с помощью функции plot_cross_validation_metric() (рис. 6.32): M4_cv_plot &lt;- plot_cross_validation_metric(M4_cv, metric = &quot;mape&quot;, rolling_window = 0.1) + ylim(c(0, 0.15)) + ggtitle(&quot;M4&quot;) M5_cv_plot &lt;- plot_cross_validation_metric(M5_cv, metric = &quot;mape&quot;, rolling_window = 0.1) + ylim(c(0, 0.15)) + ggtitle(&quot;M5&quot;) M12_cv_plot &lt;- plot_cross_validation_metric(M12_cv, metric = &quot;mape&quot;, rolling_window = 0.1) + ylim(c(0, 0.15)) + ggtitle(&quot;M12&quot;) gridExtra::grid.arrange(M4_cv_plot, M5_cv_plot, M12_cv_plot, ncol = 3) РИСУНОК 6.32: Сравнение качества предсказаний трех моделей по метрике MAPE Вспомним, что до сих пор мы строили все модели по обучающим данным из таблицы bitcoin_train. Однако у нас есть и проверочный набор данных — bitcoin_test (см. разд. 6.2). Посмотрим, как выбранная нами оптимальная модель M12 сработает на этой проверочной выборке. На рис. 6.33 представлены обучающие данные (черные точки; для удобства показаны наблюдения только за 2019 г.) и истинные значения стоимости биткоина в прогнозном периоде (красные точки). Голубая сплошная линия на этом графике соответствует предсказанным моделью значениям, а светло–голубая полоса вокруг нее — 80%–ной доверительной области предсказанных значений: plot(M12, forecast_M12) + coord_cartesian(xlim = c(as.POSIXct(&quot;2019-01-01&quot;), as.POSIXct(&quot;2019-08-24&quot;))) + geom_point(data = bitcoin_test, aes(as.POSIXct(ds), y), col = &quot;red&quot;) РИСУНОК 6.33: Сравнение истинных значений стоимости биткоина и прогнозных значений, полученных с помощью модели M12. См. пояснения в тексте Хотя выбранная нами в качестве оптимальной модель M12 не смогла правильно предсказать некоторые локальные колебания стоимости биткоина в прогнозном периоде, в целом она дала неплохой результат: большинство истинных значений стоимости оказалось в пределах 80%–ной доверительной полосы. Однако следует подчеркнуть, что это всего лишь пример использования перекрестной проверки для сравнения качества предсказаний нескольких моделей: совершенно точно не стоит разрабатывать стратегию торговли биткоином на основе приведенных здесь результатов! 6.9 Моделирование емкости системы Во всех примерах из предыдущих разделов этой главы предполагалось, что моделируемая переменная может расти во времени бесконечно. Однако многие системы имеют естественную “емкость”, выше которой рост невозможен. Классическим примером здесь будет численность популяции какого–либо биологического вида, которая ограничена имеющимися в среде обитания ресурсами. Другой пример — количество пользователей какой–либо онлайн–услуги, которое органичено доступом в Интернет. Важно отметить, что емкость системы может иметь ограничения и по нижнему порогу (например, численность населения не может быть отрицательной). Одной из особенностей пакета prophet является возможность моделировать временные ряды для подобных систем. Рассмотрим, как это происходит. 6.9.1 Тренд с насыщением Стандартным подходом для описания роста в системе с ограниченной емкостью является использование логистической функции следующего вида: \\[g(t) = \\frac{C}{1 + \\exp(-k(t-m))},\\] где \\(C\\) — это верхний порог (емкость системы), \\(k\\) — скорость роста, \\(t\\) — время, а \\(m\\) — параметр, позволяющий “сдвигать” функцию вдоль оси времени. На рис. 6.34 показаны примеры логистической функции с разными значениями параметров. logistic_growth &lt;- function(x) {C / (1 + exp(-k*(x - m)))} par(mfrow = c(1, 3)) # Пример 1: C &lt;- 10 k &lt;- 0.1 m &lt;- 0 curve(logistic_growth(x), from = -100, to = 100) abline(v = 0, col = &quot;red&quot;, lty = 2) abline(h = 10, col = &quot;blue&quot;, lty = 2) title(&quot;C=10, k=0.1, m=0&quot;) # Пример 2: C &lt;- 10 k &lt;- 0.1 m &lt;- 20 curve(logistic_growth(x), from = -100, to = 100) abline(v = 20, col = &quot;red&quot;, lty = 2) abline(h = 10, col = &quot;blue&quot;, lty = 2) title(&quot;C=10, k=0.1, m=10&quot;) # Пример 3: C &lt;- 8 k &lt;- 0.05 m &lt;- 0 curve(logistic_growth(x), from = -100, to = 100, ylim = c(0, 10)) abline(v = 0, col = &quot;red&quot;, lty = 2) abline(h = 8, col = &quot;blue&quot;, lty = 2) title(&quot;C=8, k=0.05, m=0&quot;) РИСУНОК 6.34: Примеры логистической функции с разными значениями параметров Есть два важных аспекта, которые не отражены в приведенном выше уравнении логистической функции (Taylor and Letham 2017). Во–первых, емкость многих систем непостоянна. Например, число людей, имеющих доступ в Интернет (равно как и количество потенциальных пользователей того или иного онлайн–ресурса) со временем возрастает. Поэтому в пакете prophet постоянная емкость системы \\(C\\) заменена на динамическую \\(C(t)\\). Во–вторых, непостоянной обычно бывает и скорость роста \\(k\\). Например, выход новой версии продукта может значительно ускорить рост числа его потребителей. В prophet для моделирования такого рода изменений вводится понятие точек излома тренда (разд. 6.4). Предположим, что \\(S\\) таких точек приходятся на временные отметки \\(s_j\\), \\(j = 1, 2, \\dots, S\\). Совокупность всех изменений скорости роста \\(\\delta_j\\) можно представить в виде вектора \\(\\boldsymbol \\delta \\in \\mathbb{R}^S\\). Тогда скорость роста в любой точке времени \\(t\\) будет равна сумме базовой скорости \\(k\\) и всех изменений, предшествовавших этой точке: \\(k + \\sum_{j:t &gt; s_j} \\delta_j\\). Более наглядно это можно представить с помощью такого вектора \\(\\boldsymbol{a}(t) \\in \\left\\{0, 1\\right\\}^S\\), что \\[ a_j(t) = \\begin{cases} 1 \\; \\text{если} \\; t \\ge s_j \\\\ 0 \\; \\text{в остальных случаях} \\end{cases} \\] Тогда скорость роста в момент времени \\(t\\) составит \\(k + \\boldsymbol{a}(t)^\\intercal \\boldsymbol{\\delta}\\). При изменении скорости роста необходимо также изменить параметр сдвига \\(m\\) (см. уравнение выше), чтобы обеспечить гладкий стык сегментов кривой тренда на соответствующей временной отметке. Такая поправка рассчитывается следующим образом: \\[\\gamma_j = \\left(s_j - m -\\sum_{l &lt; j}\\gamma_l \\right) \\left(1 - \\frac{k + \\sum_{l &lt; j} \\delta_l}{k + \\sum_{l \\le j} \\delta_l} \\right).\\] В результате кусочная логистическая функция (piecewise logistic growth model), которая используется в пакете prophet для моделирования тренда с насыщением (saturating growth), принимает вид \\[g(t) = \\frac{C(t)}{1 + \\exp( -(k + \\boldsymbol{a}(t)^\\intercal \\boldsymbol{\\delta})(t - (m + \\boldsymbol{a}(t)^\\intercal \\boldsymbol{\\gamma})) )}.\\] Важным параметром в приведенном уравнении является \\(C(t)\\) — емкость системы в каждый момент времени. Как задать этот параметр для подгонки модели? Часто у исследователя уже будет хорошее представление о емкости моделируемой системы (например, компании обычно хорошо знают размер рынка, на котором они работают). Если же такого понимания нет, то придется потратить некоторое время на поиск дополнительной информации из сторонних источников (например, прогнозы роста численности населения от Всемирного Банка, тренды в Google–запросах и т.п.). При неограниченном росте моделируемой системы вместо представленной выше логистической функции в пакете prophet используется кусочно–линейная функция следующего вида (обозначения те же): \\[ g(t) = (k + \\boldsymbol{a}(t)^\\intercal \\boldsymbol{\\delta})t + (m + \\boldsymbol{a}(t)^\\intercal \\boldsymbol{\\gamma}).\\] Именно такого рода рост по умолчанию предполагался во всех моделях, которые мы рассматривали в предыдущих разделах этой главы. 6.9.2 Примеры моделей с насыщением тренда Для начала построим модель, в которой рост зависимой переменной ограничен по некоторому верхнему порогу. Для этого воспользуемся данными по стоимости биткоина за период с января 2016 г. по середину декабря 2017 г., когда наблюдался практически экспоненциальный рост этой переменной (см. рис. 1.3). Предположим, что несмотря на свой экспоненциальный характер роста стоимость биткоина в будущем не могла превысить 11.5 (на логарифмической шкале, что соответствует почти 99000$ (!) на исходной шкале). Для введения этого верхнего порога в модель необходимо добавить новый столбец с (обязательным) именем cap в таблицу с обучающими данными: bitcoin_train_cap &lt;- bitcoin_train %&gt;% filter(ds &lt;= as.Date(&quot;2017-12-15&quot;)) %&gt;% mutate(cap = 11.5) Теперь построим модель по этим обучающим данным (обратите внимание использование аргумента growth = \"logistic\" при вызове функции prophet()), рассчитаем прогноз на следующие 180 дней и изобразим полученный результат (рис. 6.35): M17 &lt;- prophet(bitcoin_train_cap, growth = &quot;logistic&quot;, changepoint.range = 0.95, changepoint.prior.scale = 0.15) # Таблица с датами прогнозного периода (180 дней): future_df_cap &lt;- make_future_dataframe(M17, periods = 180) %&gt;% mutate(cap = 11.5) forecast_M17 &lt;- predict(M17, future_df_cap) plot(M17, forecast_M17) РИСУНОК 6.35: Пример модели с верхним порогом тренда Как видим, несмотря на экспоненциальный рост в историческом периоде, предсказанные значения стоимости биткоина постепенно выходят на плато, что обусловлено введенным в модель ограничением на рост. Многие естественные системы имеют ограничения емкости не только по верхнему, но и нижнему порогу (подобно упомянутой выше численности населения). В prophet имеется возможность учесть это обстоятельство путем добавления еще одного столбца — floor — в таблицы с обучающими данными и данными для расчета прогноза (верхний порог при этом также должен присутствовать в обеих таблицах). Для демонстрации этой возможности воспользуемся данными по стоимости биткоина в 2018 г., когда наблюдался выраженный тренд на ее снижение, и построим модель с верхним ограничением тренда cap = 10 и нижним ограничением floor = 7 (рис. 6.36): bitcoin_train_cap_floor &lt;- bitcoin_train %&gt;% filter(ds &gt;= as.Date(&quot;2018-01-01&quot;) &amp; ds &lt;= as.Date(&quot;2018-12-31&quot;)) %&gt;% mutate(cap = 10, floor = 7) M18 &lt;- prophet(bitcoin_train_cap_floor, growth = &quot;logistic&quot;, changepoint.range = 0.85, changepoint.prior.scale = 0.15) future_df_both &lt;- make_future_dataframe(M18, periods = 180) %&gt;% mutate(floor = 7, cap = 10) forecast_M18 &lt;- predict(M18, future_df_both) plot(M18, forecast_M18) РИСУНОК 6.36: Пример модели с нижним порогом тренда В рассмотренных выше примерах предполагалось, что и верхний, и нижний пороги емкости системы постоянны. Если в моделируемой вами системе это не так, то в таблицы с данными для соответствующих дат просто необходимо внести подходящие значения cap и/или floor. "],
["ch-intro-to-bsts.html", "ГЛАВА 7 Пакет bsts 7.1 Методология 7.2 Функция bsts() 7.3 Спецификация компонент модели 7.4 Примеры моделей без предикторов 7.5 Модели с предикторами 7.6 Выбор оптимальной модели", " ГЛАВА 7 Пакет bsts Эта глава посвящена еще одному мощному инструменту для моделирования и прогнозирования временных рядов — пакету bsts, который был разработан сотрудником компании Google Стивеном Скоттом (Steven L. Stott). В основе этого пакета лежит методология, известная в литературе под несколькими названиями: “байесовские структурные модели временных рядов” (Bayesian structural time series models), “модели пространства состояний” (state-space models), “динамические линейные модели” (dynamic linear models), модели на основе фильтра Калмана (Kalma filter models) и др. Подобно пакету prophet, bsts делает упор на прогнозирование временных рядов, представленных дневными данными, и позволяет включать в модели сторонние предикторы (в частности, эффекты “праздников”). Оба пакета используют принципы байесовской статистики для оценивания параметров моделей. Однако в отличие от пакета prophet, который выполняет подгонку единой обобщенной аддитивной модели (см. разд. 6.1), в пакете bsts присходит байесовское усреднение предсказаний ансамбля, состоящего из большого количества моделей. Лежащая в основе bsts методология является более общей и гибкой. 7.1 Методология Полное изложение реализованной в пакете bsts методологии можно найти в статье Scott and Varian (2014). Здесь мы ограничимся лишь кратким ее описанием, необходимым для уверенной работы с функциями этого пакета и их параметрами. Пусть \\(y_t\\) — это значение некоторой количественной переменной, учтенной в момент времени \\(t\\). Структурная модель временного ряда задается двумя уравнениями: \\[\\begin{aligned} &amp; y_t = Z_{t}^\\intercal \\alpha_t + \\epsilon_t \\qquad \\epsilon_t \\sim \\mathcal{N}(0, H_T),\\\\ &amp; \\alpha_{t+1} = T_t\\alpha_t + R_t\\eta_t \\qquad \\eta_t \\sim \\mathcal{N}(0, Q_T). \\end{aligned}\\] Предполагается, что в каждый момент времени \\(t\\) изучаемая динамическая система может находится в некотором ненаблюдаемом в явном виде (латентном) состоянии \\(\\alpha_t\\). Первое из приведенных уравнений связывает наблюдаемые данные с вектором таких латентных состояний и называется уравнением наблюдений (observation equation). Второе уравнение задает процесс перехода из одного латентного состояния в другое и называется уравнением переходов (transition equation). Состояние системы в каждый момент времени определяется только ее состоянием в предыдущий момент, т.е. динамика системы имеет марковский характер (рис. 7.1). Остатки \\(\\epsilon_t\\) и \\(\\eta_t\\) независимы друг от друга и имеют нормальное распределение со средним 0. Матрицы \\(Z_t\\), \\(T_t\\) и \\(R_t\\) называются структурными параметрами и обычно содержат как известные значения (индикаторные переменные 1 и 0), так и неизвестные параметры. Модели, которые можно описать с помощью приведенных двух уравнений, называют моделями пространства состояний. В этой форме можно представить очень широкий круг моделей, включая, например, все разновидности ARIMA (Цыплаков 2011). РИСУНОК 7.1: Схематичное представление модели пространства состояний для конечного временного ряда с \\(T\\) наблюдениями (см. объяснения в тексте) Структурные модели временных рядов — это одно из семейств моделей пространства состояний. В структурных моделях временной ряд представлен в виде суммы ненаблюдаемыех компонент, которые можно интерпретировать как тренд, сезонность, эффекты предикторов и т.д. Эти компоненты служат своего рода “строительными блоками”, которые исследователь может сочетать в соответствии с решаемой задачей и особенностями данных. Так, для временных рядов базовую структурную модель (basic structural model) с предикторами можно представить следующим образом: \\[ \\begin{aligned} &amp; y_t = \\underbrace{\\mu_t}_{\\text{тренд}} + \\underbrace{\\gamma_t}_{\\text{сезонность}} + \\underbrace{\\beta^\\intercal \\boldsymbol{x}_t}_{\\text{предикторы}} + \\epsilon_t,\\\\ &amp; \\mu_{t} = \\mu_{t-1} + \\delta_{t-1} + u_t,\\\\ &amp; \\delta_{t} = \\delta_{t-1} + v_t,\\\\ &amp; \\gamma_{t} = -\\sum_{s=1}^{S-1} \\gamma_{t-s} + w_t, \\end{aligned}\\] где \\(\\mu_{t}\\) — это текущее значение тренда модели, а \\(\\delta_t\\) — его коэффициент прироста. Сезонная компонента \\(\\gamma_t\\) представлена в виде \\(S-1\\) индикаторных переменных с изменяющимися во времени коэффициентами. В данном случае \\(\\eta_t = (u_t, v_t, w_t)\\) объединяет независимые нормально распределенные случайные колебания, \\(Q_T\\) — диагональная матрица, диагональ которой содержит \\(\\sigma_{u}^2\\), \\(\\sigma_{v}^2\\) и \\(\\sigma_{w}^2\\), а \\(H_T\\) — это скаляр \\(\\sigma_{\\epsilon}^2\\). Оцениванию на основе данных подлежат дисперсии \\(\\sigma_{u}^2\\), \\(\\sigma_{v}^2\\), \\(\\sigma_{w}^2\\), \\(\\sigma_{\\epsilon}^2\\), а также коэффициенты регрессии \\(\\beta\\). Подгонка структурных моделей временных рядов выполняется с использованием фильтра Калмана и метода Монте–Карло по схеме марковских цепей (Markov Chain Monte Carlo, MCMC). Для оценивания и одновременной регуляризации коэффициентов регрессии применяется т.н. метод “spike–and–slap” (найти подходящий аналог этому термину на русском языке сложно, поэтому оставим его без перевода). Вкратце, этот метод заключается в присваивании каждому коэффициенту регрессии определенной высокой априорной вероятности того, что он равен нулю (“вероятность включения” в модель, inclusion probability). Используя наблюденные данные и теорему Байеса, вероятности включения обновляют. В дальнейшем при MCMC–сэмплировании коэффициентов из полученных апостериорных распределений многие симулированные коэффициенты оказываются в точности равными нулю. Такой механизм регуляризации позволяет эффективно выполнить селекцию наиболее важных предикторов и параллельно избавиться от мультиколлинеарности, благодаря чему в байесовские структурные модели можно включать большое количество предикторов без риска переобучения. 7.2 Функция bsts() Функция bsts() генерирует упомянутые выше MCMC–выборки параметров структурных моделей временных рядов из соответствующих апостериорных распределений. Эта функция имеет следующие аргументы: formula — стандартная для R формула модели. Если предикторы в модели отсутствуют, то вместо формулы можно просто указать числовой вектор со значениями моделируемого временного ряда или объект класса zoo, xts или ts. Зависимая переменная может содержать пропущенные значения, но предикторы — нет. state.specification — список, элементы которого содержат спецификации компонент модели. Эти спецификации создаются с помощью функций, чье имя начинается с приставки Add (например, AddLocalLinerTrend(), AddSeasonal(), AddAr() — см. примеры ниже). family — параметр, определяющий “семейство” создаваемой модели в обычном для R смысле. По умолчанию предполагается, что моделируемая зависимая переменная является количественной и что ее остатки имеют гауссово распределение (\"gaussian\"). Кроме того, в пакете bsts имеется возможность моделировать счетные (family = \"poisson\") и бинарные отклики (family = \"logit\"), а также количественные переменные, в которых имеют место наблюдения–выбросы (family = \"student\"). Бинарный отклик можно подать в виде вектора со значениями 0/1, TRUE/FALSE или -1/1. Если бинарный отклик представлен числом положительных и отрицательных исходов, то его подают в виде матрицы с двумя столбцами (в первом столбце содержатся положительные исходы). Отклик со счетными данными (family = \"poisson\"), зарегистрированными в учетных единицах одинакового размера (например, на 1 м2) или продолжительности (например, за 1 ч), подают в виде вектора. Если же регистрация таких данных была выполнена в учетных единицах разного размера или продолжительности, то отклик подают в виде матрицы с двумя столбцами, первый из которых содержит результаты подсчета, а второй — размер или продолжительность учетной единицы. data — необязательный аргумент, задающий объект (таблица, список или окружение) с переменными, которые входят в формулу модели (аргумент formula). prior — список с информацией об априорных распределениях предикторов, созданный с помощью функции SpikeSlabPrior(). Этот аргумент нужен только если предикторы входят в формулу модели. Если предикторов в модели нет, то аргумент prior будет просто соответствовать априорному распределению стандартного отклонения остатков, которое можно задать самостоятельно с помощью функции SdPrior(). Обычно в этом аргументе нет необходимости, поскольку хорошие результаты получаются и с его принятыми по умолчанию настройками. contrasts — матрица контрастов модели. Этот аргумент идентичен такому же аргументу базовой функции lm(). na.action — задает правило обработки пропущенных наблюдений в зависимой переменной. По умолчанию препущенные наблюдения допускаются. Для их удаления на этот аргумент необходимо подать либо функцию na.omit(), либо функцию na.exclude() (без скобок, т.е. na.action = na.omit). niter — положительное целое число, задающее количество итераций алгоритма MCMC. ping — число, задающее частоту вывода на экран служебной информации о ходе выполнения алгоритма MCMС. Если это положительное число, то информация будет выводиться каждые ping итераций. model.options — настройки вычислительных алгоритмов, лежащих в основе функции bsts, которые можно задать с помощью функции BstsOptions(). Необходимость в изменении этих настроек возникает очень редко. timestamps — временные отметки регистрации значений моделируемой переменной. Этот аргумент бывает полезным, когда в данных имеются пропущенные значения, или когда на одну временную метку приходится несколько наблюдений зависимой переменной. Если в моделируемом временном ряду пропущенные значения отсутствуют и на каждую временную отметку приходится только одно наблюдение, то этот аргумент становится необязательным (по умолчанию он равен NULL). seed — зерно генератора случайных чисел (для воспроизводимости результатов вычислений). ... — дополнительные аргументы, которые передаются на функцию SpikeSlabPrior() (см. аргумент prior выше). Функция bsts() возвращает объект класса bsts, который представляет собой список с несколькими элементами. Содержимое этого списка определяется спецификацией конкретной модели, но как минимум он включает следующие элементы: state.specification — информация о компонентах модели, заданных с помощью одноименного аргумента функции bsts(); state.contributions — массив со значениями компонент модели, оцененными в ходе каждой из niter MCMC–итераций; one.step.prediction.errors — матрица с niter строками, содержащая т.н. “ошибки следующего шага” (см. объяснение в разд. 7.4); sigma.obs — вектор длиной niter со стандартными отклонениями остатков модели; log.likelihood — вектор длиной niter со значеними логарифма функции максимального правдоподобия модели. У объектов класса bsts есть несколько методов, включая стандартные summary(), plot() и predict(). Примеры использования этих методов приведены ниже. 7.3 Спецификация компонент модели Пакет bsts содержит целую библиотеку функций, формирующих различные компоненты структурных моделей временных рядов. Как было отмечено выше, имена всех этих функций начинаются с приставки Add, поскольку они “добавляют” спецификацию тех или иных компонент в соответствующий список. Этот список (обычно ему дают имя ss, что значит “state specification”, т.е. “спецификация состояний”) далее подается на функцию bsts(), которая непосредственно выполняет подгонку структурной модели. Основными аргументами Add–функций являются state.specification — список, в который необходимо добавить спецификацию соответствующей компоненты, и y — вектор со значениями моделируемой переменной. Приведенный ниже код поможет понять, как это работает: require(bsts) y &lt;- rnorm(100) # список со спецификациями компонент модели: ss &lt;- list() ss &lt;- AddLocalLevel(state.specification = ss, y = y) # ...или просто ss &lt;- AddLocalLevel(ss, y) ss &lt;- AddAr(ss, y, lags = 1) # подгонка модели: model &lt;- bsts(y, state.specification = ss, niter = 1500) Ниже перечислены наиболее важные Add–функции и соответствующие им компоненты структурных моделей (сравните с описанной выше базовой структурной моделью временных рядов). AddLocalLevel() — локальный уровень (local level), соответствующий процессу “случайного блуждания с шумом” (random walk with noise): \\[ \\begin{aligned} &amp; y_t = \\mu_t + \\epsilon_t,\\\\ &amp; \\mu_{t} = \\mu_{t-1} + u_t,\\\\ &amp; \\epsilon_t \\sim \\mathcal{N}(0, \\sigma_{\\epsilon}^2), u_t \\sim \\mathcal{N}(0, \\sigma_{u}^2) \\end{aligned} \\] AddAr() — авторегрессионный процесс (autoregressive process), согласно которому текущий средний уровень временного ряда линейно связан с его предыдущими значениями (вплоть до временного сдвига \\(p\\), который называют также порядком процесса): \\[ \\begin{aligned} &amp; y_t = \\mu_t + \\epsilon_t,\\\\ &amp; \\mu_{t} = \\sum_{i=1}^{p}\\phi_i \\mu_{t-i} + u_t,\\\\ &amp; \\epsilon_t \\sim \\mathcal{N}(0, \\sigma_{\\epsilon}^2), u_t \\sim \\mathcal{N}(0, \\sigma_{u}^2), \\end{aligned} \\] где \\(\\phi_1, \\dots, \\phi_{p}\\) — подлежащие оцениванию параметры модели. Для работы с большими значениями \\(p\\) в пакете bsts есть специальная функция AddAutoAr(), которая применяет метод “spike–and–slab” для регуляризации \\(\\phi_1, \\dots, \\phi_{p}\\) (т.е. многие из этих параметров в итоге окажутся в точности равными нулю). AddLocalLinearTrend() — локальный линейный тренд (local linear trend). Предполагает, что процессом “случайного блуждания” можно описать динамику как среднего уровня временного ряда \\(\\mu_t\\), так и коэффициента его прироста \\(\\delta_t\\) (“угол наклона” тренда) : \\[ \\begin{aligned} &amp; y_t = \\mu_t + \\epsilon_t,\\\\ &amp; \\mu_{t} = \\mu_{t-1} + \\delta_{t-1} + u_t,\\\\ &amp; \\delta_{t} = \\delta_{t-1} + v_t,\\\\ &amp; \\epsilon_t \\sim \\mathcal{N}(0, \\sigma_{\\epsilon}^2), u_t \\sim \\mathcal{N}(0, \\sigma_{u}^2), v_t \\sim \\mathcal{N}(0, \\sigma_{v}^2) \\end{aligned} \\] AddStudentLocalLinearTrend() — устойчивый локальный линейный тренд (robust local linear trend). Отличается от предыдущей компоненты тем, что ее случайные колебания подчиняются распределению Стьюдента, а не Гаусса. Эта компонента хорошо подходит для краткосрочных прогнозов на основе временных рядов, в которых иногда наблюдаются резкие скачки среднего уровня. Распределенные по Стьюденту остатки можно ввести также в уравнение наблюдений структурной модели (см. выше), подав аргумент family = \"student\" на функцию bsts(). Как было отмечено выше, это позволит получать более надежные прогнозы в присутствии аномальных наблюдений. AddSemiLocalLinearTrend() — полулокальный линейный тренд. Похож на модель локального уровня, но предполагает, что коэффициент прироста среднего уровня ряда развивается в соответствии с авторегрессионным процессом первого порядка: \\[ \\begin{aligned} &amp; y_t = \\mu_t + \\epsilon_t,\\\\ &amp; \\mu_{t} = \\mu_{t-1} + \\delta_{t-1} + u_t,\\\\ &amp; \\delta_{t} = D + \\phi \\times (\\delta_{t-1} - D) + v_t,\\\\ &amp; \\epsilon_t \\sim \\mathcal{N}(0, \\sigma_{\\epsilon}^2), u_t \\sim \\mathcal{N}(0, \\sigma_{u}^2), v_t \\sim \\mathcal{N}(0, \\sigma_{v}^2) \\end{aligned} \\] Стационарный авторегрессионный процесс первого порядка более стабилен, чем процесс случайного блуждания, что делает модель полулокального линейного тренда более подходящей для расчета долгосрочных прогнозов (благодаря меньшей неопределенности в отношении предсказанных значений). AddSeasonal() — сезонная компонента (seasonal component), число сезонов \\(S\\) в которой задается с помощью аргумента nseasons. В основе этой компоненты лежит линейная модель с \\(S-1\\) индикаторными переменными \\[ \\begin{aligned} &amp; y_t = \\gamma_t + \\epsilon_t,\\\\ &amp; \\gamma_{t} = -\\sum_{s=1}^{S-1} \\gamma_{t-s} + w_t,\\\\ &amp; \\epsilon_t \\sim \\mathcal{N}(0, \\sigma_{\\epsilon}^2), w_t \\sim \\mathcal{N}(0, \\sigma_{w}^2) \\end{aligned} \\] Для моделирования физических и других процесов с четко выраженными амплитудой и частотой можно воспользоваться также функцией AddTrig(), которая задает соответствующую компоненту в виде суммы элементарных тригонометрических составляющих (cos и sin) с варьирующими во времени коэффициентами. AddRegressionHoliday() — добавляет в модель эффекты “праздников” и других важных для моделируемого временного ряда событий. Список таких событий формируется с помощью нескольких вспомогательных функций (например, NamedHoliday(), FixedDateHoliday(), DateRangeHoliday и др.) и подается на аргумент holiday.list функции AddRegressionHoliday(). В основе этой функции лежит простая модель следующего вида: \\[ \\begin{aligned} &amp; y_t = \\beta_{d(t)} + \\epsilon_t,\\\\ &amp; \\epsilon_t \\sim \\mathcal{N}(0, \\sigma_{\\epsilon}^2), \\end{aligned} \\] где вектор \\(\\beta\\) содержит коэффициенты регрессии \\(\\beta_d \\sim \\mathcal{N}(0, \\sigma^2)\\). 7.4 Примеры моделей без предикторов Для иллюстрации основ работы с пакетом bsts мы воспользуемся данными по стоимости номера в гостинице 73738 из таблицы hotels (см. подразд. 1.5.4). Эти данные обладают несколькими свойствами, которые делают задачу прогнозирования достаточно трудной (рис. 7.2): крайне нерегулярная регистрация наблюдений с точностью до 1 секунды, что приводит к большому количеству пропущенных значений и “маскировке” возможных сезонных колебаний; относительно непродолжительный период учета данных (8 месяцев), что в совокупности с многочисленными пропущенными значениями затрудняет адекватное моделирование тренда и сезонных колебаний. require(dplyr) require(ggplot2) hotels %&gt;% filter(prop_id == 73738) %&gt;% ggplot(aes(date_time, price_usd)) + geom_line() + geom_point(alpha = 0.4) + scale_y_log10() + theme_minimal() + ylim(c(0, NA)) РИСУНОК 7.2: Исходные данные по динамике стоимости номера в гостинице 73738 из таблицы hotels Перечисленные трудности можно отчасти преодолеть путем агрегирования данных с секундного до некоторого более крупного интервала времени (например, путем расчета средней стоимости номера, зарегистрированной за минуту, час, день и т.п.). Выбор подходящего интервала для агрегирования — это отдельная задача, которая, к сожалению, не имеет простого решения. С одной стороны, чем шире интервал (например, дневной), тем меньше в агрегированных данных будет пропущенных наблюдений, но некоторые свойства исходных данных при этом будут утрачены (в частности, внутридневные колебания стоимости). С другой стороны, агрегирование по более узкому интервалу поможет сохранить свойства исходных данных, но не решит проблему большого количества пропущенных наблюдений. В приведенных ниже примерах агрегирование выполнено путем усреднения логафмированных значений стоимости гостиничного номера за день. Даже при таком подходе в данных все еще остается несколько пропущенных наблюдений. Хотя лежащая в основе пакета bsts методология достаточно устойчива к наличию небольшого количества пропущенных наблюдений в зависимой переменной, для упрощения примеров мы выполним простую линейную этих пропусков в обучающей выборке, как это было сделано в гл. 4. Проверочная выборка будет содержать 14 наблюдений, что соответствует прогнозному горизонту длиной 14 дней (рис. 7.3). # Агрегирование данных: h73738 &lt;- hotels %&gt;% filter(prop_id == 73738) %&gt;% index_by(dt = as.Date(date_time)) %&gt;% summarise(y = mean(log(price_usd))) %&gt;% tsibble::fill_gaps() # Разбиение на обучающую и проверочную выборки # (прогнозный горизонт составляет 14 дней): cut_point &lt;- as.Date(max(h73738$dt)) - 14 h73738_train &lt;- h73738 %&gt;% filter(as.Date(dt) &lt;= cut_point) %&gt;% mutate(y = forecast::na.interp(y)) h73738_test &lt;- h73738 %&gt;% filter(as.Date(dt) &gt; cut_point) # Визуализация агрегированных данных: rbind(mutate(h73738_train, dataset = &quot;train&quot;), mutate(h73738_test, dataset = &quot;test&quot;)) %&gt;% ggplot(aes(dt, y, col = dataset)) + geom_line() + geom_point(alpha = 0.4) + theme_minimal() + scale_color_manual(values = c(&quot;blue&quot;, &quot;black&quot;)) РИСУНОК 7.3: Обучающие (черный цвет) и проверочные (синий цвет) данные по динамике среднедневной стоимости номера в гостинице 73738. Пропуски в обучающих данных восстановлены путем интепорляции. К проверочным данным такое восстановление не применялось, в связи с чем соединяющая точки линия имеет два разрыва На рис. 7.3 виден слабый возрастающий тренд в данных, который мы попытаемся отразить в модели с помощью компоненты линейного локального тренда (функция AddLocalLinearTrend()). Кроме того, мы добавим в модель компоненту недельной сезонности (функция AddSeasonal(nseasons = 7)), поскольку дневные данные часто демонстрируют такого рода колебания: require(bsts) # Моделируемая переменная: y_73738 &lt;- h73738_train$y # временные отметки для аргумента `timestamps` функции `bsts()` # (не обязательны, но полезны для построения графиков): dt_73738 &lt;- h73738_train$dt # Спецификация компонент модели: ss &lt;- list() ss &lt;- AddLocalLinearTrend(ss, y_73738) ss &lt;- AddSeasonal(ss, y_73738, nseasons = 7) # Подгонка модели (может занять некоторое время). # Результат сохранен в объект с именем `M19` # (в продолжение нумерации модельных объектов, # начатой в главе о пакете `prophet`): M19 &lt;- bsts(y_73738, ss, timestamps = dt_73738, niter = 1500, ping = 0, seed = 42) Полученный объект M19 обладает рядом удобных методов, позволяющих исследовать свойства полученной модели. Так, стандартный метод summary() возвращает список с несколькими количественными метриками качества модели, рассчитанными по обучающим данным: summary(M19) ## $residual.sd ## [1] 0.174605 ## ## $prediction.sd ## [1] 0.1956826 ## ## $rsquare ## [1] 0.3094038 ## ## $relative.gof ## [1] 0.3437019 В приведенном списке residual.sd — это среднее значение апостериорного распределения стандартного отклонения остатков модели, а rsquare — обычный коэффициент детерминации (т.е. доля, которую дисперсия остатков составляет от обшей дисперсии в данных). Остальные две метрики рассчитываются с использованием т.н. “ошибок следующего шага” (one-step-ahead errors), которые вычисляются в ходе подгонки модели как \\(y_t - E(y_t|Y_{t-1}, \\theta)\\), где \\(Y_{t-1} = y_1, y_2, \\dots y_{t-1}\\), а \\(\\theta\\) — вектор с текущими оценками параметров модели. При необходимости эти ошибки можно извлечь с помощью функции bsts.prediction.errors(), подав на нее соответствующий модельный объект. Возвращаемая методом summary() метрика prediction.sd представляет собой стандартное отклонение ошибок следующего шага, рассчитанных по обучающим данным, а relative.gof — это т.н. статистика Харви (Harvey’s goodness of fit statistic). Статистика Харви похожа на коэффициент детерминации и вычисляется как \\(R_D^2 = 1 - \\sum \\nu^2 /(n-2) \\times \\text{var}(\\text{diff}(y))\\), где \\(\\nu\\) — ошибки следующего шага, \\(n\\) — число наблюдений \\(y\\) в анализируемом временном ряду, а \\(\\text{var}\\) и \\(\\text{diff}\\) — функции для расчета дисперсии и перехода к разностям (дифференцированию) временного ряда соответственно. Качество bsts–моделей можно проанализировать также графически с помощью диаграмм нескольких типов. Так, стандартный метод plot() изображает обучающие данные и оцененное на их основе апостериорное совместное распределение компонент модели в каждый момент времени (\\(Z_t^\\intercal \\alpha_t\\); см. рис. 7.4 и разд. 7.1): plot(M19) РИСУНОК 7.4: Результат подгонки модели M19. Кружками показаны обучающие данные. Полупрозрачными черными точками показано апостериорное совместное распределение компонент модели. Области плотного расположения этих точек выглядят темнее и соответствуют областям с большей апостериорной вероятностью. На каждую временную отметку приходится niter = 1500 таких точек Метод plot() в сочетании с аргументом y = \"components\" позволяет также изобразить вклад отдельных компонент модели (рис. 7.5; этот метод принимает и ряд других аргументов — см. справочный файл, доступный по команде ?plot.bsts): plot(M19, y = &quot;components&quot;) РИСУНОК 7.5: Апостериорные распределения компонент модели M19. Слева: локальный линейный тренд. Справа: эффекты дней недели Поскольку все графики на рис. 7.5 имеют одинаковые оси ординат, то эффекты дней недели рассмотреть очень сложно. Мы можем изобразить их отдельно с помощью следующей команды (рис. 7.6): plot(M19, y = &quot;seasonal&quot;) РИСУНОК 7.6: Апостериорные распределения эффектов дней недели, оцененные с помощью модели M19. Зеленые линии соответствуют медианам этих распределений На рис. 7.6 лучше видно, что цена на номер в гостинице 73738 в среднем была несколько выше в субботу (Saturday), воскресенье (Sunday) и понедельник (Monday), чем в другие дни, и что эффекты каждого из дней недели были практически постоянны на протяжении всего исторического периода. Тем не менее вклад сезонной компоненты в целом оказался весьма незначительным, по сравнению с вкладом линейного тренда, что согласуется также с результатом разведочного анализа этих данных, выполненного в гл. 4. Важным свойством хорошей модели временного ряда является отсутствие автокорреляции в ее остатках. Для визуальной проверки этого служит команда AcfDist(), которая принимает матрицу с остатками модели и строит диаграммы размахов (“ящики с усами”) для апостериорных распределений автокорреляционной функции. В идеале центры этих апостериорных распределений (начиная со сдвига 1 и далее) должны приходится на 0, однако в случае с моделью M19 это явно не так (рис. 7.7): хорошо видна цикличность с периодом, составляющим примерно 7 временных шагов. Похожее свойство этих данных мы ранее обнаружили также в ходе их разведочного анализа (гл. 4). m19_resid &lt;- residuals(M19) AcfDist(draws = m19_resid) РИСУНОК 7.7: Апостериорные распределения автокорреляционной функции остатков модели M19 Попробуем улучшить модель M19, добавив в нее авторегрессионный процесс и одновременно исключив сезонную компоненту. Обратите внимание на то, что для добавления авторегрессионной компоненты в приведенном ниже коде использована функция AddAutoAr(), которая, в отличие от AddAr(), применяет метод “spike–and–slab” для регуляризации авторегрессионных коэффициентов: пользователь задает максимально возможное количество таких коэффициентов (с помощью аргумента lag), но некоторые из них в отдельных MCMC–итерациях окажутся в точности приравнены к нулю. ss &lt;- list() ss &lt;- AddLocalLinearTrend(ss, y_73738) ss &lt;- AddAutoAr(ss, y_73738, lag = 7) M20 &lt;- bsts(y_73738, ss, timestamps = dt_73738, niter = 1500, ping = 0, seed = 42) summary(M20) ## $residual.sd ## [1] 0.09866681 ## ## $prediction.sd ## [1] 0.1874644 ## ## $rsquare ## [1] 0.7794775 ## ## $relative.gof ## [1] 0.3970419 AcfDist(residuals(M20)) РИСУНОК 7.8: Апостериорные распределения автокорреляционной функции остатков модели M20 Как следует из приведенных выше диагностических метрик и из рис. 7.8, модель M20 гораздо лучше описывает обучающие данные, чем модель M19. Это видно также на рис. 7.9, где изображены обучающие данные и апостериорное совместное распределение компонент M20, и на рис. 7.10, изображающем апостериорные распределения отдельных компонент этой модели: plot(M20) РИСУНОК 7.9: Результат подгонки модели M20 plot(M20, y = &quot;components&quot;) РИСУНОК 7.10: Апостериорные распределения компонент модели M20. Слева: локальный линейный тренд. Справа: авторегрессионная компонента Качество нескольких bsts-моделей можно одновременно проанализирвать с помощью функции CompareBstsModels(), которая изображает накопленные средние абсолютные ошибки следующего шага для каждой из сравниваемых моделей. Под графиком таких кривых накопленных ошибок изображаются также исходные обучающие данные, что позволяет лучше понять, где именно та или иная модель плохо справляется с описанием данных. На рис. 7.11 кривая накопленных ошибок модели M20 проходит ниже кривой M19, что еще раз подтверждает более высокое качество модели M20. models_to_compare &lt;- list(&quot;M19&quot; = M19, &quot;M20&quot; = M20) CompareBstsModels(models_to_compare) РИСУНОК 7.11: Пример сравнения качества двух моделей (M19 и M20) с помощью ошибок следующего шага. Вверху: накопленные средние абсолютные ошибки следующего шага. Внизу: обучающие данные 7.5 Модели с предикторами Как было отмечено в разд. 7.1, помимо стандартных компонент тренда и сезонности в структурные модели временных рядов можно включить также эффекты предикторов. Для этого необходимо воспользоваться аргументом formula функции bsts() и указать обычным для R образом имена соответствующих переменных справа от значка тильды (например y ~ x1 + x2 + x3, где x1, x2 и x3 — это имена предикторов). Все предикторы должны содержаться в одной таблице данных, которая подается на функцию bsts() с помощью аргумента data. Воспользуемся в качестве предикторов стоимостью номеров в двух других гостиницах (с идентификаторами 13252 и 83045) из набора данных hotels. Хотя зависимой переменной в bsts–моделях разрешается иметь пропущенные значения, наличие пропусков в предикторах недопустимо. Мы уже сталкивались с аналогичным требованием при построении prophet–моделей (разд. 6.7), где пропущенные наблюдения в предикторах были восстановлены с помощью метода LOCF (подразд. 1.5.3). Здесь мы воспользуемся другим подходом и восстановим пропущенные значения на основе отдельных bsts–моделей, подогнанных к данным по стоимости номеров в соответствующих гостиницах. Такой подход часто дает реалистичные оценки пропущенных наблюдений, что является одним из больших преимуществ байесовских методов анализа временных рядов. В приведенном ниже коде происходит подготовка каждого временного ряда предиктора к моделированию и подгонка соответствующих моделей, структура которых идентична таковой у модели M20: # Подготовка данных: h &lt;- hotels %&gt;% filter(prop_id != 73738) %&gt;% group_by_key(prop_id) %&gt;% index_by(dt = as.Date(date_time)) %&gt;% summarise(y = mean(log(price_usd))) %&gt;% tsibble::fill_gaps() %&gt;% filter(as.Date(dt) &lt;= cut_point) # Модель для гостиницы `13252` y_13252 &lt;- h %&gt;% filter(prop_id == 13252) %&gt;% pull(y) dt_13252 &lt;- h %&gt;% filter(prop_id == 13252) %&gt;% pull(dt) ss &lt;- list() ss &lt;- AddLocalLinearTrend(ss, y_13252) ss &lt;- AddAutoAr(ss, y_13252, lag = 7) M13252 &lt;- bsts(y_13252, ss, timestamps = dt_13252, niter = 1500, ping = 0, seed = 42) # Модель для гостиницы `83045` y_83045 &lt;- h %&gt;% filter(prop_id == 83045) %&gt;% pull(y) dt_83045 &lt;- h %&gt;% filter(prop_id == 83045) %&gt;% pull(dt) ss &lt;- list() ss &lt;- AddLocalLinearTrend(ss, y_83045) ss &lt;- AddAutoAr(ss, y_83045, lag = 7) M83045 &lt;- bsts(y_83045, ss, timestamps = dt_83045, niter = 1500, ping = 0, seed = 42) Результаты подгонки обеих моделей приведены на рис. 7.12. par(mfrow = c(2, 1)) plot(M13252, main = &quot;M13252&quot;) plot(M83045, main = &quot;M83045&quot;) РИСУНОК 7.12: Результаты подгонки моделей по данным стоимости номеров в гостиницах 13252 и 83045 В пакете bsts нет готовой функции, позволяющей извлекать предсказанные значения для обучающих данных (подобно базовой функции fitted()). Но не беда — мы можем легко рассчитать эти значения самостоятельно по результатам MCMC-симуляций вклада каждой компоненты модели. Эти результаты хранятся в слоте state.contributions модельного объекта в виде массива размерностью \\(i \\times s \\times l\\), где \\(i\\) — количество итераций MCMC, \\(s\\) — число входящих в модель компонент, а \\(l\\) — длина временного ряда (включая пропущенные значения). В рассматриваемом нами примере \\(i = 1500\\), \\(s = 2\\), а \\(l = 228\\): M13252$state.contributions %&gt;% dim() ## [1] 1500 2 228 M83045$state.contributions %&gt;% dim() ## [1] 1500 2 228 Для расчета модельного значения стоимости гостиничного номера на определенной временной отметке необходимо суммировать вклады всех компонент модели, соответствующих этой отметке. Для каждой временной отметки мы в итоге получим 1500 оценок стоимости гостиничного номера. Наиболее вероятную оценку можно было бы далее рассчитать путем простого усреднения этих 1500 значений. Однако, как это обычно бывает при выполнении MCMC-симуляций, первые несколько итераций алгоритма во время его “разогрева” (“burn–in”) дают неустойчивые оценки моделируемой величины. В связи с этим принято такие начальные итерации исключать из анализа. Функция SuggestBurn() из пакета bsts вычисляет количество итераций MCMC, которые следует отбросить: # Аргумент `proportion` задает число последних MCMC-итераций # (в виде доли от общего количества итераций), которые # используются для нахождения периода &quot;разогрева&quot; алгоритма (burn_13252 &lt;- SuggestBurn(proportion = 0.1, M13252)) ## [1] 78 (burn_83045 &lt;- SuggestBurn(proportion = 0.1, M83045)) ## [1] 112 Рассчитаем предсказанные моделями M13252 и M83045 наиболее вероятные значения стоимости гостиничных номеров, отбросив первые 78 и 112 МСМС–оценок соответственно, и изообразим результаты на графиках (рис. 7.13): fitted_M13252 &lt;- M13252$state.contributions %&gt;% apply(., MARGIN = 3, FUN = rowSums) %&gt;% .[-(1:burn_13252), ] %&gt;% colMeans() fitted_M83045 &lt;- M83045$state.contributions %&gt;% apply(., MARGIN = 3, FUN = rowSums) %&gt;% .[-(1:burn_83045), ] %&gt;% colMeans() par(mfrow = c(2, 1)) plot(fitted_M13252, main = &quot;M13252&quot;, type = &quot;l&quot;, ylim = c(4.6, 6.4)) points(h %&gt;% filter(prop_id == 13252) %&gt;% pull(y), col = &quot;blue&quot;) plot(fitted_M83045, main = &quot;M83045&quot;, type = &quot;l&quot;, ylim = c(4.6, 6.4)) points(h %&gt;% filter(prop_id == 83045) %&gt;% pull(y), col = &quot;blue&quot;) РИСУНОК 7.13: Модельные значения (сплошные линии) и обучающие данные по стоимости номеров в гостиницах 13252 и 83045 (кружки) Заменим теперь пропущенные данные по стоимости номеров в гостиницах 13252 и 83045 их модельными значениями (см. столбец y_imputed): require(tidyr) predictors &lt;- bind_rows(tibble(prop_id = 13252, dt = dt_13252, y_fitted = fitted_M13252), tibble(prop_id = 83045, dt = dt_83045, y_fitted = fitted_M83045)) %&gt;% inner_join(., h, by = c(&quot;prop_id&quot;, &quot;dt&quot;)) %&gt;% mutate(y_imputed = ifelse(is.na(y), y_fitted, y), prop_id = paste0(&quot;y_&quot;, prop_id)) %&gt;% dplyr::select(-y, -y_fitted) %&gt;% pivot_wider(names_from = prop_id, values_from = y_imputed) predictors ## # A tibble: 228 x 3 ## dt y_13252 y_83045 ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2012-11-01 5.21 5.37 ## 2 2012-11-02 5.28 5.65 ## 3 2012-11-03 5.16 5.40 ## 4 2012-11-04 5.26 5.39 ## 5 2012-11-05 5.39 5.39 ## 6 2012-11-06 5.28 5.44 ## 7 2012-11-07 5.47 5.41 ## 8 2012-11-08 5.32 5.34 ## 9 2012-11-09 5.47 5.29 ## 10 2012-11-10 5.58 5.26 ## # ... with 218 more rows В моделируемом нами временном ряду стоимости номеров в гостинице 73738 есть одно аномально низкое значение, приходящееся на временную отметку 2013-01-17 (рис. 7.3). Причина появления этого аномального наблюдения, к сожалению, не известна. Хотя модель M20 ранее вполне успешно “справилась” с этим наблюдением без каких–либо специальных действий с нашей стороны (рис. 7.9), мы могли бы также представить его в явном виде с помощью индикаторной переменной, которая принимает значение 1 на отметке 2013-01-17 и 0 на всех остальных отметках. Добавим такую индикаторную переменную в таблицу predictors: anomaly_dt &lt;- h73738_train %&gt;% filter(y == min(y, na.rm = TRUE)) %&gt;% pull(dt) predictors &lt;- predictors %&gt;% mutate(anomaly_event = ifelse(dt == anomaly_dt, 1, 0)) Построим теперь новую модель стоимости номеров в гостинице 73738, в которую помимо использованных нами ранее компонент войдут также независимые переменные из таблицы predictors: # Добавление предикторов в таблицу с исходными данными: h73738_train &lt;- h73738_train %&gt;% left_join(predictors, ., by = &quot;dt&quot;) ss &lt;- list() ss &lt;- AddLocalLinearTrend(ss, y_73738) ss &lt;- AddAutoAr(ss, y_73738, lag = 7) M21 &lt;- bsts(y ~ y_13252 + y_83045 + anomaly_event, ss, data = h73738_train, timestamps = dt_73738, niter = 1500, seed = 42, ping = 0) print(summary(M21), digits = 3) ## $residual.sd ## [1] 0.0529 ## ## $prediction.sd ## [1] 0.16 ## ## $rsquare ## [1] 0.937 ## ## $relative.gof ## [1] 0.562 ## ## $size ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.00 1.00 1.00 1.04 1.00 3.00 ## ## $coefficients ## mean sd mean.inc sd.inc inc.prob ## anomaly_event -1.44e+00 0.14375 -1.43979 0.1438 1.0000 ## y_13252 2.82e-04 0.00394 0.01500 0.0250 0.0188 ## y_83045 8.82e-05 0.00379 0.00504 0.0288 0.0175 ## (Intercept) 0.00e+00 0.00000 0.00000 0.0000 0.0000 Приведенная сводка по модели M21 несколько отличается от виденных нами ранее. В одном из новых элементов сводки — size — приведена таблица с описательными статистиками по количеству предикторов, включенных в модель на разных итерациях алгоритма МСМС. Как было отмечено выше (разд. 7.1), в ходе подгонки модели выполняется регуляризация ее размера с помощью метода “spike–and–slab”, в результате чего не все предикторы входят во все МСМС–реализации. В нашем примере видно, что в большинстве случаев в модель был включен лишь один предиктор (см. медианное значение). Другой новый элемент в сводке — coefficients — содержит описательные статистики по оцененным коэффициентам каждого из предикторов. Особенно важным показателем здесь является т.н. апостериорная вероятность включения предиктора в модель (posterior inclusion probability, см. столбец inc.prob). Как следует из названия, это вероятность использования того или иного предиктора в отдельных MCMC–реализациях модели, в т.ч. при расчете прогнозных значений (см. ниже). Хорошо видно, что наиболее важным предиктором оказалась индикаторная переменная anomaly_event, тогда как переменные y_13252 и y_83045 имеют достаточно низкую вероятность быть включенными в модель. Апостериорные вероятности включения предикторов можно также изобразить графически при помощи следующей простой команды (рис. 7.14): plot(M21, y = &quot;coefficients&quot;) РИСУНОК 7.14: Апостериорные вероятности включения трех предикторов в модель M21. Чем темнее заштриховка столбца, тем более вероятно, что соответствующая переменная отрицательно коррелирует с зависимой переменной. (Intercept) обозначает свободный член регрессионного уравнения 7.6 Выбор оптимальной модели До сих пор мы оценивали адекватность bsts–моделей по тому, насколько хорошо они описывали обучающие данные. Безусловно, такой подход сопровождается высоким риском выбрать в качестве оптимальной переобученную модель. Диагностика с использованием ошибок следующего шага (разд. 7.4; см. также справочный файл по функции bsts.prediction.errors()) лишь отчасти помогает застраховаться от этого, и единственным объективным тестом качества модели всегда будет точность ее предсказаний на независимом наборе данных. Объекты класса bsts обладают стандартным методом predict(), основными аргументами которого являются следующие (подробнее см. справочный файл, доступный по команде ?predict.bsts): object — модельный объект; horizon — целое число, задающее длину прогнозного горизонта; newdata — вектор, матрица или таблица со значениями предикторов (если таковые имеются); timestamps — вектор с временными отметками, соответствующими строкам в newdata, для которых необходимо рассчитать прогнозные значения; plot.original — количество последних наблюдений из обучающих данных, которые необходимо изобразить на графике; na.action — функция, которую следует применить к пропущенным значениям в newdata; quantiles — вектор с двумя значениями, задающими нижний и верхний лимиты доверительного интервала прогнозных значений (по умолчанию с(0.025, 0.975), что соответствует 95%–ному интервалу); seed — зерно генератора случайных чисел, которое можно указать для воспроизводимости результатов. Полученные с помощью метода predict() прогнозные значения можно также визуализировать с помощью метода plot() (рис. 7.15 и 7.16): M20_pred &lt;- predict(M20, horizon = 14) plot(M20_pred) РИСУНОК 7.15: Обучающие данные (черная линия) и прогнозные значения, полученные с помощью модели M20. Синей линией показаны наиболее вероятные будущие значения временного ряда. Вокруг этой линии полупрозрачными черными точками показаны также другие возможные реализации будущих значений. Зеленые прерывистые линии ограничивают 95%–ный доверительный интервал предсказанных значений plot(M20_pred, plot.original = 50) with(h73738_test, points(dt, y, pch = 19, col = &quot;yellow&quot;)) РИСУНОК 7.16: То же, что на рис. 7.15, но с изображением только последних 50 наблюдений из обучающих данных. Желтыми точками показаны также наблюденные данные из проверочной выборки, что позволяет визуально оценить качество прогноза Рассчитаем теперь прогнозные значения для всех трех простроенных нами моделей и сравним эти предсказания с данными из проверочной выборки, которые хранятся в таблице h73738_test (разд. 7.4). В качестве метрики для выбора оптимальной модели применим cреднюю абсолютную удельную ошибку предсказаний (MAPE; подразд. 6.8.3). Поскольку bsts–модели предсказывают большое количество возможных реализаций будущих значений зависимой переменной, то для расчета MAPE мы воспользуемся медианными значениями этих возможных реализаций. Метод predict() автоматически вычисляет такие значения и сохраняет их в элементе median возвращаемого списка: # Функция для расчета средней абсолютной удельной ошибки: mape &lt;- function(observed, predicted){ na_ind &lt;- which(is.na(observed)) observed &lt;- na.omit(observed) predicted &lt;- predicted[-na_ind] mean(abs(observed - predicted)/observed) } # Предсказания модели M19: M19_pred &lt;- predict(M19, horizon = 14) %&gt;% .$median # Предсказания модели M20: M20_pred &lt;- M20_pred$median # Для получения предсказаний с помощью модели M21 # нам сначала необходимо создать таблицу с будущими # значениями всех входящих в нее предикторов: predictors_future &lt;- tibble( dt = h73738_test$dt, y_13252 = predict(M13252, horizon = 14) %&gt;% .$median, y_83045 = predict(M83045, horizon = 14) %&gt;% .$median, anomaly_event = 0 ) M21_pred &lt;- predict(M21, horizon = 14, newdata = predictors_future) %&gt;% .$median # MAPE для всех трех моделей: sapply(list(&quot;M19&quot; = M19_pred, &quot;M20&quot; = M20_pred, &quot;M21&quot; = M21_pred), mape, observed = h73738_test$y) %&gt;% round(., 4) ## M19 M20 M21 ## 0.0194 0.0157 0.0163 Как следует из приведенных результатов, оптимальной следует считать модель M20. Интересно, что модель M21, несмотря на наличие в ней дополнительных предикторов, оказалась на втором месте по качеству прогноза. Подобная ситуация, когда включение предикторов не сопровождается улучшением качества модели временного ряда, довольно обычна и может быть обусловлена многими факторами (слабая связь между предикторами и моделируемой переменной, ошибки, возникающие при расчете прогнозных значений самих предикторов, эффект переобучения и т.п.). Возможности пакета bsts для прогнозирования временных рядов намного шире описанных в этой вводной главе. Дополнительную информацию о пакете и примеры его использования можно найти в статье Scott (2017), а также на сайте Стивена Скотта. "],
["ch-structural-changes.html", "ГЛАВА 8 Выявление структурных изменений 8.1 Метод “E–Divisive with Medians” (EDM) 8.2 Функция breakout() 8.3 Примеры использования функции breakout()", " ГЛАВА 8 Выявление структурных изменений Практически любая промышленная система сегодня снабжена совокупностью сенсоров, датчиков и других измерительных устройств, позволяющих вести учет ее состояния (часто в реальном времени). Количество таких измерительных устройств и учитываемых с их помощью показателей может быть очень большим, что требует автоматизации процесса мониторинга и своевременного оповещения о нестандартных (например, предаварийных и аварийных) ситуациях. Подобные нестандартные ситуации обычно сопряжены с существенными изменениям во временных рядах, описывающих те или иные показатели. Одним из типов таких изменений является сдвиг среднего уровня временного ряда, который может быть как очень резким, так и постепенным (рис. 8.1). Существует несколько методов, позволяющих автоматически обнаруживать подобные структурные изменения во временных рядах. Одним из них является метод “E–Divisive with Medians” (EDM), разработанный исследователями из Корнеллского университета и компании Twitter (James, Kejariwal, and Matteson 2015) и реализованный в пакете для R BreakoutDetection. В этой главе приведено краткое описание основ работы с данным пакетом. РИСУНОК 8.1: Примеры структурных изменений во временных рядах: резкое (вверху) и плавное (внизу) Пакет BreakoutDetection можно установить только из репозитория GitHub, для чего достаточно воспользоваться функцией install_github() из пакета devtools (который, конечно, тоже нужно сначала установить, если у вас его еще нет): install.packages(&quot;devtools&quot;) devtools::install_github(&quot;twitter/BreakoutDetection&quot;) 8.1 Метод “E–Divisive with Medians” (EDM) Подробное описание теории, лежащей в основе EDM, приведено в статье James, Kejariwal, and Matteson (2015). Вкратце, этот метод основан на использовании т.н. \\(E\\)–статистики (от “energy”, т.е. “энергия”), которая характиризует расстояние между наблюдениями из случайных выборок. Чем больше эта статистика, тем больше оснований считать, что сравниваемые выборки происходят из разных генеральных совокупностей. Во избежание влияния обычных на практике аномальных наблюдений, метод EDM оперирует не средними значениями, а медианами. Более того, это непараметрический метод, т.е. он не исходит из предположения о принадлежности анализируемых наблюдений к каким–либо конкретным распределениям вероятностей. Непараметрическая природа EDM, а также использование медианы в качестве меры центральной тенденции, делают этот метод особенно подходящим для обнаружения структурных изменений во временных рядах при решении практических задач. 8.2 Функция breakout() В пакете BreakoutDetection есть всего лишь одна функция — breakout(), которая имеет следующие аргументы: Z — временной ряд, который может быть представлен либо в виде числового вектора, либо в виде таблицы данных со столбцами timestamp (“временная отметка”) и count (“количество”). min.size — минимальное количество наблюдений, которое должно иметь место между точками излома тренда во временном ряду (по умолчанию равно 30). method — название применяемого к данным сценария по обнаружению структурных изменений. Принимает два возможных значения: \"amoc\" (от “at most one change”, т.е. “не более одного изменения”) и \"multi\" (несколько точек излома). ... — дополнительные аргументы. Набор дополнительных аргументов (...) зависит от значения параметра method. В обоих случаях (т.е. и \"amoc\", и \"multi\") функция примет следующие аргументы: plot — логический значение, включающее (TRUE) или отключающее (FALSE) построение графика временного ряда с отметками позиций обнаруженных точек излома тренда; xlab и ylab — подписи осей графика; title — заголовок графика. Если method = \"amoc\", то функция breakout() примет также: alpha — параметр, используемый в EDM для придания разных весов расстояниям между наблюдениями. Принимает любое значение из интервала (0, 2] (по умолчанию alpha = 2). exact — логический аргумент. По умолчанию равен TRUE, что приводит к расчету точных значений медиан при нахождении точек излома (в случае с длинными рядами это может сопровождаться длительными вычислениями). sig.lvl — желаемый уровень статистической значимости теста для обнаруженной точки излома тренда (по умолчанию равен 0.05). nperm — количество итераций при выполнении перестановочного теста. По умолчанию nperm = 0, т.е. перестановочный тест не выполняется. Когда требуется обнаружить несколько точек излома тренда (method = \"multi\"), на функцию breakout() можно подать также следующие аргументы, задающие уровень регуляризации в отношении количества и близости взаимного расположения этих точек: degree — параметр, определяющий близость расположения точек излома. Принимает значения 0, 1 (принято по умолчанию) и 2. Чем больше этот параметр, тем меньше точек излома располагаются близко друг к другу. percent — задает минимальное относительное (%) увеличение внутреннего критерия качества EDM (здесь не рассматривается), необходимое для классификации того или иного наблюдения как точки излома тренда. Например, значение 0.25 соответствует 25%–ному увеличению критерия. Аргумент percent не имеет значения, принятого по умолчанию. beta — принимает любое числовое значение (по умолчанию равен 0.008) и используется для регуляризации если degree и/или percent не заданы. Чем больше этот параметр, тем меньше точек излома будет обнаружено алгоритмом. 8.3 Примеры использования функции breakout() Применим функцию breakout() с ее принятыми по умолчанию параметрами к данным по стоимости биткоина (подразд. 1.5.2): require(BreakoutDetection) BO0 &lt;- breakout(log(bitcoin$y), plot = TRUE, ylab = &quot;y&quot;) BO0$plot РИСУНОК 8.2: Точка излома тренда, обнаруженная во временном ряду стоимости биткоина с помощью модели BO0 Как видно на рис. 8.2, метод EDM обнаружил одну единственную точку излома тренда (отмечена вертикальной прерывистой линией), что обусловлено принятым по умолчанию аргументом method = \"amoc\" (см. выше). Для оценивания статистической значимости этого сдвига во временном ряду следует воспользоваться аргументом nperm. Выполним такое оценивание с помощью перестановочного теста с 1000 итераций (выполнение этой команды займет какое–то время): BO0_perm &lt;- breakout(log(bitcoin$y), nperm = 1000) Объект BO0_perm представляет собой список со следующими элементами: BO0_perm ## $loc ## [1] 561 ## ## $stat ## [1] 107.6948 ## ## $time ## [1] 44.78 ## ## $pval ## [1] 0.000999001 где loc — это положение точки излома (т.е. ее порядковый номер во временном ряду), stat — значение E-критерия, time — время выполнения команды (сек.), а pval — \\(p\\)–значение, полученное с помощью перестановочного теста. Как видим, обнаруженный сдвиг уровня временного ряда оказался статистически значимым (на уровне значимости 0.05). Запросим теперь нахождение нескольких точек излома (рис. 8.3): BO1 &lt;- breakout(log(bitcoin$y), method = &quot;multi&quot;, plot = TRUE, ylab = &quot;y&quot;) BO1$plot РИСУНОК 8.3: Точки излома тренда, обнаруженные во временном ряду стоимости биткоина с помощью модели BO1 Рассмотрим теперь влияние параметров регуляризации. Сначала отключим регуляризацию с помощью параметра degree = 0 (рис. 8.4): BO2 &lt;- breakout(log(bitcoin$y), method = &quot;multi&quot;, degree = 0, plot = TRUE, ylab = &quot;y&quot;) BO2$plot РИСУНОК 8.4: Точки излома тренда, обнаруженные во временном ряду стоимости биткоина с помощью модели BO2 Как было отмечено выше, порядковые номера наблюдений, соответствующих обнаруженным точкам излома, хранятся в элементе loc получаемого объекта–списка: BO2$loc ## [1] 43 74 113 158 203 246 302 484 680 755 840 879 927 957 992 ## [16] 1048 1144 1201 Похожего эффекта (увеличение количества обнаруженных точек излома) можно добиться также с помощью низкого значения параметра percent (рис. 8.5): BO3 &lt;- breakout(log(bitcoin$y), method = &quot;multi&quot;, percent = 0.05, plot = TRUE, ylab = &quot;y&quot;) BO3$plot РИСУНОК 8.5: Точки излома тренда, обнаруженные во временном ряду стоимости биткоина с помощью модели BO3 Наконец, снижение параметра beta тоже приведет к увеличению числа обнаруженных алгоритмом точек излома (рис. 7): BO4 &lt;- breakout(log(bitcoin$y), method = &quot;multi&quot;, beta = 0.0001, plot = TRUE, ylab = &quot;y&quot;) BO4$plot РИСУНОК 8.6: Точки излома тренда, обнаруженные во временном ряду стоимости биткоина с помощью модели BO4 Итак, мы рассмотрели принципы работы с пакетом BreakoutDetection, в котором реализован один из наиболее эффективных методов обнаружения структурных изменений во временных рядах — “E-Divisive with Medians”. Как было отмечено выше, этот метод особенно хорошо подходит для использования в автоматических системах мониторинга параметров технических систем (именно для этого он изначально был разработан в компании Twitter). Другие примеры, где EDM может оказаться полезным, включают: анализ результатов, полученных в ходе A/B тестирования (аргумент method со значением \"amoc\" позволит оценить влияние экспериментального фактора на сдвиг интересующей исследователя переменной–отклика); обнаружение изменений в переменных, описывающих поведение людей (например, активность в социальных сетях) или животных и т.п. Хотя пакет BreakoutDetection очень прост в использовании и дает надежные результаты, следует отметить, что он, к сожалению, не обновлялся с 2014 г. Об этом стоит помнить, если вы решите использовать его в своих проектах. "],
["ch-anomaly-detection.html", "ГЛАВА 9 Выявление аномалий 9.1 Автоматическое обнаружение аномалий 9.2 Ручная настройка параметров для обнаружения аномалий 9.3 Одновременный анализ нескольких временных рядов", " ГЛАВА 9 Выявление аномалий Необходимость обнаружения необычных наблюдений (выбросов, или аномалий) во временных рядах часто возникает в таких ситуациях, как мониторинг состояния оборудования, отслеживание неожиданных колебаний на рынке ценных бумаг, учет показателей состояния здоровья пациентов и т.д. В этой главе мы рассмотрим один из инструментов для решения подобных задач — пакет anomalize. Для описания основ работы с этим пакетом воспользуемся данными по стоимости гостиничных номеров из таблицы hotels (подразд. 1.5.4). 9.1 Автоматическое обнаружение аномалий Для начала попробуем обнаружить необычные наблюдения во временном ряду одной из гостиниц (напрмиер, с идентификатором prop_id = 13252). Для этого к таблице с данными (класса tibble, tibbletime или tsibble) необходимо последовательно применить команды с участием следующих функций из пакета anomalize: time_decompose() — выполняет декомпозицию временного ряда на отдельные составляющие (сезонную, тренд и остатки; см. гл. 4); anomalize() — применяет к остаткам один из двух реализованных в пакете методов выявления аномалий (см. ниже); time_recompose() — восстанавливает исходный временной ряд, параллельно вычисляя верхнюю и нижнюю границы диапазона, в который входят “нормальные” наблюдения. Получаемый в итоге объект имеет следующую структуру: require(anomalize) result_13252 &lt;- hotels %&gt;% filter(prop_id == 13252) %&gt;% time_decompose(price_usd, merge = TRUE) %&gt;% anomalize(remainder) %&gt;% time_recompose() Разберемся подробнее, что именно произошло в результате выполнения приведенных выше команд: Результатом применения time_decompose() к исходным данным стал расчет четырех столбцов: observed (исходные наблюдения моделируемой переменной), season (сезонная составляющая временного ряда), trend (тренд) и remainder (остатки). Параметр merge = TRUE привел к тому, что эти новые столбцы были добавлены к исходным данным. Функция anomalize() была применена к столбцу remainder и рассчитала три новых столбца — remainder_l1 и remainder_l2 (нижняя и верхняя границы, за пределами которых наблюдения считаются аномалиями), а также индикаторную переменную anomaly, которая принимает значение \"No\", если наблюдение не является выбросом, и \"Yes\" если является. Функция time_recompose() восстанавила исходный временной ряд с использованием значений из season, trend, remainder_l1 и remainder_l2 и попутно рассчитала два новых столбца: recomposed_l1 и recomposed_l2, которые соответствуют верхней и нижней границам диапазона “нормальных” наблюдений. Суть всех этих вычислений станет понятнее, если мы представим результаты графически. Функция plot_anomaly_decomposition() изображает компоненты временного ряда, рассчитываемые функцией time_decompose(). На получаемом графике аномалии показаны в виде обведенных кружком красных точек (рис. 9.1): result_13252 %&gt;% plot_anomaly_decomposition() РИСУНОК 9.1: Результат автоматического обнаружения аномалий во временном ряду, выполненного с помощью пакета anomalize Еще одна функция — plot_anomalies() — позволяет отдельно изобразить исходный временной ряд и обнаруженные в нем необычные наблюдения (рис. 9.2): result_13252 %&gt;% plot_anomalies() РИСУНОК 9.2: Аномальные наблюдения, автоматически обнаруженные во временном ряду с помощью пакета anomalize Поскольку plot_anomalies() возвращает графический объект класса ggplot, то при желании мы можем легко добавить к нему другие ggplot–элементы обычным в таких случаях способом. Добавим, например, линию, последовательно соединяющую все точки (рис. 9.3): result_13252 %&gt;% plot_anomalies() + geom_line() РИСУНОК 9.3: Пример сочетания функции plot_anomalies() из пакета anomalize и функции geom_line() из пакета ggplot2 9.2 Ручная настройка параметров для обнаружения аномалий При создании объекта result_13252 все вычисления были выполнены с использованием настроек, заданных в пакете anomalize по умолчанию. Часто для получения приемлемого результата этого будет достаточно. Однако, в зависимости от свойств анализируемого временного ряда, иногда потребуется более тонкая настройка соответствующих параметров. Рассмотрим сначала параметры функции time_decompose(), которая выполняет разложение ряда на отдельные компоненты. Результат работы этой функции в значительной мере зависит от значений следующих ее аргументов: frequency — определяет “частоту” сезонной компоненты, т.е. число наблюдений, входящих в один сезонный цикл (например, для дневных данных этот аргумент по умолчанию будет равен \"1 week\", т.е. одной неделе); trend — определяет число наблюдений в отдельных отрезках временного ряда, используемых для расчета тренда (например, для дневных данных по умолчанию trend = \"3 months\", т.е. три месяцам); method — метод, используемый для разложения ряда на компоненты. У этого аргумента есть два возможных значения: \"stl\" (принято по умолчанию) и \"twitter\". STL — это классический метод разложения временных рядов на компоненты, в основе которого лежит сглаживание данных по методу нелинейной локальной регрессии (гл. 4). Метод\"twitter\" — более новый и, как следует из его названия, был предложен исследователями из компании Twitter (Vallis, Hochenbaum, and Kejariwal 2014). В рамках этого метода тренд оценивается путем вычисления медиан в пределах отдельных интервалов времени, на которые разбивает исходный ряд (их длина определяется параметром trend — см. выше). По умолчанию аргументы frequency и trend принимают значения \"auto\" (автоматический режим). В этом случае для выбора конкретных подходящих ситуации значений frequency и trend функция time_decompose() обращается к другой функции — get_time_scale_template(), которая возвращает “справочник” типичных значений: get_time_scale_template() ## # A tibble: 8 x 3 ## time_scale frequency trend ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 second 1 hour 12 hours ## 2 minute 1 day 14 days ## 3 hour 1 day 1 month ## 4 day 1 week 3 months ## 5 week 1 quarter 1 year ## 6 month 1 year 5 years ## 7 quarter 1 year 10 years ## 8 year 5 years 30 years Видим, например, что в случае с данными, где промежуток между наблюдениями выражается в секундах, параметры frequency и trend будут выражаться в часах и по умолчанию составят \"1 hour\" (1 час) и \"12 hours\" (12 часов) соответственно. Следует отметить однако, что в зависимости от интервала между наблюдениями и длины самого временного ряда, конкретные выбранные программой значения frequency и trend необязательно окажутся равными приведенным выше значениям из “справочника”. Именно это произошло выше при создании объекта result_13252 (frequency = 2 hours, trend = 61 hours). Помимо строковых значений (\"1 hour\", \"2 days\", \"2 weeks\" и т.д.) обоим параметрам можно присваивать и числовые значения (см. ниже) — часто это оказывается более удобным подходом. Отметим также, что “шаблонные” значения параметров frequency и trend можно изменить глобально с помощью функции set_time_scale_template() (пример не приводится). Для отмены автоматического выбора значений frequency и trend достаточно указать желаемые значения при вызове функции time_decompose(). В приведенном ниже примере мы присвоим этим параметрам значения 2 и 6 (часов) соответственно: hotels %&gt;% filter(prop_id == 13252) %&gt;% time_decompose(price_usd, frequency = 2, trend = 6) %&gt;% anomalize(remainder) %&gt;% time_recompose() %&gt;% plot_anomaly_decomposition() РИСУНОК 9.4: Результат обнаружения аномалий во временном ряду, выполненного с помощью пользовательских настроек функции time_decompose() Как видно на рис. 9.4, уменьшение параметров frequency и trend привело к классификации большего количества наблюдений как аномалий. Это обычный результат в таких случаях, обусловленный переобучением Loess–модели тренда (обратите внимание на то, какой извилистой стала кривая тренда на рис. 9.4 по сравнению с рис. 9.1). В отличие от Loess, второй метод разложения временных рядов — \"twitter\" — более устойчив к локальным колебаниям и поэтому при тех же значениях frequency и trend обычно относит к аномалиям меньше наблюдений. Это легко проверить (сравните рис. 9.4 и рис. 9.5): hotels %&gt;% filter(prop_id == 13252) %&gt;% time_decompose(price_usd, frequency = 2, trend = 6, method = &quot;twitter&quot;) %&gt;% anomalize(remainder) %&gt;% time_recompose() %&gt;% plot_anomaly_decomposition() РИСУНОК 9.5: Результат обнаружения аномалий во временном ряду, выполненного по методу twitter Функция anomalize позволяет выбрать один из двух методов выявления аномальных наблюдений, а также настроить чувствительность этих методов. Для этого служат следующие аргументы функции: method — принимает два возможных значения в соответствии с названием метода обнаружения аномалий: \"iqr\" (принято по умолчанию) и \"gesd\". Метод IQR (от “interquartile range”, т.е. “интерквартильный размах”) работает быстрее, но не так точен, как GESD (Generalized Extreme Studentized Deviate test). alpha — контролирует ширину диапазона “нормальных” значений (по умолчанию равен 0.05). Чем меньше этот параметр, тем шире диапазон значений, рассматриваемых как “нормальные”, и тем меньше наблюдений будут классифицированы как аномальные. max_anoms — максимальная доля наблюдений, которые позволено отнести к аномалиям (0.2 по умолчанию, т.е. 20%). Рассмотрим влияние параметра alpha, снизив его значение с принятого по умолчанию 0.05 до 0.025. При этом оставим параметры frequency и trend функции time_decompose() такими же, как на рис. 9.4: hotels %&gt;% filter(prop_id == 13252) %&gt;% time_decompose(price_usd, frequency = 2, trend = 6, method = &quot;twitter&quot;) %&gt;% anomalize(remainder, alpha = 0.025) %&gt;% time_recompose() %&gt;% plot_anomaly_decomposition() РИСУНОК 9.6: Результат обнаружения аномалий во временном ряду, выполненного с пониженным значением параметра alpha функции anomalize() Как видно на рис. 9.1, несмотря на все еще имеющее место переобучение Loess–модели тренда, к классу аномальных было отнесено намного меньшее количество наблюдений. Аналогичного эффекта можно было бы добиться также путем снижения параметра max_anoms функции anomalize() (пример не приводится). 9.3 Одновременный анализ нескольких временных рядов Отличительной особенностью пакета anomalize является возможность параллельно анализировать несколько временных рядов на наличие аномалий. Для этого достаточно в таблице с данными иметь группирующую переменную, которая идентифицирует отдельные ряды. В нашем примере с ценами на гостиничные номера такой одновременный анализ выглядел бы следующим образом (параметрам всех функций здесь присвоены их автоматически выбранные значения; обратите внимание на аргумент time_recompose = TRUE функции plot_anomalies() — он позволяет включить изображение на графике диапазона, в который входят “нормальные” наблюдения): hotels %&gt;% group_by_key() %&gt;% time_decompose(price_usd) %&gt;% anomalize(remainder) %&gt;% time_recompose() %&gt;% plot_anomalies(time_recomposed = TRUE) РИСУНОК 9.7: Результат обнаружения аномалий во временном ряду, выполненного с пониженным значением параметра alpha функции anomalize() Стоит подчеркнуть, что в пакете anomalize нет возможности настраивать параметры функций для отдельных рядов при таком параллельном анализе. Поэтому параллельный анализ имеет смысл выполнять только на рядах, охватывающих примерно одинаковый период, имеющих одинаковую периодичность регистрации наблюдений и, конечно же, описывающих динамику той же переменной. В нашем примере с ценами на гостиничные номера все эти условия выполняются. На этом мы завершим рассмотрение основных возможностей пакета anomalize для обнаружения необычных наблюдений во временных рядах. Дополнительные примеры работы с этим пакетом, а также более подробное описание лежащих в его основе методов, можно найти в официальной документации. "],
["ch-ts-clustering-task.html", "ГЛАВА 10 Задача кластеризации временных рядов", " ГЛАВА 10 Задача кластеризации временных рядов Задача кластеризации состоит в разбиении исходной совокупности анализируемых объектов на отдельные группы (кластеры) таким образом, чтобы различия между объектами внутри групп были минимальнымми, а различия между группами максимальными. В этой книге под такими объектами мы будем понимать одномерные временные ряды. Вот примеры лишь нескольких практических задач, требущих кластеризации временных рядов: нахождение финансовых показателей со сходной динамикой, объединение пациентов в однородные группы по форме электрокардиограммы, классификация типов активности человека по показаниям “умных часов”, обнаружение домов со сходными профилями потребления электроэнергии и т.п. Подобно кластерному анализу других объектов, качество кластеризации временных рядов определяется выбором следующих важных элементов: мера расстояния, описывающая степень различий между рядами; алгоритм разбиения рядов на кластеры. Существует большое количество алгоритмов для выполнения кластеризации, но чаще всего (по крайней мере, для относительно небольших объемов данных) на практике применяют иерархическую кластеризацию, метод \\(k\\)–средних и разбиение по медоидам (Partition Around Medoid, PAM). При этом выделяют следующие три подхода для вычисления мер расстояния между временными рядами (Liao 2005): По исходным данным (raw data–based approach): для вычисления меры расстояния используют исходные значения анализируемых временных рядов (во временной или частотной областях). Регистрация значений сравниваемых рядов, как правило, выполнятся через одинаковые промежутки времени, однако длина этих рядов не обязательно должна быть одинаковой. По описательным признакам (feature–based approach): в рамках этого подхода сначала выполняется снижение размерности путем вложения (embedding) анализируемых временных рядов в пространство, образованное их описательными признаками (например, как это было сделано в гл. 5). Вычисление расстояния между отдельными рядами далее выполняется по значениям этих признаков. По результатам подгонки моделей: в рамках этого подхода делается предположение, что анализируемые временные ряды были порождены процессом, который можно аппроксимировать моделью с определенным набором параметров. Похожими считаются ряды с близкими значениям оцененных параметров (например, коэффициентов) такой модели. Помимо параметров модели для расчета расстояния между отдельными рядами могут использоваться также остатки - либо в исходном виде, либо после снижения их размерности подобно описанному выше второму подходу. В литературе можно встретить несколько десятков мер расстояния, используемых в кластерном анализе. К стандартным мерам относятся евклидово расстояние, манхэттенское расстояние, расстояние Минковского и коэффициент корреляции Пирсона. В случае с временными рядами важным является также т.н. DTW–расстояние — мера, название которой происходит от лежащего в ее основе алгоритма динамической трансформации временной шкалы (Dynamic Time Warping). Поскольку кластерный анализ принадлежит к методам обучения без учителя (unsuprevised learning), то практически невозможно сделать объективное заключение о “правильности” получаемых с его помощью решений. Как правило, на практике выбирают тот результат, который “имеет смысл” с точки зрения решаемой задачи. Тем не менее, существует целый ряд метрик, пытающихся описать качество кластеризации количественно. Это дает возможность сравнить разные решения и выбрать наиболее “оптимальное” из них, в связи с чем расчет подобных метрик качества часто является одной из стадий кластерного анализа. По аналогии с центроидами или медоидами, получаемыми при кластеризации других объектов, полезным результатом кластеризации временных рядов является выделение т.н. “прототипов” (prototypes), т.е. наиболее представительных рядов для каждой из найденных групп. Визуализация прототипов помогает наглядно представить наиболее типичную форму временных рядов в каждом кластере. Кроме того, прототипы можно использовать для классификации, т.е. для отнесения новых временных рядов к той или иной группе. Выбор способа вычисления прототипов тесно связан с выбором меры расстояния и алгоритма кластеризации. Существует несколько таких способов, однако чаще всего используется либо простое усреднение всех рядов в кластере по каждой временной отметке, либо выбор такого временного ряда из кластера, который максимально близок (согласно выбранной мере расстояния) ко всем остальным рядам в этом кластере. Последний подход применятся в случае с упомянутой выше кластеризацией по методу PAM. В следующих главах мы рассмотрим примеры кластеризации временных рядов с использованием всех трех перечисленных выше подходов. "],
["ch-ts-clustering-by-raw-data.html", "ГЛАВА 11 Кластеризация по исходным данным 11.1 DTW-расстояние 11.2 Пакет dtwclust 11.3 Пример кластеризации с использованием DTW–расстояния", " ГЛАВА 11 Кластеризация по исходным данным Кластеризация по исходным данным подойдет для ситуаций, когда количество анализируемых временных рядов и их длина относительно невелики (иначе могут потребоваться значительные вычислительные ресурсы). Расстояние между двумя временными рядами можно выразить при помощи многих мер, однако, как показано ниже, не все меры одинаково хорошо подходят для этого. 11.1 DTW-расстояние В кластерном анализе и многих других приложениях для описания различий между объектами широко используется евклидово расстояние. В случае с двумя временными рядами \\(x_i\\) и \\(y_i\\), имеющими одинаковый интервал регистрации \\(n\\) наблюдений, оно вычисляется следущим образом (см. также рис. 11.1): \\[ d_E = \\sqrt{\\sum_{i=1}^n (x_{i} - y_{i})^2}.\\] РИСУНОК 11.1: Евклидово расстояние между двумя временными рядами вычисляется как квадратный корень из суммы квадратов расстояний от каждого \\(i\\)–го наблюдения одного ряда до \\(i\\)–го наблюдения другого ряда (показаны светло-серыми вертикальными линиями). В качестве примера использованы данные по стоимости криптовалют ethereum (верхний ряд) и litecoin (нижний ряд) за первые 6 месяцев 2019 г. К сожалению, евклидово расстояние не всегда является адекватной мерой различий между двумя временными рядами, поскольку оно плохо отражает форму сравниваемых рядов. Для пояснения этого утверждение предположим, что мы создали копию некоторого временного ряда и сдвинули ее по оси времени вправо на несколько шагов. Рассчитав евклидово расстояние между оригиналом и копией, мы бы выяснили, что оно существенно отличается от нуля. Хотя это и ожидаемый результат, в определенной степени он противоречит интуиции — ведь форма обоих рядов идентична! Единственная разница между ориналом и копией в том, что регистрация наблюдений оригинала началась на несколько временных шагов раньше. Для решения этой проблемы были разработаны специальные меры расстояния, среди которых особое место занимает т.н. DTW–расстояние, основанное на алгоритме динамической трансформации временной шкалы (Dynamic Time Warping). Впервые эта мера была применена в 1970–х гг. в задаче по распознаванию речи для устранения проблемы, связанной с разной скоростью произнесения одинаковых фраз разными людьми. Как следует из его названия, алгоритм DTW трансформирует временную шкалу (растягивает или сжимает) с целью достичь оптимального сопоставления (optimal matching), или оптимального выравнивания (optimal alignment), двух последовательностей. Проще говоря, ставится задача расположить два временных ряда относительно друг друга таким образом, чтобы расстояние между ними оказалось минимальным. Предположим, что мы сравниваем два временных ряда \\(Q = (q_1, q_2, \\dots, q_n)\\) и \\(R = (r_1, r_2, \\dots, r_m)\\) (эти обозначения происходят от часто используемых в англоязычной литературе терминов “query” — “запрос”, и “reference” — “эталон”). Символами \\(i = 1 \\dots n\\) и \\(j = 1 \\dots m\\) обозначим соответствующие индексные номера наблюдений из этих двух рядов. Алгоритм DTW включает две основные стадии. На первой стадии составляют т.н. матрицу локальных потерь \\(lcm\\) (от “Local Cost Matrix”) порядка \\(n \\times m\\), каждый элемент \\(lcm(i, j)\\) которой содержит расстояние между парой наблюдений \\(q_i\\) и \\(r_j\\). Обычно используют евклидово или манхэттенское расстояния, хотя можно воспользоваться и другими подходящими случаю мерами. На второй стадии находят такой путь трансформации (warping path) \\(\\phi = \\{(1, 1), \\dots, (n, m)\\}\\) через эту матрицу, который минимизирует суммарное расстояние между рядами \\(Q\\) и \\(R\\). Это расстояние вычисляется следующим образом: \\[DTW(Q, R) = \\underset{\\phi}{min} \\left(\\sum \\frac{m_{\\phi} lcm(k)}{M_{\\phi}}\\right), \\forall k \\in \\phi,\\] где \\(m_{\\phi} &gt; 0\\) — весовой коэффициент, ассоциированный с каждым элементом \\(\\phi\\), а \\(M_{\\phi}\\) — константа, нормализующая длину пути (Giorgino 2009). Поскольку число возможных путей трансформации экпоненциально возрастает с длиной сравниваемых последовательностей, то для нахождения оптимального пути за конечное время вычислений вводят ряд ограничений (constraints). Одно из важных локальных ограничений требует, чтобы путь трансформации был монотонным, т.е. на каждом шаге пути индексам \\(i\\) и \\(j\\) разрешается только возрастать. Это ограничение задается с помощью т.н. шаговых паттернов (step patterns), которые определяют направление разрешенных переходов между ячейками таблицы \\(lcm\\) на каждом шаге. В литературе можно встретить несколько таких шаговых паттернов (Sarda-Espinosa 2019), однако чаще всего используются symmetric1 и symmetric2 (рис. 11.2). РИСУНОК 11.2: Наиболее распространенные шаговые паттерны, используемые для нахождения оптимального пути трансформации в алгоритме DTW. Линии, соединяющие точки, соответствуют разрешенными направлениям перехода между ячейками таблицы \\(lcm\\) на каждом шаге пути, а числа рядом с этими линиями — весовым коэффициентам переходов \\(m_{\\phi}\\) Помимо локальных вводятся также и глобальные ограничения. Важнейшим из них является ограничение на ширину окна трансформации (warping window), в пределах которого разрешается прокладывать путь трансформации. Разработано множество разновидностей таких окон, но чаще всего используется т.н. окно Сакэ–Чиба (Sakoe–Chiba window), которое охватывает определенную симметричную область вдоль диагонали матрицы \\(lcm\\) (рис. 11.3). РИСУНОК 11.3: Пример окна Сакэ–Чиба На рис. 11.4 показан стандартный способ визулизации матрицы локальных потерь \\(lcm\\) (прямоугольник в центре диаграммы), найденного пути трансформации (линия, проходящая примерно по диагонали матрицы) и сравниваемых временных рядов. Индексацию матрицы принято начинать в левом нижнем углу прямоугольника (\\(lcm(1, 1)\\)), а заканчивать в правом верхнем (\\(lcm(m, n)\\)). Слева от прямоугольника принято изображать “эталон” (reference), т.е. временной ряд, с которым сравнивается изображенный внизу матрицы “запросный” временной ряд (query). Для построения диаграммы был использован пакет dtw (Giorgino 2009). require(dtw) alignment &lt;- dtw( x = ether_lite_wide$litecoin, # &quot;запрос&quot; y = ether_lite_wide$ethereum, # &quot;эталон&quot; step.pattern = symmetric1, # шаговый паттерн window.type = &quot;sakoechiba&quot;, # тип окна трансформации window.size = 7, # размер окна keep.internals = TRUE) # сохранение промежуточных вычислений plot(alignment, type = &quot;threeway&quot;, main = &quot;&quot;, xlab = &quot;Индексный номер эталона&quot;, ylab = &quot;Индексный номер запроса&quot;) РИСУНОК 11.4: Визулизация оптимального пути DTW–трансформации, найденного при сопоставлении временных рядов стоимости криптовалют ethereum и litecoin На рис. 11.5 приведен результат оптимального сопоставления временных рядов стоимости криптовалют ethereum и litecoin с помощью алгоритма DTW. plot(alignment, type = &quot;twoway&quot;, xlab = &quot;Индексный номер эталона&quot;, ylab = &quot;Запросный ряд&quot;) РИСУНОК 11.5: Результат оптимального сопоставления временных рядов стоимости криптовалют ethereum (красная прерывистая линия) и litecoin (черная сплошная линия), полученный с помощью алгоритма DTW (сравните с рис. 11.1). В алгоритме были использованы параметры, приведенные в коде для рис. 11.4 11.2 Пакет dtwclust В R есть несколько пакетов для выполнения кластерного анализа и почти любой из них можно было бы использовать для кластеризации временных рядов. Однако, как это обычно бывает, удобнее иметь один пакет, в котором эффективным образом реализовано большинство подходов, необходимых для решения конкретного круга задач. В случае с временными рядами одним из таких пакетов является dtwclust, который мы и будем использовать в рассматриваемых ниже примерах. В пакете dtwclust (Sarda-Espinosa (2019)) реализовано большое количество алгоритмов кластеризации временных рядов, включая несколько недавно разработанных методов. За исключением классической иерархической кластеризации, код для всех этих алгоритмов написан специально с целью добиться максимальной скорости вычислений. Вычислительная оптимизация алгоритмов была выполнена исходя из предположения, что в большинстве случаев пользователи будут применять DWT–расстояние в качестве меры различий между временными рядами (отсюда название пакета). Однако имеется возможность использовать и другие меры расстояния, включая пользовательские. Кроме того, с помощью dtwclust можно одновременно сравнить несколько алгоритмов и выбрать оптимальное решение на основе одной или нескольких метрик качества кластеризации. Главной функцией пакета dtwclust является tsclust(), у которой есть следующие аргументы: series — список из нескольких временных рядов (не обязятельно одинаковой длины), матрица с числовыми значениями или таблица данных. Поскольку все функции dtwclust так или иначе работают со списками временных рядов, то лучше всего на этот аргумент подавать сразу именно такой список. Для формирования списка временных рядов из матриц и таблиц данных можно применить вспомогательную функцию tslist() (при этом предполагается что каждая строка в матрице или таблице данных представляет собой один временной ряд). type — тип кластеризации: \"partitional\" (с разбиением), \"hierarchical\" (иерархическая), \"tadpole\" (по алгоритму TADPole; Begum et al. 2016) или \"fuzzy\" (нечеткая кластеризация). k — запрашиваемое число кластеров. Это может быть также вектор из нескольких целых чисел, если стоит задача проверить качество кластеризации для нескольких решений. preproc — имя функции, выполняющей предварительную обработку данных. По умолчанию принимает значение NULL. Если аргументу centroid (см. ниже) присвоено значение \"shape\" (“форма”), то preproc автоматически примет значение zscore, что соответствует функции из пакета dtwclust, которая выполняет стандартизацию данных. distance — название меры расстояния, зарегистрированное в регистре функции dist() из пакета proxy (см. объяснение ниже). Если type = \"tadpole\", то этот аргумент будет проигнорирован. centroid — либо строковое значение с названием метода нахождения центроидов (\"mean\", \"median\", \"shape\" и др.), либо функция, выполняющая нахождение центроидов. control — список с перечнем управляющих параметров алгоритма, заданного через аргумент type. Для формирования подобных списков служат специальные функции: partitional_control(), hierarchical_control(), fuzzy_control() и tadpole_control() (см. справочный файл, доступный по команде ?\"tadpole_control\" и примеры ниже). args — вложенный список параметров, определяющих процесс подготовки данных к анализу (элемент списка с именем preproc), расчета меры расстояния (элемент dist) и центроидов (прототипов) кластеров (элемент cent). Для формирования этого списка служит специальная функция tsclust_args(). seed — значение зерна генератора случайных чисел (позволяет воспроизвести результаты вычислений). trace — логический аргумент (по умолчанию равен FALSE). При значении TRUE на экран будет выводиться больше служебной информации о ходе вычислений. error.check — логический аргумент (по умолчанию равен TRUE). Включает или выключает проверку правильности значений перечисленных выше аргументов перед началом вычислений. Функция tsclust() возвращает объект класса TSClusters, который реализован с использованием S4 — одной из разновидностей классов, применяемых в R для объектно-ориентированного программирования. Отдельные атрибуты объектов класса S4 (“слоты”) можно извлекать с помощью оператора @ (в отличие от обычного $, принятого в S3–объектах). Подробную документацию по классу TSClusters и его методами можно найти в справочных файлах, доступных по командам ?\"TSClusters-class\" и ?\"tsclusters-methods\" соответственно. Пакет dtwclust во многом полагается на другой широко используемый пакет — proxy (автоматически устанавливается и загружается одновременно с dtwclust), в котором реализованы многие меры расстояния. Различная информация о функциях, вычисляющих соответствующие меры расстояния, хранится в специальном объекте–регистре под названием pr_DB. Содержимое регистра можно просмотреть с помощью команды proxy::pr_DB$get_entries() (в целях экономии места пример не приводится). Все зарегистрированные в pr_DB функции автоматически становятся доступными для функции proxy::dist(), которая используется в пакете dtwcust. Это является большим преимуществом дизайна dtwclust, поскольку пользователь получает возможность работать с любой мерой расстояния из pr_DB. 11.3 Пример кластеризации с использованием DTW–расстояния Рассмотрим особенности работы с пакетом dtwclust на примере иерархической кластеризации временных рядов стоимости 22 криптовалют из таблицы cryptos: require(dtwclust) # Преобразование исходных данных в список с 22 элементами, # содержащими отдельные временные ряды: cryptos_list &lt;- cryptos %&gt;% mutate(y = log(y)) %&gt;% pivot_wider(., names_from = coin, values_from = y) %&gt;% arrange(ds) %&gt;% dplyr::select(-ds) %&gt;% as.list() # Кластеризация: hc_4_ward &lt;- tsclust( cryptos_list, k = 4, # запрашиваемое число кластеров type = &quot;hierarchical&quot;, # тип кластеризации distance = &quot;dtw&quot;, # мера расстояния seed = 42, control = hierarchical_control(method = &quot;ward.D2&quot;), # метод агломерации args = tsclust_args(dist = list(window.size = 7)) # размер окна Сакэ-Чиба ) hc_4_ward ## hierarchical clustering with 4 clusters ## Using dtw distance ## Using PAM (Hierarchical) centroids ## Using method ward.D2 ## ## Time required for analysis: ## user system elapsed ## 14.86 1.86 16.73 ## ## Cluster sizes with average intra-cluster distance: ## ## size av_dist ## 1 5 875.9981 ## 2 7 1340.3730 ## 3 4 1644.0947 ## 4 6 875.1360 Полученный объект hc_4_ward принадлежит к упомянутому выше классу TSClusters и имеет несколько полезных атрибутов. Например, следующим образом можно просмотреть, к какому кластеру принадлежит каждый временной ряд: hc_4_ward@cluster ## augur bitcoin cardano chainlink dash decred dogecoin eos ## 1 2 3 4 2 1 3 1 ## ethereum iota litecoin maker monero nano neo qtum ## 2 4 2 2 2 4 1 1 ## stellar tether tezos tron xrp zcash ## 3 4 4 3 4 2 Объект hc_4_ward обладает также удобным методом plot(), позволяющим визулизировать полученное кластерное решение. Поскольку мы имеем дело с иерархической кластеризацией, то по умолчанию будет изображена дендрограмма (рис. 11.6): par(mar = c(0, 4, 2, 2)) plot(hc_4_ward, xlab = &quot;&quot;, sub = &quot;&quot;, main = &quot;&quot;) РИСУНОК 11.6: Результат иерархической кластеризации 22 временных рядов, выполненной с использованием DTW–расстояния Применив функцию plot() с аргументом type = \"sc\", мы получим изображение анализируемых временных рядов, сгруппированных в соответствии с результатом кластеризации (рис. 11.7). Кроме того, для каждого кластера в виде прерывистой линии будет показан его центроид, или “прототип”. По умолчанию центроиды представляют собой такие временные ряды из исходных данных, которые в среднем ближе всего ко всем остальным рядам из соответствующих кластеров. При необходимости алгоритм расчета центроидов можно изменить с помощью аргумента centroid функции tsclust(). plot(hc_4_ward, type = &quot;sc&quot;) # &quot;sc&quot; значит &quot;series and centroids&quot; РИСУНОК 11.7: Временные ряды стоимости 22 криптовалют, сгруппированные в соответствии с кластером, к которому они принадлежат Интересно, что для 4–го кластера в качестве центроида алгоритмом был выбран временной ряд tether, который по сравнению с другими рядами выглядит почти как прямая горизонтальная линия. Однако если “присмотреться поближе”, то можно увидеть, что форма этого ряда далека от горизонтальной линии и отчасти напоминает форму других рядов из кластера 4. Для изображения этого центроида достаточно воспользоваться следующей командой (рис. 11.8): plot(hc_4_ward, type = &quot;centroids&quot;, # включает изображение центроидов clus = 4) # изображает только избранные центроиды РИСУНОК 11.8: Центроид кластера 4 из объекта hc_4_ward К сожалению, как видно на рис. 11.7, исходные настройки метода plot() в пакете dtwclust не отличаются особой эстетической привлекательностью: ширина изображающей центроид линии слишком велика, что в сочетании с прерывистостью этой линии затрудняет восприятие графика и понимание свойств временного ряда. К cчастью, метод plot() является лишь оберткой для функций ggplot2, а значит мы можем легко изменить свойства линии (рис. 11.9): # Аргументы linetype, size и alpha автоматически # передаются на функцию geom_line() из пакета ggplot2: plot(hc_4_ward, type = &quot;centroids&quot;, clus = 4, linetype = 1, size = 0.5, alpha = 1) РИСУНОК 11.9: Результат изменения внешнего вида линии, изображающей центроид одного из кластеров Выполненная нами выше кластеризация была основана на фиксированных параметрах (число выделяемых кластеров, метод кластеризации и др.), выбор которых не был определен какими–то особыми соображениями. К сожалению, как это обычно бывает с кластерным анализом, очень трудно сделать уверенное заключение о “правильности” полученного решения. Тем не менее, как было отмечено в разд. 10, существует ряд метрик, которые пытаются объективно оценить качество кластеризации. В функции cvi() (от “cluster validity indices”, т.е. “индексы валидности кластеров”) реализовано несколько таких метрик, широко используемых при кластеризации временных рядов (подробнее см. справочный файл, доступный по команде ?cvi). В качестве примера использования cvi() создадим несколько вариантов кластеризации анализируемых нами временных рядов. Тип кластеризации оставим прежним (\"hierarchical\"), но будем варьировать запрашиваемое число кластеров и методы агломерации: k &lt;- 3:5 method &lt;- c(&quot;ward.D2&quot;, &quot;average&quot;, &quot;single&quot;, &quot;complete&quot;, &quot;median&quot;, &quot;mcquitty&quot;) Приведенные значения k и method дают 18 комбинаций параметров. Поскольку расчет DTW–расстояния требует значительных вычислительных ресурсов, построение 18 кластерных решений может занять довольно много времени. Один из способов ускорить эти вычисления заключается в использовании существенно оптимизированной (но ограниченной по возможностям) функции dtw_basic() вместо dtw() для расчета DTW–расстояния. Однако мы воспользуемся другим подходом — параллелизацией вычислений. В пакете dtwclust многопоточные параллельные вычисления выполняются с помощью широко используемого для этих целей пакета foreach. Такие вычисления запустятся автоматически при условии существования необходимого “бэкенда”, который можно создать с помощью еще одного популярного пакета — doParallel: require(&quot;doParallel&quot;) # Инициализация локального вычислительного кластера, # состоящего из 7 ЦПУ (если нужно, измените это число # в соответствии с конфигурацией вашего компьютера) cl &lt;- makeCluster(7) registerDoParallel(cl) hc_par &lt;- tsclust( cryptos_list, k = k, type = &quot;hierarchical&quot;, distance = &quot;dtw&quot;, seed = 42, control = hierarchical_control(method = method), args = tsclust_args(dist = list(window.size = 7)), trace = TRUE) # для мониторинга процесса вычислений ## ## Calculating distance matrix... ## Performing hierarchical clustering... ## Extracting centroids... ## ## Elapsed time is 27.94 seconds. stopCluster(cl) # терминация кластера registerDoSEQ() # возврат к последовательным вычислениям Заметьте, что построение всех 18 вариантов кластеризации было выполнено с помощью единственного вызова функции tsclust(). Для этого потребовалось лишь подать векторы с интересующими нас значениями параметров на соответствующие аргументы (k и method). Итогом этих вычислений стал список с 18 элементами, которые содержат отдельные объекты класса TSClusters: length(hc_par) ## [1] 18 class(hc_par[[1]]) ## [1] &quot;HierarchicalTSClusters&quot; ## attr(,&quot;package&quot;) ## [1] &quot;dtwclust&quot; Для нахождения “оптимального” решения можно воспользоваться несколькими индексами “внутренней” валидности кластеров, реализованными в функции cvi(). Поскольку разные индексы могут приводить к разным заключениям о качестве сравниваемых кластеризаций, часто рассчитывают сразу несколько индексов, а затем принимают окончательное заключение на основании простого “большинства голосов”. В приведенном ниже примере мы рассчитаем индекс силуэтов (Silhouette index, \"Sil\"), индекс Данна (Dunn index, \"D\") и индекс Калинского–Харабаша (Calinski–Harabasz index, \"CH\"). Чем больше каждый из этих индексов, тем лучше соответствующее кластерное решение: lapply(hc_par, cvi, type = c(&quot;Sil&quot;, &quot;D&quot;, &quot;CH&quot;)) %&gt;% do.call(rbind, .) %&gt;% apply(., MARGIN = 2, FUN = which.max) ## Sil D CH ## 2 15 2 Как видим, два из трех индексов вадидности указывают то, что оптимальным является второе решение (см. также рис. 11.10): hc_par[[2]] ## hierarchical clustering with 3 clusters ## Using dtw distance ## Using PAM (Hierarchical) centroids ## Using method average ## ## Time required for analysis: ## user system elapsed ## 1.18 0.22 27.94 ## ## Cluster sizes with average intra-cluster distance: ## ## size av_dist ## 1 11 1654.238 ## 2 1 0.000 ## 3 10 1976.140 # график временных рядов, без центроидов: plot(hc_par[[2]], type = &quot;series&quot;) РИСУНОК 11.10: Результат оптимальной иерархической кластеризации временных рядов cryptos, найденной при помощи трех индексов валидности Рассмотренные выше примеры — это лишь введение в возможности пакета dtwclust. Дополнительную информацию о реализованных в нем методах кластеризации временных рядов можно найти в статье Sarda-Espinosa (2019). "],
["ch-features-based-clustering.html", "ГЛАВА 12 Кластеризация по описательным признакам", " ГЛАВА 12 Кластеризация по описательным признакам В главе 5 мы рассмотрели способ извлечения многочисленных описательных признаков из временных рядов средствами пакета feasts. Такие признаки далее можно использовать для решения широкого круга задач, включая кластеризацию. В качестве примера воспользуемся данными из таблицы all_crypto_features (разд. 5.3), содержащей описательные признаки 22 временных рядов из набора данных cryptos (подразд. 1.5.1). Для начала обратимся к базовой функции R heatmap(), которая строит диаграммы типа “тепловая карта”, а заодно выполняет и “двойную иерархическую кластеризацию” (biclustering) по входным данным. В результате такого анализа в группы объединяются как объекты (в нашем случае — временные ряды), так и описывающие их признаки. С одной стороны, двойная кластеризация помогает понять, сколько групп сходных объектов есть в исследуемых данных, а с другой — какие из признаков коррелируют друг с другом и, возможно, явялются избыточными для описания соответствующих объектов. Подобный анализ можно выполнить с помощью базовой функции R heatmap(): x &lt;- all_crypto_features %&gt;% dplyr::select(-c(coin, zero_var_cols(.))) %&gt;% as.matrix() heatmap(x, scale = &quot;column&quot;, margins = c(10,10), col = hcl.colors(n = 12, palette = &quot;viridis&quot;), labRow = all_crypto_features$coin, cexCol = 0.9, cexRow = 0.9) РИСУНОК 12.1: Результат выполнения двойного иерархического анализа по данным из таблицы all_crypto_features На рис. 12.1 можно увидеть (хотя и с трудом, из–за размера диаграммы и свойств полученных дендрограмм), что по совокупности нескольких десятков признаков анализируемые криптовалюты образуют примерно 3–4 группы. Сами описательные признаки образуют примерно такое же количество группы, причем одна из групп включает подавляющее их большинство. Последнее обстоятельство указывает на наличие тесной корреляции между многими признаками. Это можно легко подтвердить также с помощью функции corrplot() из одноименного пакета, которая визуализирует корреляционные матрицы: require(corrplot) corrs &lt;- cor(x, method = &quot;spearman&quot;) corrplot(corrs, type = &quot;upper&quot;, order = &quot;hclust&quot;, tl.col = &quot;black&quot;, tl.cex = 0.5, tl.srt = 45) РИСУНОК 12.2: Корреляции между описательными признаками временных рядов из таблицы cryptos На рис. 12.2 хорошо видно наличие нескольких выраженных групп тесно коррелирующих признаков. Это является проблемой для большинства методов кластерного анализа в силу “проклятия размерности”. Решить эту проблему позволяют методы снижения размерности данных, такие как метод главных компонент (PCA). В разд. 5.3 мы уже применили PCA к описательным признакам временных рядов из таблицы cryptos и обнаружили, что первые 10 главных компонент объясняют около 95% всей дисперсии в данных. Используем их для выполнения кластерного анализа по методу \\(k\\) средних с \\(k = 4\\): pc &lt;- all_crypto_features %&gt;% dplyr::select(coin) %&gt;% bind_cols(., as_tibble(pca$x)) set.seed(42) cl &lt;- pc %&gt;% dplyr::select(PC1:PC10) %&gt;% kmeans(., centers = 4, nstart = 30) Изобразим теперь исходные временные ряды, сгруппировав их в соответствии с результатами кластерного анализа: require(ggrepel) pc %&gt;% mutate(cluster = paste0(&quot;C&quot;, cl$cluster)) %&gt;% dplyr::select(coin, cluster) %&gt;% inner_join(., cryptos, by = &quot;coin&quot;) %&gt;% mutate(label = ifelse(ds == max(ds), coin, NA)) %&gt;% ggplot(., aes(ds, y, group = coin)) + geom_line() + geom_text_repel(aes(label = label), size = 3, nudge_x = 50, segment.size = 0.4, segment.color = &quot;gray60&quot;, point.padding = 0.2, force = 5, na.rm = TRUE) + scale_y_log10() + facet_wrap(~cluster, scales = &quot;free_y&quot;) + theme_minimal() + xlim(c(as.Date(&quot;2018-01-01&quot;), as.Date(&quot;2020-08-01&quot;))) РИСУНОК 12.3: Результат применения кластеризации временных рядов из таблицы cryptos по методу \\(k\\) средних "],
["ch-model-based-clustering.html", "ГЛАВА 13 Кластеризация по результатам подгонки моделей", " ГЛАВА 13 Кластеризация по результатам подгонки моделей Как было отмечено в гл. 10, один из способов кластеризации временных рядов основан на подгонке модели с определенной фиксированной структурой к каждому из анализируемых рядов и последующему использованию оцененных параметров модели в качестве описательных признаков. Для демонстрации этого принципа предположим, что временные ряды из набора данных cryptos были порождены процессом, который можно хорошо аппроксимировать байесовской стуктурной моделью с линейным локальным трендом и авторегрессионной компонентой первого порядка (гл. 7). На рис. 13.1 показан результат подгонки такой модели к четырем случайно выбранным временным рядам из набора данных cryptos. require(dplyr) require(ggplot2) require(tidyr) require(bsts) # Вспомогательная функция для подгонки моделей: fit_bsts &lt;- function(y) { ss &lt;- AddLocalLinearTrend(list(), y) ss &lt;- AddAr(ss, y, lag = 1) m &lt;- bsts(y, ss, niter = 1500, ping = 0, seed = 42) return(m) } # Вспомогательная функция для извлечения наиболее # вероятных модельных значений стоимости криптовалюты: get_bsts_fit &lt;- function(m) { burn &lt;- SuggestBurn(0.1, m) m$state.contributions %&gt;% apply(., MARGIN = 3, FUN = rowSums) %&gt;% .[-(1:burn), ] %&gt;% colMeans() } # Подгонка моделей (займет какое-то время) и # сопутствующие вычисления: set.seed(101) cryptos_bsts_fit &lt;- cryptos %&gt;% filter(coin %in% sample(unique(coin), size = 4)) %&gt;% mutate(y = log(y)) %&gt;% pivot_wider(names_from = coin, values_from = y) %&gt;% arrange(ds) %&gt;% dplyr::select(-ds) %&gt;% lapply(., fit_bsts) %&gt;% lapply(., function(x) { fit &lt;- get_bsts_fit(x) x[[&#39;fit&#39;]] &lt;- fit return(x) }) %&gt;% lapply(., function(x) { tibble(idx = 1:length(x$fit), y = as.numeric(x$original.series), fitted_y = x$fit) }) %&gt;% bind_rows(., .id = &quot;coin&quot;) # Визуализация результатов подгонки моделей cryptos_bsts_fit %&gt;% ggplot() + geom_point(aes(idx, y), col = &quot;skyblue&quot;, alpha = 0.3) + geom_line(aes(idx, fitted_y)) + theme_minimal() + facet_wrap(~coin, scales = &quot;free_y&quot;) РИСУНОК 13.1: Пример подгонки байесовской структурной модели с одинаковым набором компонент к четырем случайно выбранным временным рядам из таблицы cryptos. Синими полупрозрачными точками показаны обучающие данные. Черные линии соответствуют аппроксимированным модельным значениям Для кластеризации временных рядов cryptos мы воспользуемся несколькими количественными показателями, которые можно извелечь и bsts–объектов в готовом виде или рассчитать самостоятельно. Ниже приведен код функции, которая поможет нам выполнить подгонку модели к каждому временному ряду и получить необходимые показатели в табличном виде: get_bsts_features &lt;- function(df) { # спецификация и подгонка модели y &lt;- df$y ss &lt;- AddLocalLinearTrend(list(), y) ss &lt;- AddAr(ss, y, lag = 1) m &lt;- bsts(y, ss, niter = 1500, ping = 0, seed = 42) # извлечение готовых показателей и добавление их в таблицу: s &lt;- summary(m) result &lt;- tibble( residual_sd = s$residual.sd, prediction_sd = s$prediction.sd, r_square = s$rsquare, harvey_stat = s$relative.gof, trend_level_sigma_mean = mean(m$sigma.trend.level), trend_level_sigma_sd = sd(m$sigma.trend.level), trend_slope_sigma_mean = mean(m$sigma.trend.slope), trend_slope_sigma_sd = sd(m$sigma.trend.slope), ar1_coef_mean = mean(m$AR1.coefficients), ar1_coef_sd = sd(m$AR1.coefficients), ar1_sigma_mean = mean(m$AR1.sigma), ar1_sigma_sd = sd(m$AR1.sigma), final_state_mean = m$final.state %&gt;% rowSums() %&gt;% mean(), final_state_sd = m$final.state %&gt;% rowSums() %&gt;% sd(), loglik_mean = mean(m$log.likelihood), loglik_sd = sd(m$log.likelihood) ) return(result) } Применим функцию get_bsts_features к таблице cryptos (выполнение этого кода займет около 5 минут): cryptos_bsts_features &lt;- cryptos %&gt;% group_by(coin) %&gt;% do(get_bsts_features(.)) %&gt;% ungroup() Для кластеризации анализируемых временных рядов на основе полученных описательных признаков воспользуемся методом \\(k\\) средних c \\(k=4\\) (предварительно выполнив стандартизацию данных и применив метод PCA для удаления корреляции между отдельными признаками): set.seed(42) pca_bsts_features &lt;- cryptos_bsts_features %&gt;% dplyr::select(-coin) %&gt;% prcomp(scale = TRUE) # Первые пять главных компонент объясняют примерно 97% # исходной дисперсии данных (результат не приводится # для экономии места): # summary(pca_bsts_features) # Кластеризация: set.seed(1984) cl_bsts &lt;- cryptos_bsts_features %&gt;% dplyr::select(coin) %&gt;% bind_cols(., as_tibble(pca_bsts_features$x)) %&gt;% dplyr::select(PC1:PC5) %&gt;% kmeans(., centers = 4, nstart = 50) Изобразим теперь исходные временные ряды, сгруппировав их в соответствии с результатами выполненного кластерного анализа: require(ggrepel) cryptos_bsts_features %&gt;% mutate(cluster = paste0(&quot;C&quot;, cl_bsts$cluster)) %&gt;% dplyr::select(coin, cluster) %&gt;% inner_join(., cryptos, by = &quot;coin&quot;) %&gt;% group_by(coin) %&gt;% mutate(label = ifelse(ds == max(ds), coin, NA)) %&gt;% ggplot(., aes(ds, y, group = coin)) + geom_line() + geom_text_repel(aes(label = label), size = 3, nudge_x = 50, segment.size = 0.4, segment.color = &quot;gray60&quot;, point.padding = 0.2, force = 4, na.rm = TRUE) + scale_y_log10() + facet_wrap(~cluster, scales = &quot;free_y&quot;) + theme_minimal() + xlim(c(as.Date(&quot;2018-01-01&quot;), as.Date(&quot;2020-08-01&quot;))) РИСУНОК 13.2: Результат кластеризации временных рядов из таблицы cryptos по методу \\(k\\) средних на основе количественных показателей bsts–моделей Представленный на рис. 13.2 результат заметно отличается от выполненных нами ранее кластеризаций на основе исходных данных (гл. 11) и по описательным признакам (гл. 12). Это наблюдение еще раз подчеркивает необходимость применения нескольких вариантов кластеризации для нахождения решения, которое имеет смысл с т.з. его дальнейшего практического использования. "],
["ch-literature.html", "ГЛАВА 14 Литература", " ГЛАВА 14 Литература "]
]
